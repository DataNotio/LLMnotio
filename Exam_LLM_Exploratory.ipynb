{"cells":[{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchain_openai in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.1.21)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.29 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain_openai) (0.2.30)\n","Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain_openai) (1.40.6)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain_openai) (0.7.0)\n","Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (6.0.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (0.1.99)\n","Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (24.1)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (2.8.2)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (8.5.0)\n","Requirement already satisfied: typing-extensions>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.29->langchain_openai) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.4.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.27.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n","Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.7.24)\n","Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.7)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.1)\n","Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.7.4)\n","Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.29->langchain_openai) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.29->langchain_openai) (3.10.7)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.29->langchain_openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.29->langchain_openai) (2.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: chromadb in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.5.5)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.2.1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (2.8.2)\n","Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (0.7.6)\n","Requirement already satisfied: fastapi>=0.95.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (0.112.0)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n","Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.26.4)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (4.12.2)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.18.1)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.26.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.26.0)\n","Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (0.47b0)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.26.0)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (0.19.1)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (4.66.5)\n","Requirement already satisfied: overrides>=7.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (6.4.0)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (1.65.4)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (4.2.0)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (0.12.3)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (30.1.0)\n","Requirement already satisfied: tenacity>=8.2.3 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (8.5.0)\n","Requirement already satisfied: PyYAML>=6.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (6.0.1)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (4.1.0)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from chromadb) (3.10.7)\n","Requirement already satisfied: httpx>=0.27.0 in /home/codespace/.local/lib/python3.10/site-packages (from chromadb) (0.27.0)\n","Requirement already satisfied: packaging>=19.1 in /home/codespace/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.1)\n","Requirement already satisfied: pyproject_hooks in /usr/local/python/3.10.13/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n","Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n","Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n","Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n","Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n","Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n","Requirement already satisfied: six>=1.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.33.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/python/3.10.13/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Requirement already satisfied: coloredlogs in /usr/local/python/3.10.13/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/python/3.10.13/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/python/3.10.13/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.4)\n","Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.0)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n","Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n","Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n","Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n","Requirement already satisfied: opentelemetry-util-http==0.47b0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n","Requirement already satisfied: asgiref~=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.24.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n","Requirement already satisfied: httptools>=0.5.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.23.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: transformers in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.44.0)\n","Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (0.24.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (2024.7.24)\n","Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: faiss-cpu in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.8.0.post1)\n","Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n","Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from faiss-cpu) (24.1)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: langchain-community in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.2.12)\n","Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (2.0.32)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (3.10.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (0.2.13)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (0.2.30)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (0.1.99)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n","Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: langchain in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.2.13)\n","Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (2.0.32)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (3.10.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.2.30)\n","Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.2.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (0.1.99)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain) (2.8.2)\n","Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain) (4.12.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain) (3.0.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install langchain_openai\n","!pip install chromadb\n","!pip install transformers\n","!pip install faiss-cpu\n","!pip install -qU langchain-huggingface\n","!pip install -U langchain-community\n","!pip install langchain"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import bs4\n","from langchain import hub\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","\n","loader = WebBaseLoader(\n","    web_paths=(\"https://other-docs.snowflake.com/en/polaris/overview\",),\n",")\n","docs = loader.load()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["data = []\n","for doc in docs:\n","  page_number = doc.metadata['source']\n","  page_title = doc.metadata['title']\n","  page_content = doc.page_content\n","  data.append([page_number,page_title, page_content])\n","\n","df = pd.DataFrame(data, columns=['Page','Page Title','Content'])"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 40 English links. Loading documents...\n","                                                Page  \\\n","0  https://docs.snowflake.com/en/user-guide/data-...   \n","1  https://docs.snowflake.com/en/user-guide/dynam...   \n","2  https://docs.snowflake.com/en/release-notes/ne...   \n","3  https://docs.snowflake.com/en/user-guide-getti...   \n","4            https://docs.snowflake.com/en/reference   \n","\n","                                          Page Title  \\\n","0  Understanding & using Time Travel | Snowflake ...   \n","1           Dynamic tables | Snowflake Documentation   \n","2               What’s New | Snowflake Documentation   \n","3          Getting Started - Snowflake Documentation   \n","4                Reference | Snowflake Documentation   \n","\n","                                             Content  \n","0  Understanding & using Time Travel | Snowflake ...  \n","1  Dynamic tables | Snowflake DocumentationDOCUME...  \n","2  What’s New | Snowflake DocumentationDOCUMENTAT...  \n","3  Getting Started - Snowflake DocumentationDOCUM...  \n","4  Reference | Snowflake DocumentationDOCUMENTATI...  \n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def get_english_links(url):\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        links = soup.find_all('a')\n","        hrefs = {link.get('href') for link in links if link.get('href')}\n","        english_links = {href for href in hrefs if '/en/' in href}\n","        english_links = {href if href.startswith('http') else url + href for href in english_links}\n","        return english_links\n","    else:\n","        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n","        return set()\n","\n","def load_documents_from_links(links):\n","    loader = WebBaseLoader(web_paths=tuple(links))\n","    docs = loader.load()\n","    data = []\n","    for doc in docs:\n","        page_number = doc.metadata['source']\n","        page_title = doc.metadata['title']\n","        page_content = doc.page_content\n","        data.append([page_number, page_title, page_content])\n","    return pd.DataFrame(data, columns=['Page', 'Page Title', 'Content']), docs\n","\n","\n","url = \"https://docs.snowflake.com\"\n","english_links = get_english_links(url)\n","if english_links:\n","    print(f\"Found {len(english_links)} English links. Loading documents...\")\n","    df, docs = load_documents_from_links(english_links)\n","    print(df.head())\n","else:\n","    print(\"No English links found or webpage could not be accessed.\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/data-time-travel', 'title': 'Understanding & using Time Travel | Snowflake Documentation', 'language': 'en'}, page_content=\"Understanding & using Time Travel | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryReplicationIntroductionConsiderationsConfigurationSecurity Integrations and Network Policy ReplicationStage, Pipe, and Load History ReplicationUnderstanding CostFailoverAccount FailoverClient RedirectOverviewData RecoveryTime TravelFail-safeStorage CostsPerformance OptimizationCost & BillingGuidesBusiness Continuity & Data RecoveryTime Travel\\n\\nUnderstanding & using Time Travel¶\\nSnowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period.\\nIt serves as a powerful tool for performing the following tasks:\\n\\nRestoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.\\nDuplicating and backing up data from key points in the past.\\nAnalyzing data usage/manipulation over specified periods of time.\\n\\n\\nIntroduction to Time Travel¶\\n\\n\\n\\nUsing Time Travel, you can perform the following actions within a defined period of time:\\n\\nQuery data in the past that has since been updated or deleted.\\nCreate clones of entire tables, schemas, and databases at or before specific points in the past.\\nRestore tables, schemas, and databases that have been dropped.\\n\\n\\nNote\\nWhen querying historical data in a table or non-materialized view, the current table or view schema is used. For more\\ninformation, see Usage notes for AT | BEFORE.\\n\\nAfter the defined period of time has elapsed, the data is moved into Snowflake Fail-safe and these actions\\ncan no longer be performed.\\n\\nNote\\nA long-running Time Travel query will delay moving any data and objects (tables, schemas, and databases) in the account into Fail-safe,\\nuntil the query completes.\\n\\n\\nTime Travel SQL extensions¶\\nTo support Time Travel, the following SQL extensions have been implemented:\\n\\nAT | BEFORE clause which can be specified in SELECT statements and CREATE … CLONE commands (immediately\\nafter the object name). The clause uses one of the following parameters to pinpoint the exact historical data you want to access:\\n\\nTIMESTAMP\\nOFFSET (time difference in seconds from the present time)\\nSTATEMENT (query ID for statement)\\n\\n\\nUNDROP command for tables, schemas, and databases.\\n\\n\\n\\n\\n\\n\\n\\nData retention period¶\\nA key component of Snowflake Time Travel is the data retention period.\\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\\nTime Travel operations (SELECT, CREATE … CLONE, UNDROP) can be performed on the data.\\nThe standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts:\\n\\nFor Snowflake Standard Edition, the retention period can be set to 0 (or unset back to the default of 1 day) at the account and object\\nlevel (i.e. databases, schemas, and tables).\\nFor Snowflake Enterprise Edition (and higher):\\n\\nFor transient databases, schemas, and tables, the retention period can be set to 0 (or unset back to the default of 1 day). The same\\nis also true for temporary tables.\\nFor permanent databases, schemas, and tables, the retention period can be set to any value from 0 up to 90 days.\\n\\n\\n\\n\\nNote\\nA retention period of 0 days for an object effectively deactivates Time Travel for the object.\\n\\nWhen the retention period ends for an object, the historical data is moved into Snowflake Fail-safe:\\n\\nHistorical data is no longer available for querying.\\nPast objects can no longer be cloned.\\nPast objects that were dropped can no longer be restored.\\n\\nTo specify the data retention period for Time Travel:\\n\\nThe DATA_RETENTION_TIME_IN_DAYS object parameter can be used by users with the ACCOUNTADMIN role to set the default\\nretention period for your account.\\nThe same parameter can be used to explicitly override the default when creating a database, schema, and individual table.\\nThe data retention period for a database, schema, or table can be changed at any time.\\nThe MIN_DATA_RETENTION_TIME_IN_DAYS account parameter can be set by users with the ACCOUNTADMIN role to set a minimum\\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\\n\\n\\n\\n\\nEnabling and deactivating Time Travel¶\\nNo tasks are required to enable Time Travel. It is automatically enabled with the standard, 1-day retention period.\\nHowever, you may want to upgrade to Snowflake Enterprise Edition to enable configuring longer data retention periods of up to 90 days\\nfor databases, schemas, and tables. Note that extended data retention requires additional storage which will be reflected in your monthly\\nstorage charges. For more information about storage charges, see Storage Costs for Time Travel and Fail-safe.\\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set DATA_RETENTION_TIME_IN_DAYS to 0 at\\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\\nby default; however, this default can be overridden at any time for any database, schema, or table.\\nA user with the ACCOUNTADMIN role can also set the MIN_DATA_RETENTION_TIME_IN_DAYS at the account level. This parameter\\nsetting enforces a minimum data retention period for databases, schemas, and tables. Setting MIN_DATA_RETENTION_TIME_IN_DAYS does not\\nalter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. It may, however, change the effective data retention period for objects.\\nWhen MIN_DATA_RETENTION_TIME_IN_DAYS is set at the account level, the data retention period for an object is determined by\\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\\nTime Travel can be deactivated for individual databases, schemas, and tables by specifying DATA_RETENTION_TIME_IN_DAYS with a\\nvalue of 0 for the object. However, if DATA_RETENTION_TIME_IN_DAYS is set to a value of 0, and MIN_DATA_RETENTION_TIME_IN_DAYS is set\\nat the account level and is greater than 0, the higher value setting takes precedence.\\n\\nAttention\\nBefore setting DATA_RETENTION_TIME_IN_DAYS to 0 for any object, consider whether you want to deactivate Time Travel for the object,\\nparticularly as it pertains to recovering the object if it is dropped. When an object with no retention period is dropped, you will not\\nbe able to restore the object.\\nAs a general rule, we recommend maintaining a value of (at least) 1 day for any given object.\\n\\nIf the Time Travel retention period is set to 0, any modified or deleted data is moved into Fail-safe (for permanent tables)\\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\\n\\n\\nSpecifying the data retention period for an object¶\\n\\n Enterprise Edition Feature\\nSpecifying a retention period greater than 1 day requires Enterprise Edition (or higher). To inquire about upgrading, please contact\\nSnowflake Support.\\n\\nBy default, the maximum retention period is 1 day (i.e. one 24 hour period). With Snowflake Enterprise Edition (and higher), the default\\nfor your account can be set to any value up to 90 days:\\n\\nWhen creating a table, schema, or database, the account default can be overridden using the DATA_RETENTION_TIME_IN_DAYS\\nparameter in the command.\\nIf a retention period is specified for a database or schema, the period is inherited by default for all objects created in the\\ndatabase/schema.\\n\\nA minimum retention period can be set on the account using the MIN_DATA_RETENTION_TIME_IN_DAYS parameter. If this parameter is\\nset at the account level, the data retention period for an object is determined by\\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\\n\\n\\nChanging the data retention period for an object¶\\nIf you change the data retention period for a table, the new retention period impacts all data that is active, as well as any data currently\\nin Time Travel. The impact depends on whether you increase or decrease the period:\\n\\nIncreasing Retention:\\nCauses the data currently in Time Travel to be retained for the longer time period.\\nFor example, if you have a table with a 10-day retention period and increase the period to 20 days, data that would have been removed\\nafter 10 days is now retained for an additional 10 days before moving into Fail-safe.\\nNote that this doesn’t apply to any data that is older than 10 days and has already moved into Fail-safe.\\n\\nDecreasing Retention:\\nReduces the amount of time data is retained in Time Travel:\\n\\nFor active data modified after the retention period is reduced, the new shorter period applies.\\nFor data that is currently in Time Travel:\\n\\n\\nIf the data is still within the new shorter period, it remains in Time Travel.\\nIf the data is outside the new period, it moves into Fail-safe.\\n\\n\\n\\n\\nFor example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved\\ninto Fail-safe, leaving only the data from day 1 accessible through Time Travel.\\nHowever, the process of moving the data from Time Travel into Fail-safe is performed by a background process, so the change is not immediately\\nvisible. Snowflake guarantees that the data will be moved, but does not specify when the process will complete; until the background process\\ncompletes, the data is still accessible through Time Travel.\\n\\n\\n\\nNote\\nIf you change the data retention period for a database or schema, the change only affects active objects contained within\\nthe database or schema. Any objects that have been dropped (for example, tables) remain unaffected.\\nFor example, if you have a schema s1 with a 90-day retention period and table t1 is in schema s1,\\ntable t1 inherits the 90-day retention period. If you drop table s1.t1, t1 is retained in Time Travel\\nfor 90 days. Later, if you change the schema’s data retention period to 1 day, the retention\\nperiod for the dropped table t1 is unchanged. Table t1 will still be retained in Time Travel for 90 days.\\nTo alter the retention period of a dropped object, you must undrop the object, then alter its retention period.\\n\\nTo change the retention period for an object, use the appropriate ALTER <object> command. For example, to change the\\nretention period for a table:\\nCREATE TABLE mytable(col1 NUMBER, col2 DATE) DATA_RETENTION_TIME_IN_DAYS=90;\\n\\nALTER TABLE mytable SET DATA_RETENTION_TIME_IN_DAYS=30;\\n\\nCopy\\n\\nAttention\\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\\nretention period explicitly set. For example:\\n\\nIf you change the retention period at the account level, all databases, schemas, and tables that do not have an explicit retention period\\nautomatically inherit the new retention period.\\nIf you change the retention period at the schema level, all tables in the schema that do not have an explicit retention period inherit the\\nnew retention period.\\n\\nKeep this in mind when changing the retention period for your account or any objects in your account because the change might have\\nTime Travel consequences that you did not anticipate or intend. In particular, we do not recommend changing the retention period to 0\\nat the account level.\\n\\n\\nDropped containers and object retention inheritance¶\\nCurrently, when a database is dropped, the data retention period for child schemas or tables, if explicitly set to be different from the\\nretention of the database, is not honored. The child schemas or tables are retained for the same period of time as the database.\\nSimilarly, when a schema is dropped, the data retention period for child tables, if explicitly set to be different from the retention of\\nthe schema, is not honored. The child tables are retained for the same period of time as the schema.\\nTo honor the data retention period for these child objects (schemas or tables), drop them explicitly before you drop the database\\nor schema.\\n\\n\\n\\nQuerying historical data¶\\nWhen any DML operations are performed on a table, Snowflake retains previous versions of the table data for a defined period of time. This\\nenables querying earlier versions of the data using the AT | BEFORE clause.\\nThis clause supports querying data either exactly at or immediately preceding a specified point in the table’s history within the\\nretention period. The specified point can be time-based (e.g. a timestamp or time offset from the present) or it can be the ID for a\\ncompleted statement (e.g. SELECT or INSERT).\\nFor example:\\n\\nThe following query selects historical data from a table as of the date and time represented by the specified\\ntimestamp:\\n\\nSELECT * FROM my_table AT(TIMESTAMP => 'Wed, 26 Jun 2024 09:20:00 -0700'::timestamp_tz);\\n\\nCopy\\n\\n\\nThe following query selects historical data from a table as of 5 minutes ago:\\n\\nSELECT * FROM my_table AT(OFFSET => -60*5);\\n\\nCopy\\n\\n\\nThe following query selects historical data from a table up to, but not including any changes made by the specified statement:\\n\\nSELECT * FROM my_table BEFORE(STATEMENT => '8e5d0ca9-005e-44e6-b858-a8f5b37c5726');\\n\\nCopy\\n\\n\\n\\n\\nNote\\nIf the TIMESTAMP, OFFSET, or STATEMENT specified in the AT | BEFORE clause falls outside the data\\nretention period for the table, the query fails and returns an error.\\n\\n\\n\\nCloning historical objects¶\\nIn addition to queries, the AT | BEFORE clause can be used with the CLONE keyword in the CREATE command\\nfor a table, schema, or database to create a logical duplicate of the object at a specified point in the object’s history.\\nFor example:\\n\\nThe following CREATE TABLE statement creates a clone of a table as of the date and time represented by the\\nspecified timestamp:\\n\\nCREATE TABLE restored_table CLONE my_table\\n  AT(TIMESTAMP => 'Wed, 26 Jun 2024 01:01:00 +0300'::timestamp_tz);\\n\\nCopy\\n\\n\\nThe following CREATE SCHEMA statement creates a clone of a schema and all its objects as they existed 1 hour\\nbefore the current time:\\n\\nCREATE SCHEMA restored_schema CLONE my_schema AT(OFFSET => -3600);\\n\\nCopy\\n\\n\\nThe following CREATE DATABASE statement creates a clone of a database and all its objects as they existed prior\\nto the completion of the specified statement:\\n\\nCREATE DATABASE restored_db CLONE my_db\\n  BEFORE(STATEMENT => '8e5d0ca9-005e-44e6-b858-a8f5b37c5726');\\n\\nCopy\\n\\n\\n\\n\\nNote\\nThe cloning operation for a database or schema fails:\\n\\n\\nIf the specified Time Travel time is beyond the retention time of any current child (e.g., a table) of the entity.\\nAs a workaround for child objects that have been purged from Time Travel, use the\\nIGNORE TABLES WITH INSUFFICIENT DATA RETENTION parameter of the\\nCREATE <object> … CLONE command. For more information, see Child objects and data retention time.\\n\\nIf the specified Time Travel time is at or before the point in time when the object was created.\\n\\n\\n\\n\\nThe following CREATE DATABASE statement creates a clone of a database and all its objects as they existed\\nfour days ago, skipping any tables that have a data retention period of less than four days:\\n\\nCREATE DATABASE restored_db CLONE my_db\\n  AT(TIMESTAMP => DATEADD(days, -4, current_timestamp)::timestamp_tz)\\n  IGNORE TABLES WITH INSUFFICIENT DATA RETENTION;\\n\\nCopy\\n\\n\\n\\n\\n\\nDropping and restoring objects¶\\n\\nDropping objects¶\\nWhen a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. Instead, it is retained for the\\ndata retention period for the object, during which time the object can be restored. Once dropped objects are moved to\\nFail-safe, you cannot restore them.\\nTo drop a table, schema, or database, use the following commands:\\n\\nDROP TABLE\\nDROP SCHEMA\\nDROP DATABASE\\n\\n\\nNote\\nAfter dropping an object, creating an object with the same name does not restore the object. Instead, it creates a new version of the\\nobject. The original, dropped version is still available and can be restored.\\nRestoring a dropped object restores the object in place (i.e. it does not create a new object).\\n\\n\\n\\nListing dropped objects¶\\nDropped tables, schemas, and databases can be listed using the following commands with the HISTORY keyword specified:\\n\\nSHOW TABLES\\nSHOW SCHEMAS\\nSHOW DATABASES\\n\\nFor example:\\n\\nSHOW TABLES HISTORY LIKE 'load%' IN mytestdb.myschema;\\n\\nSHOW SCHEMAS HISTORY IN mytestdb;\\n\\nSHOW DATABASES HISTORY;\\n\\nCopy\\n\\nThe output includes all dropped objects and an additional DROPPED_ON column, which displays the date and time when the object was dropped.\\nIf an object has been dropped more than once, each version of the object is included as a separate row in the output.\\n\\nNote\\nAfter the retention period for an object has passed and the object has been purged, it is no longer displayed in the\\nSHOW <object_type> HISTORY output.\\n\\n\\n\\nRestoring objects¶\\nA dropped object that has not been purged from the system (i.e. the object is displayed in the SHOW <object_type> HISTORY output) can be\\nrestored using the following commands:\\n\\nUNDROP TABLE\\nUNDROP SCHEMA\\nUNDROP DATABASE\\n\\nCalling UNDROP restores the object to its most recent state before the DROP command was issued.\\nFor example:\\n\\nUNDROP TABLE mytable;\\n\\nUNDROP SCHEMA myschema;\\n\\nUNDROP DATABASE mydatabase;\\n\\nCopy\\n\\n\\nNote\\nIf an object with the same name already exists, UNDROP fails. You must rename the existing object, which then enables you to restore\\nthe previous version of the object.\\n\\n\\n\\nAccess control requirements and name resolution¶\\nSimilar to dropping an object, a user must have OWNERSHIP privileges for an object to restore it. In addition, the user must have CREATE\\nprivileges on the object type for the database or schema where the dropped object will be restored.\\nRestoring tables and schemas is only supported in the current schema or current database, even if a fully-qualified object name is specified.\\n\\n\\nExample: Dropping and restoring a table multiple times¶\\nIn the following example, the mytestdb.public schema contains two tables: loaddata1 and proddata1. The loaddata1 table is\\ndropped and recreated twice, creating three versions of the table:\\n\\n\\nCurrent version\\nSecond (i.e. most recent) dropped version\\nFirst dropped version\\n\\n\\nThe example then illustrates how to restore the two dropped versions of the table:\\n\\n\\nFirst, the current table with the same name is renamed to loaddata3. This enables restoring the most recent version of the dropped\\ntable, based on the timestamp.\\nThen, the most recent dropped version of the table is restored.\\nThe restored table is renamed to loaddata2 to enable restoring the first version of the dropped table.\\nLastly, the first version of the dropped table is restored.\\n\\nSHOW TABLES HISTORY;\\n\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | [NULL]                          |\\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n\\nDROP TABLE loaddata1;\\n\\nSHOW TABLES HISTORY;\\n\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n\\nCREATE TABLE loaddata1 (c1 number);\\nINSERT INTO loaddata1 VALUES (1111), (2222), (3333), (4444);\\n\\nDROP TABLE loaddata1;\\n\\nCREATE TABLE loaddata1 (c1 varchar);\\n\\nSHOW TABLES HISTORY;\\n\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | Fri, 13 May 2016 19:05:51 -0700 |\\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n\\nALTER TABLE loaddata1 RENAME TO loaddata3;\\n\\nUNDROP TABLE loaddata1;\\n\\nSHOW TABLES HISTORY;\\n\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | [NULL]                          |\\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | Fri, 13 May 2016 19:04:46 -0700 |\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n\\nALTER TABLE loaddata1 RENAME TO loaddata2;\\n\\nUNDROP TABLE loaddata1;\\n\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n| created_on                      | name      | database_name | schema_name | kind  | comment | cluster_by | rows | bytes | owner  | retention_time | dropped_on                      |\\n|---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------|\\n| Tue, 17 Mar 2016 17:41:55 -0700 | LOADDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 48   | 16248 | PUBLIC | 1              | [NULL]                          |\\n| Fri, 13 May 2016 19:05:32 -0700 | LOADDATA2 | MYTESTDB      | PUBLIC      | TABLE |         |            | 4    | 4096  | PUBLIC | 1              | [NULL]                          |\\n| Fri, 13 May 2016 19:06:01 -0700 | LOADDATA3 | MYTESTDB      | PUBLIC      | TABLE |         |            | 0    | 0     | PUBLIC | 1              | [NULL]                          |\\n| Tue, 17 Mar 2016 17:51:30 -0700 | PRODDATA1 | MYTESTDB      | PUBLIC      | TABLE |         |            | 12   | 4096  | PUBLIC | 1              | [NULL]                          |\\n+---------------------------------+-----------+---------------+-------------+-------+---------+------------+------+-------+--------+----------------+---------------------------------+\\n\\nCopy\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageIntroduction to Time TravelEnabling and deactivating Time TravelSpecifying the data retention period for an objectChanging the data retention period for an objectQuerying historical dataCloning historical objectsDropping and restoring objectsRelated contentOverview of the Data LifecycleData storage considerationsRelated info\\nFor a tutorial on using Time Travel, see the following page:\\n\\nGetting Started with Time Travel\\n(Snowflake Quickstarts)\\n\\nLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/dynamic-tables-intro', 'title': 'Dynamic tables | Snowflake Documentation', 'language': 'en'}, page_content='Dynamic tables | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingLoading Semi-Structured DataIntroductionSupported FormatsConsiderationsAccessing Unstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesOverviewWorking with dynamic tablesBest practicesUnderstanding costLimitationsDynamic table performanceTroubleshootingStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData LoadingDynamic tables\\n\\nDynamic tables¶\\nDynamic tables simplify data engineering in Snowflake by providing a reliable, cost-effective, and\\nautomated way to transform data. Instead of managing transformation steps with tasks and scheduling, you\\ndefine the end state using dynamic tables and let Snowflake handle the pipeline management.\\nHere’s why they’re beneficial:\\n\\nDeclarative programming: Define your pipeline outcomes using declarative SQL without worrying\\nabout the steps to achieve them, reducing complexity.\\nTransparent orchestration: Easily create pipelines of various shapes, from linear chains to\\ndirected graphs, by chaining dynamic tables together. Snowflake manages the orchestration and\\nscheduling of pipeline refresh based on your data freshness target.\\nPerformance boost with incremental processing: For favorable workloads\\nthat are suited for incremental processing, dynamic tables can provide a significant performance\\nimprovement over full refreshes.\\nEasy switching: Transition seamlessly from batch to streaming with a single ALTER DYNAMIC TABLE\\ncommand. You control how often data is refreshed in your pipeline, which helps balance cost and data\\nfreshness.\\nOperationalization: Dynamic tables are fully observable and manageable through Snowsight, and also\\noffer programmatic access to build your own observability apps.\\n\\nA dynamic table reflects query results, eliminating the need for a separate target table and custom code\\nfor data transformation. An automated process updates the results regularly through scheduled\\nrefreshes. Since a dynamic table’s content is based on the\\nquery, you can’t modify it using DML operations. The automated refresh process materializes query results\\ninto the dynamic table.\\nThe following topics introduce dynamic table concepts and explain how to transform data in a continuous\\ndata pipeline using dynamic tables.\\n\\n\\n\\n\\n\\n\\nConcept\\nDescription\\n\\n\\n\\nHow dynamic tables work\\nLearn about the privileges needed to work with dynamic tables, how dynamic table refresh operates, and the distinctions between dynamic tables, streams & tasks, and materialized views.\\n\\nWorking with dynamic tables\\nLearn about creating, managing, and monitoring dynamic tables.\\n\\nBest practices for dynamic tables\\nUnderstand the best practices for working with dynamic tables.\\n\\nUnderstanding cost for dynamic tables\\nUnderstand the compute and storage cost for dynamic tables.\\n\\nKnown limitations for dynamic tables\\nSome actions might be restricted due to limitations on using dynamic tables or if you don’t have the necessary privileges.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/release-notes/new-features', 'title': 'What’s New | Snowflake Documentation', 'language': 'en'}, page_content=\"What’s New | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusWhat's NewRecent ReleasesAug 09, 2024 - Streamlit in SnowflakePreview - 8.30Aug 08, 2024 - RANGE BETWEEN window frames with explicit offsetsAug 08, 2024 - Snowflake Data Clean RoomsAug 08, 2024 - Cross-region inference for LLM functionsAug 06, 2024 - Support for Virtual Private Snowflake (VPS) on AWSAug 06, 2024 - Document AIAug 02, 2024 - Custom UI in Streamlit in SnowflakeAug 07-08, 2024 - 8.29Aug 02, 2024 - Classification function error message improvementsAug 02, 2024 - Snowflake Native App FrameworkAug 01, 2024 - Streamlit in SnowflakeAug 01, 2024 - Snowpark Container ServicesAug 01, 2024 -- SnowsightJul 31, 2024 - VS Code extensionJul 31, 2024 - Streamlit in SnowflakeJul 30, 2024 - Cortex Fine-tuning function in Snowsight PreviewJul 29, 2024 - Snowsight Notebooks default warehouseJul 29-Aug 01, 2024 - 8.28Jul 25, 2024 - AI21 model available in CORTEX AI Jul 25, 2024 - Snowflake Data Clean RoomsJul 25, 2024 - Cortex Search previewJul 24, 2024 - Document AIJul 24, 2024 - Cortex LLM functions updateJul 23, 2024 - Managing Listings using SQL GAJul 23, 2024 - New Meta AI model available in CORTEX AIJul 22-25, 2024 - 8.27 (with behavior changes)Jul 19, 2024 - CORTEX_FUNCTIONS_USAGE_HISTORY viewJul 18, 2024 - Support for shared external table and Iceberg tablesJul 17, 2024 - Network rules and policies in Snowsight GAJul 15, 2024 - Snowflake Copilot GAJul 15-17, 2024 - 8.26Jul 11, 2024 - Snowflake Data Clean RoomsJul 11, 2024 - Snowflake connectors for MySQL and PostgreSQLJul 08-12, 2024 - 8.25Jul 03, 2024 - External network access in Streamlit in SnowflakeJul 03, 2024 - Data pipelines: Support for Iceberg tables with dynamic tables and streams - PreviewJul 02, 2024 - Snowflake NotebooksJul 01-03, 2024 - 8.24Jul 01, 2024 - Snowsight home pageJun 28, 2024 - Snowsight MFA reminderJun 28, 2024 - New geospatial H3 functions GAJun 28, 2024 - Streamlit in SnowflakeJun 27, 2024 - Document AIJun 26, 2024 - Cost ManagementJun 25, 2024 - TO_QUERY table functionJun 25, 2024 - Snowflake Data Clean RoomsJun 24, 2024 - Time Travel for hybrid tablesJun 21, 2024 - Document AIJun 17-30, 2024 - 8.23Jun 17, 2024 - New LLM functionsJun 15, 2024 - Anomaly DetectionJun 11, 2024 - Data sharing in non-secure viewsJun 11, 2024 - Snowflake Data Clean RoomsJun 10-15, 2024 - 8.22 (with behavior changes)Jun 10, 2024 - Iceberg tables GAJun 05, 2024 - New geospatial functions in previewJun 03-06, 2024 - SummitJun 03, 2024 - New 1024 dimensional vector embedding functionJun 03, 2024 - Entity-Level PrivacyPrevious Releases2024 Releases2023 Releases2022 Releases2015 - 2021 ReleasesBehavior ChangesDeprecated FeaturesPreview FeaturesPerformance ImprovementsSQL ImprovementsClients, Drivers, and LibrariesMonthly ReleasesClient versions & support policySnowflake ConnectorsSnowflake Connector for Google Analytics Raw DataSnowflake Connector for Google Analytics Aggregate DataSnowflake Connector for ServiceNowSnowflake Connector for ServiceNow V2Snowflake Connector for MySQLSnowflake Connector for PostgreSQLNative SDK for ConnectorsNative SDK for Connectors Java libraryNative SDK for Connectors Java Test libraryNative SDK for Connectors Java TemplateNative SDK Example Java GitHub ConnectorReleasesWhat's New\\n\\nWhat’s New¶\\nOverview of the new features, enhancements, and important fixes introduced in the most recent releases of Snowflake.\\nIf you have questions about any of these features, please feel free to contact\\nSnowflake Support.\\n\\n\\nAugust 09, 2024 — Streamlit in Snowflake on AWS GovCloud –— General Availability\\nPreview — 8.30 Release Notes\\nNew features\\nSQL updates\\nData loading / unloading updates\\nRelease notes change log\\n\\n\\nAugust 08, 2024 — RANGE BETWEEN window frames with explicit offsets — General Availability\\nAugust 08, 2024 — Snowflake Data Clean Rooms Release Notes\\nSupport for external tables and Iceberg tables\\nIntegration with TransUnion TruAudience Identity\\n\\n\\nAugust 08, 2024 — Cross-region inference for Snowflake AI & ML features — General Availability\\nAugust 08, 2024 — Snowflake Native App Framework Release Notes release notes\\nAugust 06, 2024: Document AI release notes\\nAugust 02, 2024 — Custom UI in Streamlit in Snowflake –— General Availability\\nAugust 07-08, 2024 — 8.29 Release Notes\\nSecurity updates\\nExtensibility updates\\nData lake updates\\nRelease notes change log\\n\\n\\nAugust 02, 2024 — ML Functions: Improved Error Messages in Classification\\nAugust 02, 2024 — Snowflake Native App Framework Release Notes release notes\\nAugust 01, 2024 — Support for Streamlit 1.35.0 in Streamlit in Snowflake\\nAugust 01, 2024 — Snowpark Container Services release notes\\nAugust 01, 2024 — Snowsight Release Notes\\nNew stage explorer in Snowsight — General availability\\nSchema detection and visual column mapping for loading files to existing tables in Snowsight –— Preview\\n\\n\\nJuly 31, 2024 — Context functions and row access policies in Streamlit in Snowflake –— General Availability\\nJuly 30, 2024 — Snowsight Release Notes\\nCortex Fine-tuning –— Preview\\n\\n\\nJuly 29, 2024 — Snowsight Release Notes\\nDefault warehouse for Notebooks workloads — Public Preview\\n\\n\\nJuly 29-August 01, 2024 — 8.28 Release Notes\\nSQL updates\\nData governance updates\\nData loading/unloading updates\\nData pipelines updates\\nData lake updates\\nRelease notes change log\\n\\n\\nJuly 25, 2024 — New AI21 model available in Snowflake Cortex AI\\nJuly 25, 2024 — Snowflake Data Clean Rooms Release Notes\\nAcxiom Real ID integration\\nUsing developer APIs for provider activation\\nUser interface for custom templates enhancement\\nSQL Query template enhancement\\n\\n\\nJuly 25, 2024 — Cortex Search — Preview\\nJuly 24, 2024 — Cortex Guard for Snowflake Cortex AI — General Availability\\nJuly 24, 2024 — Document AI release notes\\nJuly 23, 2024 — New Meta AI models available in Snowflake Cortex AI\\nJuly 22-25, 2024 — 8.27 Release Notes (with behavior changes)\\nBehavior change bundles\\nNew features\\nSQL updates\\nData governance updates\\nRelease notes change log\\n\\n\\nJuly 19, 2024 — CORTEX_FUNCTIONS_USAGE_HISTORY view — General Availability\\nJuly 18, 2024 —  Snowflake Native App Framework - Support for shared external table and Iceberg tables — Preview\\nJuly 17, 2024 — Snowsight Release Notes\\nNetwork rules and network policies –— Generally available\\n\\n\\nJuly 15, 2024 — Snowflake Copilot — Generally available\\nJuly 15-17, 2024 — 8.26 Release Notes\\nNew features\\nSQL updates\\nExtensibility updates\\nCost management updates\\nRelease notes change log\\n\\n\\nJuly 11, 2024 — Snowflake Data Clean Rooms Release Notes\\nSequenced template execution\\nMulti-factor authentication\\nRegister objects in a managed access schema\\nSupport for additional region\\nSingle-party SQL query\\n\\n\\nJuly 11, 2024 — Snowflake connectors\\nSnowflake Connector for PostgreSQL\\nSnowflake Connector for MySQL\\n\\n\\nJuly 08-12, 2024  — 8.25 Release Notes\\nSQL updates\\nRelease notes change log\\n\\n\\nJuly 03, 2024 — External network access in Streamlit in Snowflake –— General Availability\\nJuly 03, 2024 — Data pipelines: Support for Iceberg tables with dynamic tables and streams –— Preview\\nJuly 02, 2024 — Snowsight Release Notes\\nSnowflake Notebooks external access —– Preview\\n\\n\\nJuly 01-03, 2024 — 8.24 Release Notes\\nSecurity updates\\nVirtual warehouse updates\\nRelease notes change log\\n\\n\\nJuly 01, 2024 — Snowsight Release Notes\\nNew homepage for Snowsight —– Preview\\n\\n\\nJune 28, 2024 — Snowsight Release Notes\\nSnowsight and Multi-Factor Authentication reminders –— General Availability\\n\\n\\nJune 28, 2024 — New geospatial H3 functions — General Availability\\nJune 28, 2024 — Custom UI in Streamlit in Snowflake –— Preview\\nJune 27, 2024 — Document AI release notes\\nJune 26, 2024 — Cost Management Release Notes\\nOrganization Overview Page —– General Availability\\n\\n\\nJune 25, 2024 — New TO_QUERY table function\\nJune 25, 2024 — Snowflake Data Clean Rooms Release Notes\\nProvider-run analyses\\nConsumer-defined templates\\nGranular access controls for tables & templates\\nActivating results across regions\\nSQL Template enhancement\\n\\n\\nJune 24, 2024: Time Travel for hybrid tables –— Preview\\nJune 21, 2024: Document AI release notes\\nJune 17-30, 2024 — 8.23 Release Notes\\nSecurity updates\\nSQL updates\\nData governance updates\\nData pipelines updates\\nSnowflake Native App Framework\\nRelease notes change log\\n\\n\\nJune 17, 2024 — New LLM helper functions - TRY_COMPLETE and COUNT_TOKENS\\nNew SQL function\\n\\n\\nJune 15, 2024 — Anomaly Detection\\nJun 11, 2024 — Sharing data in non-secure views –— Preview\\nJune 11, 2024 — Snowflake Data Clean Rooms Release Notes\\nAdditional supported regions — General Availability\\nGranular access management for Snowflake data — General Availability\\nChoosing a warehouse when running an analysis — General Availability\\nSupport for multiple custom templates in web app — General Availability\\n\\n\\nJune 10-15, 2024 — 8.22 Release Notes (with behavior changes)\\nBehavior change bundles\\nSQL updates\\nVirtual warehouse updates\\nRelease notes change log\\n\\n\\nJune 10, 2024 — Iceberg tables — General Availability\\nJune 05, 2024 — New geospatial functions in preview\\nNew geospatial functions available –— Preview\\n\\n\\nJune 03-06, 2024 — Summit announcements\\nNew features\\nExtensibility updates\\nSnowsight updates\\n\\n\\nJune 03, 2024 — New EMBED_TEXT_1024 function for 1024 dimensional output vectors\\nNew SQL function\\n\\n\\nJune 3, 2024 — Entity-Level Privacy Release Notes\\nAggregation policies with entity-level privacy — General Availability\\n\\n\\n\\n\\nFor earlier release notes (through the current year and the prior two years), click Previous Releases in the menu on the left.\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentSnowflake EditionsSnowflake ReleasesOverview of Key FeaturesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide-getting-started', 'title': 'Getting Started - Snowflake Documentation', 'language': 'en'}, page_content='Getting Started - Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBefore you BeginLogging InSnowsight TourConceptsTutorialsSample DataContacting SupportGetting StartedBegin your adventure: a comprehensive guide to getting started with Snowflake and making the most of its features and benefits.Introduction to SnowflakeBasic information and instructions for first-time users of Snowflake.Before you beginMake sure that you meet the prerequisites for using our applications that connect to Snowflake.Logging into SnowflakeLearn how to use Snowsight or SnowSQL to log into Snowflake.Snowsight Quick TourLearn about the features in Snowsight that you can use to manage database objects and query data.Learn the Key ConceptsThese topics introduce the Snowflake architecture and basic featuresArchitectureLearn about Snowflake’s architecture, how data is stored, and how queries are processed.Cloud PlatformsLearn about the cloud platforms that are supported for use with Snowflake.Cloud RegionsLearn about the cloud provider regions in which Snowflake accounts are hosted.EditionsLearn about the different editions provided by Snowflake and the features and levels of service offered with each edition.ReleasesLearn about our process for deploying new releases.FeaturesLearn about some of the key features of Snowflake.Manage Your DataData LifecycleLearn about how data is organized, stored, queried, and managed.Continuous Data ProtectionLearn about how your data is protected and how you can access and recover your data.Regulatory ComplianceLearn about our commitment to meeting industry-standard regulatory compliance requirements.Start LearningTUTORIALSnowflake in 20 MinutesLearn how to create Snowflake objects, load data, and query data.TUTORIALCreate Users and Grant RolesLearn how to create a user and grant roles to it using SQL.TUTORIALLoad and Query Sample Data Using SQLLearn how to load and query Tasty Bytes sample data using SQL.TUTORIALLoad and Query Sample Data Using Snowpark PythonLearn how to load and query Tasty Bytes sample data using Python.TUTORIALLoad Data from Cloud Storage (Amazon S3)Learn how to load a table from an S3 bucket.TUTORIALLoad Data from Cloud Storage (Microsoft Azure)Learn how to load a table from an Azure container.TUTORIALLoad Data from Cloud Storage (Google)Learn how to load a table from a GCS bucket.TUTORIALBulk Loading from a Local File System Using COPYLearn how to load data from files in your local file system into a table.TUTORIALBulk Loading from Amazon S3 Using COPYLearn how to load data from files in an existing Amazon Simple Storage Service (Amazon S3) bucket into a table.TUTORIALJSON BasicsLearn how to load JSON data from files, query the data, and flatten and store the data in a table.TUTORIALLoading JSON Data into a Relational TableLearn more about how to store JSON objects, extract JSON elements, and transform those elements into table columns.TUTORIALLoading and Unloading Parquet DataLearn how to upload Parquet data by transforming elements of a staged Parquet file directly into table columns.Sample Data SetsSnowflake provides sample data sets, such as the industry-standard TPC-DS and TPC-H benchmarks, for evaluating and testing a broad range of Snowflake’s SQL support.See allSNOWFLAKE_SAMPLE_DATAThe sample database, SNOWFLAKE_SAMPLE_DATA, is identical to the databases that you create in your account, except that it is read-only.TPC-DSTPC-DS models the decision support functions of a retail product supplier. The supporting schema contains vital business information, such as customer, order, and product data.TPC-HTPC-H is a decision support benchmark. It consists of a suite of business-oriented ad hoc queries and concurrent data modifications.Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/reference', 'title': 'Reference | Snowflake Documentation', 'language': 'en'}, page_content='Reference | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSQL command referenceFunction and stored procedure referenceClass referenceScripting referenceGeneral referenceAPI referenceReferenceOverview\\n\\nReference¶\\nReference information on various areas of Snowflake.\\n\\nSQL data types referenceReference for SQL data types.\\n\\nSQL command referenceReference for SQL commands.\\n\\nFunction and stored procedure referenceReference for SQL functions.\\n\\nSQL class referenceReference for SQL classes.\\n\\nSnowflake Scripting referenceReference for Snowflake Scripting constructs.\\n\\nGeneral referenceReference material on other subjects.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-udafs', 'title': 'Creating User-Defined Aggregate Functions (UDAFs) for DataFrames in Python | Snowflake Documentation', 'language': 'en'}, page_content='Creating User-Defined Aggregate Functions (UDAFs) for DataFrames in Python | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIJavaPythonSetting Up a Development EnvironmentCreating a SessionWriting Snowpark Code in Python WorksheetsUsing DataFramesSnowpark pandas APICreating User Defined FunctionsCreating User Defined Table FunctionsCreating User Defined Aggregate FunctionsCreating Stored Procedures for DataFramesCalling Functions and Stored ProceduresTraining Machine Learning Models with Snowpark PythonWriting Tests for Snowpark PythonLocal Testing FrameworkTutorial: Testing Python SnowparkTroubleshootingPython API ReferenceScalaMachine LearningSnowflake MLSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewReferenceAPI ReferenceDeveloperSnowpark APIPythonCreating User Defined Aggregate Functions\\n\\nCreating User-Defined Aggregate Functions (UDAFs) for DataFrames in Python¶\\nYou can use Snowpark Python APIs to create and call user-defined aggregate functions (UDAFs). A UDAF takes one or more rows as input and\\nproduces a single row of output. It operates on values across multiple rows to perform mathematical calculations such as sum, average,\\ncounting, finding minimum or maximum values, standard deviation, and estimation, as well as some non-mathematical operations.\\nTo create and register a UDAF with Snowpark, you need to:\\n\\nImplement a UDAF handler.\\nThe handler contains the UDAF’s logic. A UDAF handler must implement functions that Snowflake will invoke at runtime when the UDAF is\\ncalled. For more information, see Implementing a handler.\\n\\nRegister the UDAF and its handler in the Snowflake database.\\nOnce you’ve registered the UDAF, you can call it from SQL or by using the Snowpark API. You can use the Snowpark API to register the\\nUDAF and its handler. For more information about registering, see Registering a UDAF.\\n\\n\\nYou can also create your own UDAFs using SQL as described in Python user-defined aggregate functions.\\n\\nImplementing a handler¶\\nAs described in Interface for aggregate function handler, a UDAF handler class must implement methods that Snowflake invokes\\nwhen the UDAF is called. You can use the class you write as a handler whether you’re registering the UDAF with the Snowpark API or\\ncreating it with SQL using the CREATE FUNCTION statement.\\nYour UDAF handler class implements methods listed in the following table, which Snowflake invokes at run time. See\\nexamples in this topic.\\n\\n\\n\\n\\n\\n\\n\\nMethod\\nRequirement\\nDescription\\n\\n\\n\\n__init__\\nRequired\\nInitializes the internal state of an aggregate.\\n\\naggregate_state\\nRequired\\nReturns the internal state of an aggregate.\\n\\nThe method must have a @property decorator.\\nAn aggregate state object can be any Python data type serializable by the\\nPython pickle library.\\nFor simple aggregate states, use a primitive Python data type. For more complex aggregate states, use\\nPython data classes.\\n\\n\\n\\naccumulate\\nRequired\\nAccumulates the state of the aggregate based on the new input row.\\n\\nmerge\\nRequired\\nCombines two intermediate aggregated states.\\n\\nfinish\\nRequired\\nProduces the final result based on the aggregated state.\\n\\n\\n\\n\\n\\nRegistering a UDAF¶\\nOnce you’ve implemented a UDAF handler, you can use the Snowpark API to register the UDAF on the Snowflake database. Registering the UDAF\\ncreates the UDAF so that it can be called.\\nYou can register the UDAF as a named or anonymous function, as you can for a scalar UDF. For related information about registering a scalar\\nUDF, see Creating an Anonymous UDF and Creating and Registering a Named UDF. When you register a UDAF,\\nyou specify parameter values that Snowflake needs to create the UDAF.\\nYou can register the function using the following functions and methods:\\n\\nUse the register method or udaf function, specifying the name of your handler class, along with arguments to define the\\nfunction. You can also use udaf as a @udaf decorator on the handler class.\\nFor reference information on these, see the following:\\n\\nsnowflake.snowpark.functions.udaf\\nsnowflake.snowpark.udaf.UDAFRegistration.register\\n\\n\\nUse the register_from_file function, pointing to a Python file or zip file containing Python source code.\\nFor the function reference, see snowflake.snowpark.udaf.UDAFRegistration.register_from_file.\\n\\n\\n\\n\\nExamples¶\\n\\nCreate a UDAF with a return value and a single parameter¶\\nPython code in the following handler example supports a sum_int UDAF that receives a single integer argument, adds the value\\nacross rows and returns the result.\\n\\nRegister the function¶\\nimport snowflake.snowpark as snowpark\\nfrom snowflake.snowpark.types import IntegerType\\nfrom snowflake.snowpark.functions import udaf\\ndef main(session: snowpark.Session):\\nclass PythonSumUDAF:\\n  def __init__(self):\\n    # This aggregate state is a primitive Python data type.\\n    self._partial_sum = 0\\n\\n  @property\\n  def aggregate_state(self):\\n    return self._partial_sum\\n\\n  def accumulate(self, input_value):\\n    self._partial_sum += input_value\\n\\n  def merge(self, other_partial_sum):\\n    self._partial_sum += other_partial_sum\\n\\n  def finish(self):\\n    return self._partial_sum\\nsum_udaf = udaf(PythonSumUDAF, name=\"sum_int\", replace=True, return_type=IntegerType(), input_types=[IntegerType()])\\n\\nCopy\\n\\n\\nCall the function¶\\nPython code in the following example invokes the sum_int UDAF with a DataFrame.\\ndf = session.create_dataframe([[1, 3], [1, 4], [2, 5], [2, 6]]).to_df(\"a\", \"b\")\\nresult = df.agg(sum_udaf(\"a\")).collect()\\nprint(result.collect())\\n\\nCopy\\n\\n\\n\\nCreate a UDAF with a return value and two parameters¶\\n\\nRegister the function¶\\nPython code in the following handler example supports a sum_int UDAF that receives two integer arguments, adds the argument\\nvalues together across rows and returns the result.\\nimport snowflake.snowpark as snowpark\\nfrom snowflake.snowpark.types import IntegerType\\nfrom snowflake.snowpark.functions import udaf\\ndef main(session: snowpark.Session):\\n  class PythonSumUDAF:\\n    def __init__(self):\\n      self._partial_sum = 0\\n\\n    @property\\n  def aggregate_state(self):\\n    return self._partial_sum\\n\\n  def accumulate(self, input_value, input_value2):\\n    self._partial_sum += input_value + input_value2\\n\\n  def merge(self, other_partial_sum):\\n    self._partial_sum += other_partial_sum\\n\\n  def finish(self):\\n    return self._partial_sum\\nsum_udaf = udaf(PythonSumUDAF, name=\"sum_int\", replace=True, return_type=IntegerType(), input_types=[IntegerType(), IntegerType()])\\n\\nCopy\\n\\n\\nCall the function¶\\nPython code in the following example invokes the sum_int UDAF with a DataFrame.\\ndf = session.create_dataframe([[1, 3], [1, 4], [2, 5], [2, 6]]).to_df(\"a\", \"b\")\\nresult = df.agg(sum_udaf(\"a\", \"b\"))\\nprint(result.collect())\\n\\nCopy\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageImplementing a handlerRegistering a UDAFExamplesRelated contentPython user-defined aggregate functionsCreating User-Defined Functions (UDFs) for DataFrames in PythonUser-defined functions overviewLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/tutorials', 'title': 'Tutorials - Snowflake Documentation', 'language': 'en'}, page_content='Tutorials - Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusChoose CategoryFeaturedGetting StartedData EngineeringPartner IntegrationsData Science & MLApp DevelopmentSolution ExamplesCybersecurityTasty BytesArchitecture PatternsStreamlitDevOpsSee All (63)Browse TutorialsFollow along with our tutorials to get you up and running with the Snowflake Data Cloud297 Tutorials25 MinutesCommon setup for Snowflake Python API tutorialsThis tutorial provides instructions for the common setup required for all Snowflake Python API tutorials.SnowflakeGetting StartedApp DevelopmentData Engineering20 MinutesCommon Setup for Snowpark Container Services TutorialsThis tutorial provides instructions for the common setup required for all Snowpark Container Services tutorials.App Development30 MinutesCreate users and grant rolesIn this tutorial you will learn how to create users and grant roles with SQL commands.SnowflakeGetting Started45 MinutesGetting started with hybrid tablesIn this tutorial you will learn how to create and use hybrid tables.SnowflakeGetting StartedHybrid TablesUnistore30 MinutesLoad and query sample data using Snowpark PythonIn this tutorial you will learn how to load and query Tasty Bytes sample data using Snowpark Python.SnowflakeGetting StartedSnowpark30 MinutesLoad and query sample data using SQLIn this tutorial you will learn how to load and query Tasty Bytes sample data using SQL.SnowflakeGetting Started30 MinutesLoad data from cloud storage: Amazon S3In this tutorial you will learn how to use SQL commands to load data from cloud storage.SnowflakeGetting Started30 MinutesLoad data from cloud storage: GCSIn this tutorial you will learn how to use SQL commands to load data from cloud storage.SnowflakeGetting Started30 MinutesLoad data from cloud storage: Microsoft AzureIn this tutorial you will learn how to use SQL commands to load data from cloud storage.SnowflakeGetting Started20 MinutesSnowflake in 20 minutesIn this tutorial you will learn how to interact with Snowflake using SnowSQL.SnowflakeGetting Started15 MinutesTutorial 1: Build a simple search application with Cortex SearchThis tutorial walks through a basic end-to-end setup for using Cortex Search on a dataset consisting of AirBnb listings.Cortex SearchGetting Started20 MinutesTutorial 1: Create a database, schema, table, and warehouseCreate and manage a Snowflake database, schema, table, and warehouse using the Snowflake Python API.SnowflakeGetting StartedApp DevelopmentData Engineering60 MinutesTutorial 1: Create a Snowpark Container Services ServiceCreate an introductory Snowpark Container Service that echoes prompts.App Development15 MinutesTutorial 2: Build a simple chat application with Cortex SearchThis tutorial walks through building a basic chatbot with Cortex Search and LLM functions on a dataset consisting of TED Talk transcripts.Cortex SearchGetting Started60 MinutesTutorial 2: Create a Snowpark Container Services Job ServiceCreate a Snowpark Container Service Job.App Development20 MinutesTutorial 2: Create and manage tasks and task graphs (DAGs)Create and manage Snowflake tasks and task graphs (DAGs) using the Snowflake Python API.SnowflakeGetting StartedApp DevelopmentData Engineering30 MinutesTutorial 3: Build a PDF chatbot with Cortex SearchThis tutorial walks through an end-to-end setup for creating a Chatbot using Cortex Search on a PDF dataset consisting of Federal Open Market Committee meeting minutes.Cortex SearchGetting Started30 MinutesTutorial 3: Create and manage Snowpark Container ServicesCreate and manage components in Snowpark Container Services using the Snowflake Python API.SnowflakeGetting StartedApp DevelopmentData Engineering60 MinutesTutorial 3: Service-to-Service CommunicationsCreate a Snowpark Container Service that performs service-to-service communication.App Development60 MinutesTutorial 4: Create a service with a block storage volume mountedCreate a Snowpark Container Service that uses a block storage volume.App Development60 MinutesTutorial 5: Configure and test service endpoint privileges Create a Snowpark Container Service that uses a block storage volume.App Development30 MinutesTutorial: Bulk loading from a local file system using COPYThis tutorial describes how to load data from files in your local file system into a table.Getting Started:30 MinutesTutorial: Bulk loading from Amazon S3 using COPYThis tutorial describes how to load data from files in an existing Amazon Simple Storage Service (Amazon S3) bucket into a table.Getting Started30 MinutesTutorial: Create a document processing pipeline with Document AIThis tutorial describes how to prepare a Document AI model build and create a processing pipeline using streams and tasks.Document AIData Science & AIAI60 MinutesTutorial: Create a Snowflake Native App with Snowpark Container ServicesThis tutorial describes how to create a Snowflake Native App with Snowpark Container Services using the Snowflake Native App Framework and Snowpark Container Services.App Development20 MinutesTutorial: Create your first Iceberg tableIn this tutorial you will learn how to create, update, and query a Snowflake-managed Iceberg table.SnowflakeIcebergData Lake30 MinutesTutorial: Developing a Snowflake Native App with the Snowflake Native App FrameworkThis tutorial describes how to use the Snowflake Native App Framework to create a Snowflake Native App to share data and related business logic with other Snowflake accounts.App Development30 MinutesTutorial: Get started with logging and tracingThis tutorial introduces the basics of emitting, collecting, and querying log and trace data from function and procedure handler code.App Development30 MinutesTutorial: Get started with the web app of a Snowflake Data Clean RoomThis tutorial describes how to use a Snowflake Data Clean Room to collaborate within a privacy-preserving environment.Data Sharing40 MinutesTutorial: Getting started with BudgetsIn this tutorial you will learn how to activate and set up the account budget and create a custom budget.SnowflakeCost MonitoringGetting StartedAudit30 MinutesTutorial: Getting started with data metric functionsIn this tutorial you will learn how to use a custom data metric function to measure data quality.SnowflakeData GovernanceData Quality30 MinutesTutorial: Improve Workload Performance with the Query Acceleration ServiceIn this tutorial you will learn how to leverage the Query Acceleration Service (QAS) to improve your overall workload performance.Data Engineering20 MinutesTutorial: JSON basics for SnowflakeIn this tutorial you will learn the basics of using JSON with Snowflake.Getting Started20 MinutesTutorial: Loading and unloading Parquet dataLoading and unloading Parquet dataGetting Started20 MinutesTutorial: Loading JSON data into a relational tableLoading JSON data into a relational table.Getting Started60 MinutesTutorial: Native SDK for Connectors Java TemplateIn this tutorial you will learn how to use the Native SDK for Connectors template to accelerate initial development of a Connector Native Application.SnowflakeConnectorNative Sdk30 MinutesTutorial: Testing Python SnowparkThis tutorial introduces the basics of testing your Snowpark Python code.App Development30 MinutesTutorial: Snowflake ServiceNow® data ingestion connector installationStep-by-step to set up a ServiceNow® connectorGetting Started,Connectors,Data Engineering40 MinutesWorking with the MySQL and PostgreSQL connectors for SnowflakeIn this tutorial you will connect to and import both MySQL and PostgreSQL data.SnowflakePostgresqlMysqlConnector0 MinutesA Comprehensive Guide: Creating GraphQL API on Top of Snowflake Using PropelThis guide teaches how to create a GraphQL API using Propel on Snowflake, focusing on setup, configuration, and development of a high-performance API.Getting Started71 MinutesA Comprehensive Guide: Ingesting Data into SnowflakeLearn how to ingest data into Snowflake with Python Connector, Streaming SDK, Snowpipe, Snowpark, and KafkaFeaturedGetting StartedData Engineering21 MinutesA Data Integration Guide: Load Banking Data into Snowflake with IBM DataStageLab to demonstrate ease of loading enterprise data into Snowflake through DataStage.FeaturedGetting StartedData EngineeringPartner Integrations132 MinutesA Dataiku and Snowflake Introduction to Data ScienceThis is an introduction to Dataiku and SnowflakeData Science & MLSolution ExamplesPartner IntegrationsData Science & AIFeatured21 MinutesA Dive Into Slowly Changing Dimensions with Snowpark and StreamSetsA Dive into Slowly Changing Dimensions with SnowparkData EngineeringPartner Integrations21 MinutesA Faster Path to Operational AI with Continual and SnowflakeBuild an operational, continually updating predictive model for customer churn with Snowflake and ContinualData Science & MLPartner Integrations30 MinutesA Getting Started Guide With Snowflake Arctic and Snowflake CortexThis guide provides the instructions for writing a Streamlit application that uses Snowflake Arctic for custom tasks like summarizing long-form text into JSON formatted output using prompt engineering and Snowflake Cortex task-specific LLM functions to perform operations like translate text between languages or score the sentiment of a piece of text.FeaturedGetting StartedData EngineeringApp Development22 MinutesA Getting Started Guide With Snowflake NotebooksThis guide provides the instructions on how to get started with your first Snowflake Notebook.FeaturedGetting Started31 MinutesA Guide to Visual Data Storytelling in Snowflake NotebooksThis guide provides the instructions on how to build visualizations in Snowflake Notebook.FeaturedGetting Started28 MinutesA Image Recognition App in Snowflake using Snowpark Python, PyTorch, Streamlit and OpenAIThis guide provides the instructions for writing image recognition applications in Snowflake using Snowpark for Python, PyTorch, Streamlit and OpenAI.Data Science & MLApp DevelopmentSolution Examples64 MinutesA No Code Approach to Machine Learning with Snowflake and DataikuThis is an introduction to Dataiku and SnowflakeData Science & MLSolution ExamplesPartner IntegrationsData Science & AI24 MinutesA Postman Tutorial for the Snowflake SQL APIExplore the Snowflake SQL API with PostmanFeaturedGetting StartedData EngineeringApp DevelopmentPartner Integrations111 MinutesAccelerate Data Transformation with the Telecom Data Cloud and InformaticaThis is a guide for getting started with Data Integration using Informatica Data Management CloudGetting Started18 MinutesAccelerate Transformations with Coalesce and SnowflakeAccelerate Transformations with Coalesce and SnowflakeData EngineeringPartner Integrations20 MinutesAccelerate Your GraphQL Development on Snowflake with HasuraInstantly Build GraphQL APIs on Your Snowflake Data with HasuraApp DevelopmentData EngineeringFeaturedGetting StartedPartner Integrations77 MinutesAccelerating Data Science with Snowflake and DataikuAccelerating Data Science with Snowflake and DataikuData Science & MLPartner Integrations92 MinutesAccelerating Data Teams with Snowflake and dbt Cloud Hands On LabBuild a dbt project and data pipeline with dbt Cloud and SnowflakeGetting StartedData EngineeringPartner IntegrationsMarketing128 MinutesAccelerating Machine Learning with Snowflake and DataRobotThis lab will walk you through how to use Snowflake and DataRobot to perform supervised machine learning.Data Science & MLPartner Integrations11 MinutesALTR Quickstart - Data Access ControlALTR Quickstart - Data Access Control. Connect ALTR to snowflake and create an automatically enforced role based data access policy.Cybersecurity10 MinutesAn Introduction to Tasty BytesThis is the Tasty Bytes Introduction and Data Foundation Quickstart guideTasty BytesGetting StartedFeatured31 MinutesAnalyze Data with R using Posit Workbench and SnowflakeAnalyze Data with R using Posit Workbench and SnowflakeGetting Started28 MinutesAnalyze PDF Invoices using Snowpark for Java and PythonThis is a guide to get familiar with Snowflake’s support for unstructured dataData Science & MLData EngineeringArchitecture PatternsSolution Examples36 MinutesAnalyzing real estate properties using StreamlitAnalyzing real estate properties with StreamlitApp DevelopmentStreamlit60 MinutesAnalyzing SAP Accounts Payables with NTT DATA\\'s Snowflake Native App and FivetranThis is the Fivetran NTT SAP AP GuideGetting Started23 MinutesAttaining Consumer Insights with Snowflake and Microsoft Power BIIn this lab, you’ll learn how to access all relevant data from a single source and turn data into insights through the Microsoft PowerBI integration.Partner Integrations43 MinutesAuto-Ingest Twitter Data into SnowflakeAuto-Ingest Twitter Data into SnowflakeData EngineeringApp DevelopmentSolution ExamplesMarketing92 MinutesAutomating Data Pipelines to Drive Marketing Analytics with Snowflake & FivetranAutomating Data Pipelines to Drive Marketing Analytics with Snowflake & FivetranData EngineeringPartner IntegrationsMarketing56 MinutesAutomating Document Processing Workflows With Document AIThis guide will provide step-by-step details for automating document processing workflows with Document AIGetting Started85 MinutesAutoML with Snowflake and H2O Driverless AIThis lab will walk you through how to use Snowflake and H2O to perform supervised machine learning.Data Science & MLPartner Integrations30 MinutesAWS Cloudtrail IngestionAWS CloudTrail is an AWS service that helps you enable operational and risk auditing, governance, and compliance of your AWS account. By ingesting and analyzing these logs in Snowflake, practitioners are able to gain analytical insights and work toward securing their environments at scale. This quickstart is a guide to ingesting and processing AWS CloudTrail events into snowflake. It provides detailed instructions for configuring an automated ingestion and processing pipeline as well as example queries for analytics, threat detection and posture management.CybersecuritySolution ExamplesPartner Integrations27 MinutesAWS S3 Access Logs IngestionThis quickstart is a guide to ingesting and processing S3 Access Logs into snowflake. It provides detailed instructions for configuring an automated ingestion and processing pipeline as well as example queries for data loss prevention and incident response.CybersecuritySolution ExamplesPartner Integrations30 MinutesAWS VPC Flow Logs IngestionThis tutorial is a guide for ingestion AWS VPC Flowlogs into Snowflake. It demonstrates configuration of VPC flowlogs on AWS, ingestion using an external stage with Snowpipe and sample queries for CSPM and threat detection.CybersecuritySolution ExamplesPartner Integrations24 MinutesBuild a Custom API in Java on AWSA guide to building and running a custom API (in Java) Powered by Snowflake and AWSGetting StartedApp DevelopmentArchitecture PatternsSolution Examples23 MinutesBuild a Custom API in Python and FlaskA guide to building and running a custom API Powered by Snowflake and Python/FlaskGetting StartedApp DevelopmentArchitecture PatternsSolution Examples24 MinutesBuild a Custom API in Python on AWSA guide to building and running a custom API (in Python) Powered by Snowflake and AWSGetting StartedApp DevelopmentArchitecture PatternsSolution Examples35 MinutesBuild a Data App and run it on Snowpark Container ServicesBuild a Data App and run it on Snowpark Container ServicesGetting Started135 MinutesBuild a Data App with SnowflakeA guide to building and running a data application powered by Snowflake and Node.jsGetting StartedApp DevelopmentArchitecture PatternsSolution Examples28 MinutesBuild a Data Application with Hybrid TablesBuild a data application using Snowflake Hybrid Tables for transactional processingGetting Started60 MinutesBuild a Native App with SPCSThis is a sample Snowflake GuideGetting Started35 MinutesBuild a Pull-based Native Connector in PythonOverview of building Snowflake native connectors using Python, Snowpark and Native Apps. Example connector connects to GitHub issues API.ConnectorsSolution ExamplesPartner Integrations0 MinutesBuild a ReactJS app with ThoughtSpot and SnowflakeBuild an interactive react app with ThoughtSpot and SnowflakeApp DevelopmentPartner Integrations98 MinutesBuild a Recommendation Engine with Amazon SageMakerBuild a Recommendation Engine with AWS SageMaker and SnowflakeData Science & MLSolution ExamplesPartner Integrations75 MinutesBuild a Retrieval Augmented Generation (RAG) based LLM assistant using Streamlit and Snowflake CortexStep-by-step guide on how to create a RAG app using Snowflake Cortex and Streamlit.FeaturedGetting StartedData ScienceGen AI40 MinutesBuild a Snowflake Native App to Analyze Chairlift Sensor DataThis Snowflake Native App demonstrates how a chairlift manufacturer can build a native app to provide chairlift customers with analytics on purchased chairlifts by using sensor data collected from the chairlifts owned by the customer.Getting Started27 MinutesBuild an Ad Tag in Python on AWSA guide to building and running a simple ad tag (in Python) Powered by Snowflake and AWSGetting StartedApp DevelopmentArchitecture PatternsSolution ExamplesMarketing90 MinutesBuild and Secure Multi-Tenant Data Applications with Snowflake and SigmaSnowflake Summit 2023 HoL with SigmaSnowflake60 MinutesBuild Customer Facing Applications Using Sigma and SnowflakeThis guide will lead you through the process of connecting Sigma to a Snowflake environment and building an application that leverages the data in Snowflake. This guide additionally highlights unique end user capabilities when Sigma is embedded in an application.App DevelopmentPartner Integrations26 MinutesBuild Data Lakes using Apache Iceberg with Snowflake and AWS GlueThis guide shows how to integrate Snowflake with External Volumes to S3 and the Glue Data Catalog for use with Iceberg tablesData Engineering60 MinutesBuild Data Products and a Data Mesh with dbt CloudBuild Data Products and a Data Mesh with dbt CloudGetting StartedData Engineering65 MinutesBuild GenAI Inpainting and Hybrid Table App in Snowpark Container ServicesThrough this quickstart guide, you will explore GenAI inpainting and Hybrid tables with everything running within a Streamlit app in Snowpark Container ServicesGetting StartedData Science & AIStreamlit60 MinutesBuild LLM App Powered By NVIDIA on Snowpark Container ServicesBuild LLM App Powered By NVIDIA on Snowpark Container ServicesGetting StartedLlmSnowpark Container ServicesNvidiaSnowflake Native App66 MinutesBuild predictive features without MLOps using RudderStack and SnowflakeCreate predictive features with RudderStack and Snowflake without MLOpsData Science & AIGetting Started71 MinutesBuild Rag Based Equipment Maintenance App Using Snowflake CortexBuild Rag Based Equipment Maintenance App Using Snowflake CortexCortexGetting StartedApp Development53 MinutesBuilding a Data ApplicationBuilding a Single-Page Data Application on SnowflakeApp Development43 MinutesBuilding a data application with Snowflake Marketplace, Snowpark and StreamlitThis is a sample Snowflake GuideFeaturedApp Development69 MinutesBuilding a Real-Time Data Vault in SnowflakeBuilding a Real-time Data Vault in SnowflakeData EngineeringArchitecture PatternsSolution ExamplesMarketing65 MinutesBuilding and deploying a time series forecast with Hex + SnowflakeThis lab will walk you through how to use Snowflake and Hex.Data Science & MLPartner Integrations37 MinutesBuilding Dynamic Tables in Snowflake with CoalesceBuilding Dynamic Tables in SNowflake with CoalesceData Engineering58 MinutesBuilding ML Models In Minutes With Hex And Snowflake ML FunctionsThis lab will walk you through how to use Snowflake ML Functions and Hex.Data Science & MLPartner Integrations35 MinutesCall Centre Analytics with Snowflake Cortex LLM and Snowpark Container ServicesThis guide will provide step-by-step details for building call centre analytics using Snowflake Cortex LLM functions and Snowpark Container ServicesGetting StartedData ScienceGen AI65 MinutesChurn modeling using Snowflake and HexThis lab will walk you through how to use Snowflake and Hex.Data Science & MLPartner Integrations140 MinutesCloud Native Data Engineering with Matillion and SnowflakeData EngineeringPartner Integrations46 MinutesCloud-native Database Modeling with SqlDBMBegin modeling your Snowflake database online with SqlDBMData EngineeringPartner Integrations6 MinutesCoherent Spark Connector - use business logic from Excel spreadsheets in SnowflakeUse business logic from Excel in Snowflake with Coherent SparkGetting Started9 MinutesCollibra Data Governance with SnowflakeThis is a sample Snowflake Guide to show how a user can work with Collibra in integration with Snowflake.Getting Started32 MinutesCommunity Detection using RelationalAIThis guide shows how to use RelationalAI and Snowflake to create a social graph and detect customer communities from retail transaction data.Data ScienceGraph AnalysisRelationalaiCommunity Detection24 MinutesCreate Eureka Moments with data.world and SnowflakeThis guide will help you connect Snowflake to data.world, where you can join, query, and share your dataGetting Started80 MinutesCross Cloud Business Continuity With SnowflakeThis guide demonstrates Snowflake replication, failover and client redirect features for business continuityBusiness ContinuityData EngineeringApp Development28 MinutesCustomer Reviews Analytics using Snowflake CortexThis guide demonstrates how we can leverage Snowflake Cortex to enable customer review analytics.Data ScienceGen AIData Science & AITasty BytesCortex54 MinutesData Engineering Pipeline With Snowpark PandasThis guide will provide step-by-step details for doing data engineering with Snowpark Pandas APIGetting Started130 MinutesData Engineering Pipelines with Snowpark PythonThis guide will provide step-by-step details for building data engineering pipelines with Snowpark PythonFeaturedData Engineering46 MinutesData Engineering with Apache Airflow, Snowflake, Snowpark, dbt & CosmosThis guide shows you how to build a Data Pipeline with Apache Airflow that manages DBT model transformations and conducts data analysis with Snowpark, all in a single DAGData EngineeringArchitecture PatternsPartner Integrations54 MinutesData Engineering with Snowpark Python and dbtThis guide will provide step-by-step details for building data engineering pipelines with Snowpark Python and dbtData EngineeringPartner Integrations46 MinutesData Mapping in Snowflake Native Apps using StreamlitThis guide will provide step-by-step details for building a data mapping requirement in Snowflake Native Apps and StreamlitGetting StartedFeaturedData EngineeringNative AppStreamlit21 MinutesData Quality Testing with SodaFollow this guide to set up and run a Soda scan for data quality in Snowflake.Getting StartedData Engineering50 MinutesDeploying Custom Models To Snowflake Model RegistryThrough this quickstart guide, you will learn how to deploy a custom model to the Snowflake Model Registry.Getting Started45 MinutesDeploying Models from AzureML and Sagemaker to Snowpark MLLearning how to easily access models from AzureML and Amazon Sagemaker then deploy them to Snowpark ML registriesFeaturedGetting StartedSnowparkAzureAWSStreamlitGenAIAIMLAzureML94 MinutesDevelop a Predictive Model using Snowflake and SigmaBuilding predictive modeling in Snowflake and SigmaData Science & MLSolution ExamplesPartner IntegrationsConnectors29 MinutesDeveloping React Native Application using Snowflake Hybrid TablesDeveloping Tasty Bytes React Native Data Application using Snowflake, Hybrid Tables, SQLAlchemy, FlaskTasty BytesGetting StartedApp DevelopmentHybrid TablesSql AlchemyFlaskReact31 MinutesDevOps: Database Change Management with schemachange and Azure DevOpsThis guide will provide step-by-step details for getting started with DevOps on Snowflake by leveraging schemachange and Azure DevOpsDevOps34 MinutesDevOps: Database Change Management with schemachange and GitHubThis guide will provide step-by-step details for getting started with DevOps on Snowflake by leveraging schemachange and GitHubDevOps54 MinutesDevOps: Database Change Management with schemachange and JenkinsThis guide will provide step-by-step details for getting started with DevOps on Snowflake by leveraging schemachange and JenkinsDevOps56 MinutesDevOps: Database Change Management with Terraform and GitHubThis quickstart will provide step-by-step details for getting started with DevOps on Snowflake by leveraging Terraform and GitHubDevOps66 MinutesEnd to End Model Development with Hex and SnowparkThis lab will walk you through how to use Snowflake and Hex.Data Science & MLPartner Integrations1 MinutesEnhancing Customer Experiences using Cortex Fine TuningBuild an automated customer support agent that showcases the power of Cortex Fine-Tuning and helps the Tasty Bytes team to respond with a highly accurate automated email to customer ticketsTasty BytesCortexLlmFinetuning65 MinutesEnrich Salesforce data with Snowflake to deliver your Customer 360This lab will walk you through how to use Snowflake, Salesforce and Tableau CRM to integrate data, and deliver Customer 360.Solution ExamplesPartner IntegrationsMarketing18 MinutesExploratory Data Analysis with Snowflake and DeepnoteDeepnote’s SQL + Python notebook, along with their suite of low-code tools, allow for rapid data exploration of the Snowflake warehouse.Data Science & MLPartner Integrations20 MinutesExtract Attributes from DICOM Files using Snowpark for Python and JavaThis is a guide to get familiar with Snowflake’s support for unstructured dataData EngineeringApp DevelopmentArchitecture PatternsSolution Examples38 MinutesExtracting Insights from Unstructured Data with Document AIWalkthrough extracting data from PDF Inspection Reports using Document AITasty BytesGetting Started50 MinutesFine-Tuning an LLM in Snowpark Container Services with AutoTrainFine-Tuning an LLM in Snowpark Container Services with AutoTrainGetting Started38 MinutesFivetran - Automate Salesforce Insights: Source, Target, Transformations, Dashboard...NO CODELab to demonstrate the ease of use to ingest data from Salesforce to Snowflake through Fivetran with insights dashboard from dbt models.Getting StartedMarketing20 MinutesGeocoding Address Data with MapboxForward and Reverse Geocoding with the Mapbox AppFeaturedGetting StartedData EngineeringPartner Integrations80 MinutesGeospatial Analysis using Geometry and Geography Data TypeThis is a sample Snowflake GuideGetting Started140 MinutesGeospatial Analytics for Retail with Snowflake and CARTOThis is a sample Snowflake GuideGetting Started80 MinutesGeospatial Analytics for Telecom with Snowflake and CartoGeospatial Analysis for Telecom with Snowflake and CartoGetting Started71 MinutesGetting Started - Processing HL7 FHIR Messages with SnowflakeThis is a guide to get familiarized with how to ingest and process HL7 FHIR JSON messages using Snowflake native features.Data EngineeringArchitecture PatternsSolution Examples57 MinutesGetting Started - Processing HL7 V2 Messages with SnowflakeThis is a guide to get familiarised with how to ingest and process HL7 V2.x messages using Snowflake Data Programmability features.Data EngineeringArchitecture PatternsSolution Examples46 MinutesGetting Started with Amazon Managed Service for Grafana and Streamlit On Real-time DashboardingThis guide will walk you through how to build real-time dashboards with AMG and Streamlit to track data ingested by Snowpipe StreamingGetting Started45 MinutesGetting Started with Amazon Sagemaker Canvas and Snowflake for Generative AI Use CasesThis is a quickstart for using Sagemaker with Canvas for Generative AIGetting StartedData Science & MLData EngineeringApp DevelopmentGen AILLMsAWSSagemakerCanvas95 MinutesGetting Started with Azure Data Factory and SnowflakeThis is a quickstart for using Snowflake with Azure Data FactoryGetting StartedData EngineeringMicrosoft60 MinutesGetting Started with Azure OpenAI, Streamlit and SnowflakeGetting Started with Azure OpenAI Streamlit and Snowflake using Snowpark External Access for image use casesFeaturedGetting StartedApp DevelopmentAzureOpenaiStreamlitGenAIAIMLImage55 MinutesGetting Started with Bedrock, Streamlit and SnowflakeGetting Started with Bedrock, Streamlit and Snowflake using Snowpark External AccessFeaturedGetting StartedApp DevelopmentBedrockAWSStreamlitGenAIAIML90 MinutesGetting Started with Cost and Performance OptimizationThis is a Snowflake Guide that walks through optimization features and options on the platformGetting Started48 MinutesGetting Started with Data Engineering and ML using Snowpark for Python and Snowflake NotebooksGetting Started with Data Engineering and ML using Snowpark for Python and Snowflake NotebooksFeaturedGetting StartedData Science & MLData EngineeringApp Development90 MinutesGetting Started with Data Engineering using Snowflake NotebooksThis guide will provide step-by-step details for building data engineering pipelines with Snowflake NotebooksFeaturedData EngineeringNotebooks90 MinutesGetting Started with DatameerThis is a broad introduction of Datameer and covers how to start, load and transform data, and share insights.Getting StartedPartner Integrations95 MinutesGetting Started with DCDF Data Architecture Incremental Processing & Logical PartitionsGetting Started with DCDF Data Architecture Incremental Processing/Logical PartitionsArchitecture Patterns56 MinutesGetting Started with Document AIThis guide will provide step-by-step details for getting started with Document AI.Getting Started24 MinutesGetting Started with Event Tables and AlertsGetting Started with Event Tables and AlertsFeaturedGetting StartedData EngineeringApp Development57 MinutesGetting Started With External Functions on AWSGetting Started With External Functions on AWSGetting StartedPartner Integrations82 MinutesGetting Started With External Functions on AzureGetting Started With External Functions on AzureGetting StartedPartner Integrations33 MinutesGetting Started With External Functions Using the Google Cloud ConsoleGetting Started With External Functions Using the Google Cloud ConsoleGetting StartedPartner Integrations60 MinutesGetting Started with Generative AI in Snowflake and StreamlitGetting Started with Generative AI in Snowflake and StreamlitFeaturedGetting StartedApp Development120 MinutesGetting Started with Geospatial - GeographyGetting Started with Geospatial - GeographyGetting Started100 MinutesGetting started with Geospatial AI and ML using Snowflake CortexCortex, Geospatial and Streamlit features for Machine Learning use casesGetting Started55 MinutesGetting Started with Google, Snowflake and Streamlit for Generative AIThis is a quickstart for using Snowflake with Google for Generative AIGetting StartedData Science & MLData EngineeringApp Development78 MinutesGetting Started with Horizon for Data Governance in SnowflakeThis guide is used to demonstrate the capabilities of Horizon for Data Governance. The walk-through consists of being able to ingest and monitor data pipelines in your organization, how to actively govern data including lineage and masking, and how Horizon can help with monitoring data in Snowflake using Snowsight.DatagovernanceDataengineering38 MinutesGetting Started with Hybrid TablesFollow this tutorial to learn the basics of hybrid tablesGetting Started53 MinutesGetting Started with Iceberg TablesThis is a sample Snowflake GuideGetting Started45 MinutesGetting Started with KeboolaGetting Started With KeboolaGetting StartedPartner Integrations16 MinutesGetting Started with Llama 3.1 405B: Build Custom LLMs with Synthetic Data Generation and DistillationThis guide provides the instructions for creating custom LLMs with synthetic data and distillation using Snowflake Cortex AI.Getting Started55 MinutesGetting Started with LLMOps using Snowflake Cortex and TruLensThis is a guide for getting started with LLMOps using Snowflake Cortex and TruLens.Getting Started45 MinutesGetting Started with Microsoft Purview and SnowflakeThis is a Quickstart for using Snowflake with Microsoft PurviewGetting StartedData GovernanceMicrosoft45 MinutesGetting Started with Power Apps and SnowflakeThis is a quickstart for using Microsoft Power Platform, Power Apps, Power Automate and SnowflakeGetting StartedData EngineeringMicrosoftPower AppsPower PlatformSnowflake28 MinutesGetting Started with PythonGetting Started with PythonGetting Started50 MinutesGetting started with Search OptimizationThis is a sample Snowflake GuideGetting Started90 MinutesGetting Started with SigmaThis is a guide to setting up Sigma on top of Snowflake and visualizing some sample data.Getting StartedPartner Integrations75 MinutesGetting Started with Snowflake - Zero to SnowflakeThis is a broad introduction of Snowflake and covers how to login, run queries, and load data.FeaturedGetting Started90 MinutesGetting Started with Snowflake and Amazon Data Firehose (ADF)This guide will walk you through how to apply a data-intensive approach to real-time data using Snowpipe Streaming and Amazon Data Firehose (ADF).Getting Started24 MinutesGetting Started with Snowflake as a backend for DjangoHow to get started with Snowflake as a backend for Django.Getting Started75 MinutesGetting Started with Snowflake CLIGetting Started with Snowflake CLIGetting Started71 MinutesGetting Started with Snowflake DevOpsGetting Started with Snowflake DevOpsDevOpsGetting StartedData Engineering19 MinutesGetting Started with Snowflake Dynamic TablesGetting Started with Snowflake Dynamic TablesGetting StartedMarketing101 MinutesGetting Started with Snowflake Feature StoreLearn how to get started and use Snowflake Feature Store to manage features and deploy models into productionGetting Started34 MinutesGetting Started with Snowflake MLThrough this quickstart guide, you will explore Snowflake for Machine Learning.Getting Started40 MinutesGetting Started with Snowflake ML Forecasting and ClassificationThis is an introduction to building with ML functions.Getting Started65 MinutesGetting Started with Snowflake ML Functions: Anomaly Detection & ForecastingGetting started with Anomaly Detection & Forecasting ML FunctionsGetting Started35 MinutesGetting Started with Snowflake Native AppsFollow this tutorial to get up and running with your first Snowflake Native ApplicationGetting Started19 MinutesGetting Started With Snowflake SQL APIGetting Started With Snowflake SQL APIGetting Started65 MinutesGetting Started with Snowpark and the Dataframe APILearn how to get started with Jupyter Notebooks on Snowpark and use the DataFrame API.Getting Started62 MinutesGetting Started with Snowpark for Machine Learning on AzureMLThis is a quickstart for using Snowpark for ML on AzureMLGetting StartedData ScienceData EngineeringAzureAzureMLSnowpark55 MinutesGetting Started with Snowpark for Machine Learning on SageMakerThis is a qucikstart for using Snowpark for ML on SageMakerGetting StartedData Science & MLData EngineeringApp Development55 MinutesGetting Started with Snowpark for Machine Learning on Vertex AIThis is a qucikstart for using Snowpark for ML on Vertex AIGetting StartedData Science & MLData EngineeringApp Development60 MinutesGetting Started with Snowpark for Python and FeastThis is a guide to assist users to setup Feast feature store with Snowflake for both online and offline feature stores.Getting StartedData Science & ML38 MinutesGetting Started With Snowpark for Python and StreamlitThis guide provides the instructions for writing a Streamlit application using Snowpark for Python and Cybersyn data from Snowflake Marketplace.FeaturedGetting StartedData EngineeringApp Development78 MinutesGetting Started with Snowpark for Python with Scikit-learnGetting Started with Snowpark for Python with Scikit-learnGetting Started33 MinutesGetting Started with Snowpark in Snowflake Python WorksheetsGetting Started with Data Engineering and ML using Snowpark for PythonFeaturedGetting StartedData EngineeringApp Development29 MinutesGetting Started with Snowpark pandasThrough this quickstart guide, you will learn how to use the Snowpark pandas APIGetting Started77 MinutesGetting Started With Snowpark ScalaThis guide provides the basic instructions for setting up a simple example using Snowpark.Getting Started29 MinutesGetting Started with SnowpipeThis guide will help you get started with Snowpipe.FeaturedGetting Started105 MinutesGetting Started with Snowpipe Streaming and Amazon MSKThis guide will walk you through how to apply a data-intensive approach to real-time data using Snowpipe Streaming and Amazon Managed Streaming for Apache Kafka (MSK).Getting Started80 MinutesGetting Started with Snowpipe Streaming and Azure Event HubsThis guide will walk you through how to apply a data-intensive approach to real-time data using Snowpipe Streaming and Azure Eventhubs.Getting Started24 MinutesGetting Started with SnowSQLGetting Started with SnowSQLGetting Started65 MinutesGetting Started with Streams & TasksLearn how to use streams and tasks with Snowflake.FeaturedGetting Started28 MinutesGetting Started with Sundeck OpsCenterThis Guide shows you how to quickly get started with Sundeck OpsCenter on Snowflake, a free Native App which providing a range of tools to better understand, manage, and alert on Snowflake activity and spend.Getting Started103 MinutesGetting Started with the Application Control Framework (ACF)A guide to building a Native App with consumer controls and limits.Getting StartedApp DevelopmentStreamlit90 MinutesGetting Started with Time Series Analytics for IoT in SnowflakeGetting Started with Time Series Analytics for IoT in SnowflakeGetting StartedTime SeriesFeaturedData EngineeringStreamingStreamlitSnowparkCortexSolution ExamplesSnowflake34 MinutesGetting Started with Time TravelUse Time Travel to query previous database states and undrop data.FeaturedGetting Started66 MinutesGetting Started with Unstructured DataThis is a guide to get familiar with Snowflake’s support for unstructured dataGetting StartedArchitecture Patterns27 MinutesGetting Started With User-Defined SQL FunctionsGuide to getting started with user-defined SQL functionsGetting Started81 MinutesGuida introduttiva al data engineering e al machine learning con Snowpark per PythonGetting Started with Data Engineering and ML using Snowpark for PythonFeaturedGetting StartedData Science & MLData EngineeringApp Development33 MinutesGuida introduttiva all\\'uso di Snowpark nei fogli di lavoro Snowflake PythonGuida introduttiva all’uso di Snowpark nei fogli di lavoro Snowflake PythonFeaturedGetting StartedData EngineeringApp Development76 MinutesHarness the Power of Snowflake with Informatica Intelligent Data Management CloudThis is a guide for getting started with Data Engineering using Informatica Data Management CloudData Science & MLData EngineeringPartner Integrations156 MinutesHorizon Access for Intra-Company Sharingexplore Horizon Access pillar features for intra-company sharing of data and appsData Sharing6 MinutesHow To Resolve Data with FullContact and SnowflakeThis is a sample Snowflake GuideGetting Started11 MinutesImprovado Composable \"Agentic\" Data Platform with SnowflakeGetting Started guide to show how users can use Improvado Composable “Agentic” Data Platform with SnowflakeGetting StartedPartner IntegrationsMarketing14 MinutesImproving ad performance with Facebook\\'s Conversion API (CAPI), using Hightouch and Snowflake.Improving ad performance with Facebook’s Conversion API (CAPI), using Hightouch and Snowflake.Solution ExamplesPartner Integrations30 MinutesIngest data from PubSub to Snowflake with Apache BeamIngest data from PubSub to Snowflake with Apache BeamData EngineeringArchitecture PatternsSolution ExamplesPartner Integrations32 MinutesIntegrating Tasty Bytes Location Recommendations ML model into the React Native Data ApplicationIntegrating Tasty Bytes Location Recommendations ML model into the React Native Data ApplicationTasty BytesGetting StartedApp DevelopmentData Science & ML74 MinutesIntro to Data Engineering with Snowpark PythonThis guide will provide step-by-step details for building data engineering pipelines with Snowpark PythonGetting StartedFeaturedData Engineering100 MinutesIntro to Snowpark Container ServicesThrough this quickstart guide, you will explore Snowpark Container ServicesGetting Started100 MinutesIntro to Snowpark Container Services with Snowflake Python APIsThrough this quickstart guide, you will explore Snowpark Container Services using Python APIGetting Started81 MinutesIntrodução à engenharia de dados e ML com Snowpark para PythonIntrodução à engenharia de dados e ML com Snowpark para PythonFeaturedGetting StartedData Science & MLData EngineeringApp Development10 MinutesIntrodução à Tasty BytesEste é o quickstart guide “Introdução à Tasty Bytes e base de dados“Tasty BytesGetting StartedFeatured33 MinutesIntrodução ao Snowpark em planilhas Python para SnowflakeIntrodução ao Snowpark em planilhas Python para SnowflakeFeaturedGetting StartedData EngineeringApp Development10 MinutesIntroduzione a Tasty BytesThis is the Tasty Bytes Introduction and Data Foundation Quickstart guideTasty BytesGetting StartedFeatured18 MinutesLead Scoring with ML-Powered ClassificationShows how marketers can predict the value of leads and newData Science & MLSolution ExamplesMarketing52 MinutesLearn How Alation Powers Data Intelligence on SnowflakeThis guide is used to help with the installation of the Alation Data Intelligence Platform. The walk-through consists of being able to find & trust data in your organization, how to actively govern data, and how Alation can help with when migrating data to Snowflake.Partner IntegrationsMarketing68 MinutesLeverage dbt Cloud to Generate ML ready pipelines using Snowpark pythonThis is a sample Snowflake GuideGetting Started59 MinutesMachine Learning on Unstructured Data with Saturn Cloud and SnowflakeMachine learning on unstructured data with Saturn Cloud and SnowflakeData Science & MLData EngineeringPartner Integrations7 MinutesMachine Learning with Snowpark and Apache AirflowThis is a sample Snowflake GuideData EngineeringArchitecture PatternsPartner Integrations85 MinutesMachine Learning with Snowpark Python: - Credit Card Approval PredictionMachine Learning with Snowpark Python for Credit Card Approval PredictionData Science & MLApp DevelopmentArchitecture PatternsSolution Examples28 MinutesMarketing Data Foundation Starter GuideMarketing Data Foundation Starter GuideMarketing87 MinutesMarketing Orchestration and Campaign Intelligence with Hightouch and SnowflakeMarketing Orchestration and Campaign Intelligence with Hightouch and SnowflakeMarketingGetting StartedPartner Integrations40 MinutesMongoDB to Snowflake CDC Data Replication with RiveryMongoDB to Snowflake CDC Data Replication with RiveryData EngineeringArchitecture PatternsSolution ExamplesPartner Integrations0 MinutesMonitor Schema Drift in Data Shares with SharewatchStep-by-step to set up Sharewatch ( a schema drift monitor for data shares)Data SharingDevOpsStreamlit40 MinutesNative Connector - Java (Pull Based)Overview of building Snowflake native connectors using Java snowpark and Native Apps. Example connector connects to GitHub issues API.ConnectorsSolution ExamplesPartner Integrations44 MinutesNative Connector - Java (Push Based)Overview of building Snowflake push based connectors using Java and Native Apps.ConnectorsSolution ExamplesPartner Integrations35 MinutesNLP and ML with Snowpark Python and Streamlit for Sentiment AnalysisThis is a sample Snowflake GuideGetting Started71 MinutesOperationalizing AI with Snowflake and DataRobotThis is a sample Snowflake Guide!!Data Science & ML14 MinutesParsing Semi-Structured Data with CoalesceParsing Semi-Structured Data (JSON / XML) with CoalesceData Engineering95 MinutesPerformance Optimization Techniques for Geospatial queriesPerformance optimization techniques for GeoGetting Started19 MinutesPredict Ad Impressions with ML-Powered AnalysisShows how advertisers can predict ad impressions using ML-powered analysis in SnowflakeData Science & MLSolution ExamplesMarketing75 MinutesPredicting Sporting Outcomes with Snowpark MLBuilding a Sports Prediction Model with Snowpark MLData Science & MLData Engineering81 MinutesPremiers pas avec le Data Engineering et le Machine Learning à l\\'aide de Snowpark for PythonPremiers pas avec le Data Engineering et le Machine Learning à l’aide de Snowpark for PythonFeaturedGetting StartedData Science & MLData EngineeringApp Development33 MinutesPremiers pas avec Snowpark dans les feuilles de calcul Python de SnowflakePremiers pas avec Snowpark dans les feuilles de calcul Python de SnowflakeFeaturedGetting StartedData EngineeringApp Development10 MinutesPrésentation de Tasty BytesPrésentation de Tasty\\xa0BytesTasty BytesGetting StartedFeatured24 MinutesPrice Optimization Using Snowflake Notebooks and Streamlit in SnowflakePrice Optimization Using Snowflake Notebooks and StreamlitTasty BytesGetting Started14 MinutesProcess Change Data Capture (CDC) data from Oracle to Snowflake Using StreamSetsCDC Data from Oracle to Snowflake in StreamSetsData EngineeringSolution Examples45 MinutesProcess PII data using Snowflake RBAC, DAC, Row Access Policies, and Column Level SecurityLearn how to process PII data using Snowflake RBAC, DAC, Row Access Policies, and Column Level SecurityCybersecurityArchitecture PatternsSolution ExamplesMarketing12 MinutesQuickstart Guide: Cybersyn Financial & Economic Essentials AppHow to access and use Cybersyn’s Financial & Economic Essentials App.Streamlit9 MinutesQuickstart Guide: Cybersyn Streamlit in Snowflake - Financial DemoHow to run custom Streamlit app in SnowflakeStreamlit30 MinutesReach and Frequency queries for advertising measurementShows common reach and frequency queries for advertising measurementData Science & MLSolution Examples45 MinutesRecommender Systems with RelationalAI\\'s Snowflake Native AppThis guide shows how to use RelationalAI and Snowflake to create a recommender system.Data ScienceGraph AnalysisRelationalai0 MinutesResource Optimization: Billing MetricsThis guide can be used to help customers setup and run queries pertaining to monitoring billing metrics that might help identify areas of over-consumption.Data Science & MLResource Optimization0 MinutesResource Optimization: PerformanceThis guide can be used to help customers setup and run queries pertaining to identifying areas where poor performance might be causing excess consumption, driven by a variety of factors.Resource Optimization0 MinutesResource Optimization: Setup & ConfigurationThis guide can be used to help customers setup and run queries pertaining to specific setup & configuration items that might be causing over-consumption.Resource Optimization0 MinutesResource Optimization: Usage MonitoringThis guide can be used to help customers setup and run queries pertaining to monitoring usage that might be causing over-consumption.Resource Optimization60 MinutesRunning Apache Airflow on SPCSRunning Apache Airflow on SPCSData EngineeringApp Development306 MinutesSAP Accounts Receivable to Snowflake using ADFExtract SAP data using Azure Data Factory and transform accounts receivable data in Snowflake.Solution Examples20 MinutesSeamless Machine Learning Workflows with Snowpark & DeepnotePrototype your ML models in Deepnote’s collaborative, multi-language notebook. Then, use Snowpark to productionize models without moving your data out of the warehouse or changing your code.Data Science & MLPartner Integrations10 MinutesSecure Your Organization with Security Analytics Using Snowflake and SigmaLearn how to monitor your Snowflake security posture using SigmaCybersecurityData Warehousing23 MinutesSecurity Analytics with Wiz and Snowflakelearn about the Wiz and Snowflake integration for analyzing Wiz data in SnowflakeCybersecuritySolution ExamplesPartner Integrations44 MinutesSentiment Analysis Using Snowflake Cortex AI on Iceberg TablesThis is a guide for applying artificial intelligence on an Iceberg open data lakehouseCortex21 MinutesServerless LLM Fine-tuning using Snowflake Cortex AIThis guide provides the instructions for fine-tuning large language models using Snowflake Cortex AI.FeaturedGetting StartedCortex23 MinutesSnowflake Connector for ServiceNow InstallationStep-by-step to set up Servicenow connectorConnectors30 MinutesSnowflake for Data LakeThis is the guide for the hands-on lab, Snowflake for Data LakeGetting StartedData EngineeringArchitecture Patterns41 MinutesSnowflake Security DashboardsWalkthrough of creating security dashboards for Snowflake based on Snowflake audit and config dataCybersecurityAudit58 MinutesSnowpark Python: Top Three Tips for Optimal PerformanceSnowpark Python: Top Three Tips for Optimal PerformanceData Engineering53 MinutesSnowpipe Streaming and Dynamic Tables for Real-Time Ingestion (CDC Use Case)Learn how to use Snowpipe Streaming and Dynamic Tables for Real-Time Streaming (CDC use case)Data Engineering92 MinutesStreaming Data Integration with SnowflakeStreaming Data Integration with SnowflakeData Engineering96 MinutesStreamSets\\' Transformer for Snowflake: Hands on LabHands on Lab for Transformer for SnowflakeData EngineeringPartner Integrations14 MinutesSuppress existing customers from a Youtube campaign with Hightouch and SnowflakeBuild and sync an audience of existing customers to Youtube to suppress from future prospecting campaigns, using Hightouch and Snowflake.Solution ExamplesPartner Integrations27 MinutesTasty Bytes - Customer Support Streamlit Application Powered by CortexThis quickstart builds a customer support email application in Streamlit using Cortex LLM features to help agents quickly and effectively answer customer questions.Tasty BytesGetting StartedCortex10 MinutesTasty Bytes - Da zero a Snowflake - CollaborazioneQuickstart Tasty Bytes - Da zero a Snowflake - CollaborazioneTasty BytesGetting StartedFeatured10 MinutesTasty Bytes - Da zero a Snowflake - Dati semi-strutturatiQuickstart Tasty Bytes - Da zero a Snowflake - Dati semi-strutturati categorie: Tasty-Bytes, Guida-introduttiva, In-evidenzaTasty BytesGetting StartedFeatured10 MinutesTasty Bytes - Da zero a Snowflake - TrasformazioneQuickstart Tasty Bytes - Da zero a Snowflake - TrasformazioneTasty BytesGetting StartedFeatured22 MinutesTasty Bytes - Developing React Native Data Application with SQL APIDeveloping Tasty Bytes React Native Data Application using Snowflake, SQL APITasty BytesGetting StartedApp Development32 MinutesTasty Bytes - Enhance Customer Experience Streamlit AppThis application supports Tasty Bytes management by analyzing both customer reviews and food truck inspections, streamlining communication with truck owners to improve the overall customer experience.Getting StartedTasty BytesCortex41 MinutesTasty Bytes - Introduzione a Snowpark per la data scienceQuickstart Tasty Bytes - Introduzione a Snowpark per la data scienceTasty BytesGetting StartedFeaturedData Science & ML28 MinutesTasty Bytes - RAG Chatbot Using Cortex and StreamlitIn this guide, we build a Cortex LLM powered chatbot to help Tasty Bytes support agents find answers to customer questions faster.Getting StartedTasty BytesCortex41 MinutesTasty Bytes - Snowpark 101 for Data ScienceTasty Bytes - Snowpark 101 for Data Science QuickstartTasty BytesGetting StartedFeaturedData Science & ML28 MinutesTasty Bytes - Working with Iceberg TablesThis tutorial is a guide built as an extention to the Tasty Bytes series, where we will go over how data from an external Data Lake is made available in Snowflake using External Tables & Iceberg Tables.Getting StartedTasty BytesData Engineering10 MinutesTasty Bytes - Zero to Snowflake - CollaborationTasty Bytes - Zero to Snowflake - Collaboration QuickstartTasty BytesGetting StartedFeatured14 MinutesTasty Bytes - Zero to Snowflake - Cost ManagementTasty Bytes - Zero to Snowflake - Cost Management QuickstartTasty BytesGetting StartedFeatured14 MinutesTasty Bytes - Zero to Snowflake - GeospatialTasty Bytes - Zero to Snowflake - Geospatial QuickstartTasty BytesGetting StartedFeatured36 MinutesTasty Bytes - Zero to Snowflake - Governance with Snowflake HorizonTasty Bytes - Zero to Snowflake - Governance with Snowflake Horizon QuickstartTasty BytesGetting StartedFeatured11 MinutesTasty Bytes - Zero to Snowflake - Semi-Structured DataTasty Bytes - Zero to Snowflake - Semi-Structured Data QuickstartTasty BytesGetting StartedFeatured8 MinutesTasty Bytes - Zero to Snowflake - TransformationTasty Bytes - Zero to Snowflake - Transformation QuickstartTasty BytesGetting StartedFeatured10 MinutesTasty Bytes – Débuter avec Snowflake – CollaborationTasty Bytes – Débuter avec Snowflake – CollaborationTasty BytesGetting StartedFeatured10 MinutesTasty Bytes – Débuter avec Snowflake – Données semi-structuréesTasty Bytes – Débuter avec Snowflake – Données semi-structuréesTasty BytesGetting StartedFeatured10 MinutesTasty Bytes – Débuter avec Snowflake – TransformationTasty Bytes – Débuter avec Snowflake – TransformationTasty BytesGetting StartedFeatured41 MinutesTasty Bytes – Snowpark 101 pour la Data ScienceTasty Bytes – Snowpark\\xa0101 pour la Data ScienceTasty BytesGetting StartedFeaturedData Science & ML10 MinutesTasty Bytes: do zero ao Snowflake - colaboraçãoQuickstart Tasty Bytes: do zero ao Snowflake - colaboraçãoTasty BytesGetting StartedFeatured10 MinutesTasty Bytes: do zero ao Snowflake - dados semiestruturadosQuickstart Tasty Bytes: do zero ao Snowflake - dados semiestruturadosTasty BytesGetting StartedFeatured10 MinutesTasty Bytes: do zero ao Snowflake - transformaçãoQuickstart Tasty Bytes: do zero ao Snowflake - transformaçãoTasty BytesGetting StartedFeatured41 MinutesTasty Bytes: introdução ao Snowpark para ciência de dadosTasty Bytes: introdução ao Snowpark para ciência de dadosTasty BytesGetting StartedFeaturedData Science & ML41 MinutesTerraforming SnowflakeLearn how to manage Snowflake using TerraformFeaturedGetting StartedApp DevelopmentDevOps12 MinutesText Embedding As A Snowpark Python UDFText Embedding As A Snowpark Python UDFData Science & ML48 MinutesThreat Intelligence on Snowflake Setup GuideThis guide will walk you through how to load sample security data info Snowflake tables and then join the data meaningfully and present that information using a Streamlit frontend.Tbd43 MinutesTokenization in Snowflake Using Python UDFs (Python Camouflage)Project Python Camouflage aims to give a working MVP for tokenization in Snowflake using Python. The problem this aims to solve is allowing customers to obfuscate (or “mask”) PII while at the same time not losing the ability to use that data in joins and other operations where the consistency of the data through operations is required. Python offers libraries to achieve this using encryption, and through the use of Snowflake Python UDFs we can apply that to information in Snowflake natively. As an MVP, this is not meant to offer a complete solution to the problem. Rather, this is a framework that others can embrace and extend.CybersecurityArchitecture PatternsSolution Examples195 MinutesTwo Way Data CollaborationThis is a Snowflake Guide on how to use Snowflake’s Data Collaboration features to share an enrich data.Getting Started22 MinutesUse Segment Reverse ETL to sync your Snowflake customer table to SalesforceUse Segment Reverse ETL to sync your Snowflake customer table to SalesforceGetting StartedMarketing71 MinutesUsing Fluentd to Send Log Files to Snowflake for Security AnalyticsWalkthrough of using Fluentd to send system event logs to Snowflake as a part of a SIEM or log storage solutionCybersecurity65 MinutesUsing Snowflake with Azure OpenAI to build a RAG workflowUsing Snowflake with Azure OpenAI to build a RAG workflowFeaturedRagApp DevelopmentAzureOpenaiStreamlitGenAIAIMLImage30 MinutesUsing trace events in JavaAn example of emitting trace events from Java, and analyzing themGetting Started32 MinutesValidate Your Customer Identity Model with IdentityQAUse IdentityQA to validate your customer identity modelGetting StartedData WarehouseData EngineeringMarketing42 MinutesVisual Analytics powered by Snowflake and TableauVisual Analytics Powered by Snowflake and TableauGetting StartedWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/guides', 'title': 'Guides - Snowflake Documentation', 'language': 'en'}, page_content='Guides - Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingUser GuidesInstructions on performing various Snowflake operationsConnecting to SnowflakeSnowflake provides a variety of mechanisms for connecting to Snowflake and executing database commands. Choose between the web interface or the command line tool to connect to your Snowflake account. Learn how to use connectors to integrate third-party data into Snowflake.See allWeb InterfaceSnowsight distills Snowflake’s powerful SQL support into a unified, easy-to-use experience. Use Snowsight to perform your critical Snowflake operations.Learn moreCommand LineDetailed instructions for installing, configuring, and using the Snowflake command-line client, snowsql.Learn moreConnectorsThe Snowflake Connectors provide native integration of third-party applications and database systems in Snowflake. The connectors provide instant access to current data without the need to manually integrate against API endpoints.Learn moreSnowflake FundamentalsLearn the basics of warehouses, tables, and views in Snowflake.Snowflake WarehousesLearn how to set up and use virtual data warehouses to process the SQL statements that you execute.Overview of WarehousesMulti-cluster WarehousesWarehouse ConsiderationsWorking with WarehousesUsing the Query Acceleration ServiceSee allBasics of Snowflake Tables and ViewsLearn how to design and create tables and views for your data.Understanding Snowflake Table StructuresTable Design ConsiderationsOverview of ViewsWorking with Secure ViewsCloning ConsiderationsTable Storage ConsiderationsSee allBasics of Data TypesLearn about Snowflake data types and their usesIntroduction to Snowflake Data TypesNumeric Data TypesString and Binary Data TypesLogical Data TypesDate & Time Data TypesGeospatial Data TypesSee allGetting data in to SnowflakeSnowflake provides several different methods to load data in to Snowflake, such as by using Snowpipe, loading from cloud storage, or uploading files using Snowsight.Understanding Data LoadingData can be loaded into Snowflake in a number of ways. Learn about data loading concepts, different tasks, tools, and techniques to quickly and easily load data into Snowflake.Bulk Data LoadingLearn to use the COPY command to load data on-demand directly from an AWS S3 bucket, Google Cloud Share, or a Microsoft Azure storage container into Snowflake.SnowpipeUse Snowflake Snowpipe to load data automatically as it arrives.Working with dataQueries and other standard database features are just the beginning when you work with your data in Snowflake. You also use machine learning functions to analyze data in Snowflake.See allQueriesSnowflake supports standard SQL, including a subset of ANSI SQL:1999 and the SQL:2003 analytic extensions. Learn how to use queries to interact with Snowflake using simple queries, joins, and more.Learn moreViews, Materialized Views, & Dynamic TablesViews are just the beginning of how you can examine data. Snowflake provides a number of mechanism for joining data including Materialized Views and Dynamic Tables.Learn moreStreams and TasksStreams and tasks make executing complex task based solutions simple and easy. Streams allow you to track changes to database objects and tasks provide a mechanism to then execute SQL when those events occur.Learn moreML FunctionsML Functions are Snowflake’s intelligent, fully-managed service that enables organizations to quickly analyze data within Snowflake.Learn moreCollaboratingShare data and applications with other Snowflake users. Discover and publish listings of data products on the Snowflake Marketplace, share data products privately, or use a direct share to quickly share data with someone in the same region.What are listings?With listings, you can provide data and other information to other Snowflake users, and you can access data and other information shared by Snowflake providers.Becoming a listing providerBecoming a provider of listings in Snowflake makes it easier to manage sharing from your account to other Snowflake accounts.Becoming a listing consumerGet access to data products shared privately or on the Snowflake Marketplace by becoming a consumer of listings.More GuidesAlerts and NotificationsSetting Up Alerts Based on Data in SnowflakeSending Email NotificationsSee allSecurityAuthenticationAccess ControlEncryption key managementEncryptionNetworkingSee allGovernance and ComplianceData Lineage and DependenciesData Access PoliciesData SensitivityClassificationComplianceSee allPrivacyAggregation PoliciesProjection PoliciesSee allOrganizations and AccountsOrganizationsAccount identifiersSee allBusiness Continuity & Data RecoveryReplication & FailoverClient RedirectTime TravelFail-safeSee allPerformance and CostCost ManagementQuery PerformanceSee allWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/sql-reference/constructs/asof-join', 'title': 'ASOF JOIN | Snowflake Documentation', 'language': 'en'}, page_content='ASOF JOIN | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSQL command referenceQuery syntaxSELECTWITHTOP <n>INTOFROMATBEFORECHANGESCONNECT BYJOINASOF JOINLATERALMATCH_RECOGNIZEPIVOTUNPIVOTVALUESSAMPLETABLESAMPLEWHEREGROUP BYGROUP BY CUBEGROUP BY GROUPING SETSGROUP BY ROLLUPHAVINGQUALIFYORDER BYLIMITFETCHFOR UPDATEQuery operatorsGeneral DDLGeneral DMLAll commands (alphabetical)AccountsUsers, roles, & privilegesIntegrationsReplication & failoverSessionsTransactionsVirtual warehouses & resource monitorsDatabases, schemas, & sharesTables, views, & sequencesFunctions, procedures, & scriptingStreams & tasksClasses & instancesMachine learning modelsCortex Search ServiceListingsSecurityData GovernancePrivacyData loading & unloadingFile stagingAlertsNative Apps FrameworkStreamlitNotebookSnowpark Container ServicesFunction and stored procedure referenceClass referenceScripting referenceGeneral referenceAPI referenceReferenceSQL command referenceQuery syntaxASOF JOIN\\n\\nCategories:Query syntax\\n\\n\\n\\nASOF JOIN¶\\nAn ASOF JOIN operation combines rows from two tables based on timestamp values that follow each\\nother, precede each other, or match exactly. For each row in the first (or left) table, the join finds a single\\nrow in the second (or right) table that has the closest timestamp value. The qualifying row on the right side\\nis the closest match, which could be equal in time, earlier in time, or later in time, depending on the specified\\ncomparison operator.\\nThis topic describes how to use the ASOF JOIN construct in the FROM clause. For a more detailed conceptual\\nexplanation of ASOF joins, see Analyzing time-series data.\\nSee also JOIN, which covers the syntax for other standard join types, such as\\ninner and outer joins.\\n\\nSyntax¶\\nThe following FROM clause syntax is specific to ASOF JOIN:\\nFROM <left_table> ASOF JOIN <right_table>\\n  MATCH_CONDITION ( <left_table.timecol> <comparison_operator> <right_table.timecol> )\\n  [ ON <table.col> = <table.col> [ AND ... ] | USING ( <column_list> ) ]\\n\\nCopy\\n\\n\\nParameters¶\\n\\nFROMThe first (or left) table in the FROM clause is assumed to contain records that either follow (in time),\\nprecede, or are exactly synchronized with, the records in the second (or right) table. When there is no\\nmatch for a row in the left table, the columns from the right table are null-padded.\\nIn addition to regular tables and views, any object reference can be used in an ASOF JOIN.\\nSee FROM.\\nASOF JOIN can be used in most contexts where joins are supported. For information about some restrictions, see Usage Notes.\\n\\nMATCH_CONDITION ( left_table.timecol comparison_operator right_table.timecol )This condition names the specific timestamp columns to be compared in each table.\\n\\nThe order of tables is important in the condition. The left table must be named first.\\nThe parentheses are required.\\nThe comparison operator must be one of the following: >=, <=, >, <. The equals operator (=) is\\nnot supported.\\nAll of the following data types are supported: DATE, TIME, DATETIME, TIMESTAMP, TIMESTAMP_LTZ, TIMESTAMP_NTZ, TIMESTAMP_TZ.\\nYou can also use NUMBER columns in the match condition. For example, you might have NUMBER columns that contain UNIX\\ntimestamps (which define the number of seconds that have elapsed since January 1st, 1970).\\nThe data types of the two matched columns don’t have to be exactly the same, but they must be\\ncompatible.\\n\\n\\nON table.col = table.col [ AND ... ]\\xa0 | USING (column_list)The optional ON or USING clause defines one or more equality conditions on columns in the two tables, for the purpose of\\nlogically grouping the results of the query.\\nFor general information about ON and USING, see JOIN. Note that a join specified with USING\\nprojects one of the joining columns in its intermediate result set, not both. A join specified with an ON clause projects both\\njoining columns.\\nThe following notes are specific to ASOF JOIN:\\n\\nThe comparison operator in the ON clause must be the equal sign (=).\\nThe ON clause cannot contain disjuncts (conditions connected with OR). Conditions connected with AND are supported.\\nEach side of a condition must refer to only one of the two tables in the join. However, the order of the table references doesn’t matter.\\nEach condition can be enclosed in parentheses, but they aren’t required.\\n\\n\\n\\nSee also More Details on Join Behavior and Specifying a USING condition instead of an ON condition.\\n\\n\\nUsage notes¶\\n\\nIf no match is found in the right table for a given row, the result is null-padded for the selected columns from the right table. (ASOF joins are similar to left outer joins in this respect.)\\nIf you use TIME columns in the match condition (as opposed to one of the timestamp types), you might need to set the TIME_OUTPUT_FORMAT parameter in order to see the exact TIME values that are being compared when you look at ASOF JOIN query results. By default, the display of a TIME column truncates milliseconds. See TIME columns in the match condition.\\nYou can use more than one ASOF join in the same query as long as all of the syntax rules are followed for each join. Each join must be immediately followed by its own MATCH_CONDITION. You cannot apply a single MATCH_CONDITION to multiple ASOF joins. See Multiple ASOF joins in a query.\\nASOF joins are not supported for joins with LATERAL table functions or LATERAL inline views. For more information about lateral joins, see LATERAL.\\nAn ASOF join with a self-reference is not allowed in a RECURSIVE common table expression (CTE). For information about CTEs, see WITH.\\nThe EXPLAIN output for ASOF JOIN queries identifies the ON (or USING) conditions and the MATCH_CONDITION. For example, in text or tabular format, output similar to the following text appears above the table scans in the plan:\\n->ASOF Join  joinKey: (S.LOCATION = R.LOCATION) AND (S.STATE = R.STATE),\\n  matchCondition: (S.OBSERVED >= R.OBSERVED)\\n\\n\\n\\nQuery profiles also clearly identify the ASOF JOIN operation in the plan. In this example, you can see that the table scan reads 22M rows from the left table, which are all preserved by the join. The profile also shows the match condition for the join.\\n\\n\\n\\n\\n\\n\\nMore details on join behavior¶\\nThe optional ON (or USING) conditions for ASOF JOIN provide a way of grouping or partitioning table rows before the final matching rows\\nare singled out by the required match condition. If you want the rows from the joined tables to be grouped on one or more dimensions\\nthat the tables share (stock symbol, location, city, state, company name, etc.), use an ON condition.\\nIf you don’t use an ON condition, each row from the left table may be matched (by time) with any row from the right table\\nin the final result set.\\nIn the following example, tables left_table and right_table have values A, B, etc.\\nin column c1, and values 1, 2, etc. in column c2. Column c3 is a TIME column, and c4 is a numeric value (column of interest).\\nFirst, create and load the two tables:\\nCREATE OR REPLACE TABLE left_table (\\n  c1 VARCHAR(1),\\n  c2 TINYINT,\\n  c3 TIME,\\n  c4 NUMBER(3,2)\\n);\\n\\nCREATE OR REPLACE TABLE right_table (\\n  c1 VARCHAR(1),\\n  c2 TINYINT,\\n  c3 TIME,\\n  c4 NUMBER(3,2)\\n);\\n\\nINSERT INTO left_table VALUES\\n  (\\'A\\',1,\\'09:15:00\\',3.21),\\n  (\\'A\\',2,\\'09:16:00\\',3.22),\\n  (\\'B\\',1,\\'09:17:00\\',3.23),\\n  (\\'B\\',2,\\'09:18:00\\',4.23);\\n\\nINSERT INTO right_table VALUES\\n  (\\'A\\',1,\\'09:14:00\\',3.19),\\n  (\\'B\\',1,\\'09:16:00\\',3.04);\\n\\nCopy\\nSELECT * FROM left_table ORDER BY c1, c2;\\n\\nCopy\\n+----+----+----------+------+\\n| C1 | C2 | C3       |   C4 |\\n|----+----+----------+------|\\n| A  |  1 | 09:15:00 | 3.21 |\\n| A  |  2 | 09:16:00 | 3.22 |\\n| B  |  1 | 09:17:00 | 3.23 |\\n| B  |  2 | 09:18:00 | 4.23 |\\n+----+----+----------+------+\\n\\n\\nSELECT * FROM right_table ORDER BY c1, c2;\\n\\nCopy\\n+----+----+----------+------+\\n| C1 | C2 | C3       |   C4 |\\n|----+----+----------+------|\\n| A  |  1 | 09:14:00 | 3.19 |\\n| B  |  1 | 09:16:00 | 3.04 |\\n+----+----+----------+------+\\n\\n\\nIf c1 and c2 are both ON condition columns in the query, a row in the left table only matches a row in the right table\\nwhen A and 1, A and 2, B and 1, or B and 2 are found in both tables.\\nIf no match is found for such values, the right table columns are null-padded.\\nSELECT *\\n  FROM left_table l ASOF JOIN right_table r\\n    MATCH_CONDITION(l.c3>=r.c3)\\n    ON(l.c1=r.c1 and l.c2=r.c2)\\n  ORDER BY l.c1, l.c2;\\n\\nCopy\\n+----+----+----------+------+------+------+----------+------+\\n| C1 | C2 | C3       |   C4 | C1   | C2   | C3       |   C4 |\\n|----+----+----------+------+------+------+----------+------|\\n| A  |  1 | 09:15:00 | 3.21 | A    |  1   | 09:14:00 | 3.19 |\\n| A  |  2 | 09:16:00 | 3.22 | NULL | NULL | NULL     | NULL |\\n| B  |  1 | 09:17:00 | 3.23 | B    |  1   | 09:16:00 | 3.04 |\\n| B  |  2 | 09:18:00 | 4.23 | NULL | NULL | NULL     | NULL |\\n+----+----+----------+------+------+------+----------+------+\\n\\n\\nIf the ON conditions are removed, any combination of values in c1 and c2 may be matched in the final result.\\nOnly the match condition determines the results.\\nSELECT *\\n  FROM left_table l ASOF JOIN right_table r\\n    MATCH_CONDITION(l.c3>=r.c3)\\n  ORDER BY l.c1, l.c2;\\n\\nCopy\\n+----+----+----------+------+----+----+----------+------+\\n| C1 | C2 | C3       |   C4 | C1 | C2 | C3       |   C4 |\\n|----+----+----------+------+----+----+----------+------|\\n| A  |  1 | 09:15:00 | 3.21 | A  |  1 | 09:14:00 | 3.19 |\\n| A  |  2 | 09:16:00 | 3.22 | B  |  1 | 09:16:00 | 3.04 |\\n| B  |  1 | 09:17:00 | 3.23 | B  |  1 | 09:16:00 | 3.04 |\\n| B  |  2 | 09:18:00 | 4.23 | B  |  1 | 09:16:00 | 3.04 |\\n+----+----+----------+------+----+----+----------+------+\\n\\n\\n\\n\\nExpected behavior when “ties” exist in the right table¶\\nASOF JOIN queries always attempt to match a single row in the left table with a single row in the right table.\\nThis behavior is true even if two (or more) rows in the right table are identical and qualify for the join. When\\nsuch ties exist and you run the same join query multiple times, you might get different results. The results are\\nnon-deterministic because any one of the tying rows might be returned. If you’re unsure about the results of ASOF JOIN\\nqueries, check for exact matches in the timestamp values for rows in the right table.\\nFor example, using the same tables from the examples in the previous section, add a right_id column to right_table\\nand insert the following rows:\\nCREATE OR REPLACE TABLE right_table\\n  (c1 VARCHAR(1),\\n  c2 TINYINT,\\n  c3 TIME,\\n  c4 NUMBER(3,2),\\n  right_id VARCHAR(2));\\n\\nINSERT INTO right_table VALUES\\n  (\\'A\\',1,\\'09:14:00\\',3.19,\\'A1\\'),\\n  (\\'A\\',1,\\'09:14:00\\',3.19,\\'A2\\'),\\n  (\\'B\\',1,\\'09:16:00\\',3.04,\\'B1\\');\\n\\nSELECT * FROM right_table ORDER BY 1, 2;\\n\\nCopy\\n+----+----+----------+------+----------+\\n| C1 | C2 | C3       |   C4 | RIGHT_ID |\\n|----+----+----------+------+----------|\\n| A  |  1 | 09:14:00 | 3.19 | A1       |\\n| A  |  1 | 09:14:00 | 3.19 | A2       |\\n| B  |  1 | 09:16:00 | 3.04 | B1       |\\n+----+----+----------+------+----------+\\n\\n\\nTwo of the rows are identical except for their right_id values. Now run the following ASOF JOIN query:\\nSELECT *\\n  FROM left_table l ASOF JOIN right_table r\\n    MATCH_CONDITION(l.c3>=r.c3)\\n  ORDER BY l.c1, l.c2;\\n\\nCopy\\n+----+----+----------+------+----+----+----------+------+----------+\\n| C1 | C2 | C3       |   C4 | C1 | C2 | C3       |   C4 | RIGHT_ID |\\n|----+----+----------+------+----+----+----------+------+----------|\\n| A  |  1 | 09:15:00 | 3.21 | A  |  1 | 09:14:00 | 3.19 | A2       |\\n| A  |  2 | 09:16:00 | 3.22 | B  |  1 | 09:16:00 | 3.04 | B1       |\\n| B  |  1 | 09:17:00 | 3.23 | B  |  1 | 09:16:00 | 3.04 | B1       |\\n| B  |  2 | 09:18:00 | 4.23 | B  |  1 | 09:16:00 | 3.04 | B1       |\\n+----+----+----------+------+----+----+----------+------+----------+\\n\\n\\nNote that rows A1 and A2 from right_table both qualify for the join, but only A2 is returned. On a\\nsubsequent run of the same query, A1 could be returned instead.\\n\\n\\nRewriting ASOF JOIN queries to reduce scans on the right table¶\\nWhen the cardinality of the ON or USING join column in the left table is lower than the cardinality of the\\njoin column in the right table, the optimizer does not prune\\nthe unmatched rows from the right table. Therefore, more rows than are needed for the join will be scanned\\nfrom the right table. This behavior typically occurs when the query includes a highly selective filter on a\\nnon-join column from the left table, and the filter reduces the cardinality of the join column.\\nYou can work around this problem by manually reducing the rows that qualify for the join. For example, the\\noriginal query has the following pattern, and t1.c1 has lower cardinality than t2.c1:\\nSELECT ...\\n  FROM t1\\n    ASOF JOIN t2\\n      MATCH_CONDITION(...)\\n      ON t1.c1 = t2.c1\\n  WHERE t1 ...;\\n\\nCopy\\nYou can rewrite the query as follows to manually select the rows from t2 where t2.c1 values are\\nfound in t1.c1:\\nWITH t1 AS (SELECT * FROM t1 WHERE t1 ...)\\nSELECT ...\\n  FROM t1\\n    ASOF JOIN (SELECT * FROM t2 WHERE t2.c1 IN (SELECT t1.c1 FROM t1)) AS t2\\n      MATCH_CONDITION(...)\\n      ON t1.c1 = t2.c1;\\n\\nCopy\\n\\n\\nUsing ASOF and MATCH_CONDITION as object names and aliases¶\\nUse of the ASOF and MATCH_CONDITION keywords in SELECT command syntax is restricted:\\n\\nIf a SELECT statement uses ASOF or MATCH_CONDITION as the name of a table, view, or inline view, you must identify it\\nas follows:\\n\\nIf the object was created with double quotes in the name, use the same double-quoted name.\\nIf the object was created without double quotes in the name, use double quotes and capital letters.\\n\\nFor example, the following statements are no longer allowed and return errors:\\nSELECT * FROM asof;\\n\\nWITH match_condition AS (SELECT * FROM T1) SELECT * FROM match_condition;\\n\\nCopy\\nIf you created the objects with double quotes, fix the problem by using double quotes:\\nSELECT * FROM \"asof\";\\n\\nWITH \"match_condition\" AS (SELECT * FROM T1) SELECT * FROM \"match_condition\";\\n\\nCopy\\nIf you created the objects without double quotes, fix the problem by using double quotes and capital letters:\\nSELECT * FROM \"ASOF\";\\n\\nWITH \"MATCH_CONDITION\" AS (SELECT * FROM T1) SELECT * FROM \"MATCH_CONDITION\";\\n\\nCopy\\nSee also Unquoted identifiers.\\n\\nIf a SELECT statement uses ASOF or MATCH_CONDITION as an alias, you must use AS before the alias or double-quote the\\nalias. For example, the following statements are no longer allowed and return errors:\\nSELECT * FROM t1 asof;\\n\\nSELECT * FROM t2 match_condition;\\n\\nCopy\\nFix the problem in one of the following ways:\\nSELECT * FROM t1 AS asof;\\n\\nSELECT * FROM t1 \"asof\";\\n\\nSELECT * FROM t2 AS match_condition;\\n\\nSELECT * FROM t2 \"match_condition\";\\n\\nCopy\\n\\n\\n\\n\\nExamples¶\\nThe following examples demonstrate the expected behavior of ASOF JOIN queries.\\nStart by running the query under Joining two tables on the closest match (alignment), then proceed with\\nthe examples here.\\n\\nNULL-padded results¶\\nInsert a new row into the trades table with a date that’s a day earlier than the existing rows in both\\ntrades and quotes:\\nINSERT INTO trades VALUES(\\'SNOW\\',\\'2023-09-30 12:02:55.000\\',3000);\\n\\nCopy\\n+-------------------------+\\n| number of rows inserted |\\n|-------------------------|\\n|                       1 |\\n+-------------------------+\\n\\n\\nNow run the first example query again. Note that the query returns four rows, but the new row is null-padded.\\nThere is no row in the quotes table that qualifies for the match condition.\\nThe columns from trades are returned, and the corresponding columns from quotes are null-padded.\\nSELECT t.stock_symbol, t.trade_time, t.quantity, q.quote_time, q.price\\n  FROM trades t ASOF JOIN quotes q\\n    MATCH_CONDITION(t.trade_time >= quote_time)\\n    ON t.stock_symbol=q.stock_symbol\\n  ORDER BY t.stock_symbol;\\n\\nCopy\\n+--------------+-------------------------+----------+-------------------------+--------------+\\n| STOCK_SYMBOL | TRADE_TIME              | QUANTITY | QUOTE_TIME              |        PRICE |\\n|--------------+-------------------------+----------+-------------------------+--------------|\\n| AAPL         | 2023-10-01 09:00:05.000 |     2000 | 2023-10-01 09:00:03.000 | 139.00000000 |\\n| SNOW         | 2023-09-30 12:02:55.000 |     3000 | NULL                    |         NULL |\\n| SNOW         | 2023-10-01 09:00:05.000 |     1000 | 2023-10-01 09:00:02.000 | 163.00000000 |\\n| SNOW         | 2023-10-01 09:00:10.000 |     1500 | 2023-10-01 09:00:08.000 | 165.00000000 |\\n+--------------+-------------------------+----------+-------------------------+--------------+\\n\\n\\n\\n\\nUsing a different comparison operator in the match condition¶\\nFollowing on from the previous example, the results of the query change again when the comparison operator in the\\nmatch condition is changed. The following query specifies the <= operator (instead of >=):\\nSELECT t.stock_symbol, t.trade_time, t.quantity, q.quote_time, q.price\\n  FROM trades t ASOF JOIN quotes q\\n    MATCH_CONDITION(t.trade_time <= quote_time)\\n    ON t.stock_symbol=q.stock_symbol\\n  ORDER BY t.stock_symbol;\\n\\nCopy\\n+--------------+-------------------------+----------+-------------------------+--------------+\\n| STOCK_SYMBOL | TRADE_TIME              | QUANTITY | QUOTE_TIME              |        PRICE |\\n|--------------+-------------------------+----------+-------------------------+--------------|\\n| AAPL         | 2023-10-01 09:00:05.000 |     2000 | 2023-10-01 09:00:07.000 | 142.00000000 |\\n| SNOW         | 2023-10-01 09:00:10.000 |     1500 | NULL                    |         NULL |\\n| SNOW         | 2023-10-01 09:00:05.000 |     1000 | 2023-10-01 09:00:07.000 | 166.00000000 |\\n| SNOW         | 2023-09-30 12:02:55.000 |     3000 | 2023-10-01 09:00:01.000 | 166.00000000 |\\n+--------------+-------------------------+----------+-------------------------+--------------+\\n\\n\\nSee also Less than and greater than comparison operators.\\n\\n\\nSpecifying a USING condition instead of an ON condition¶\\nYou can use an ON condition or a USING condition with ASOF JOIN queries. The following query is equivalent to the\\nprevious query, but it replaces ON with USING. The syntax USING(stock_symbol) implies the condition\\nt.stock_symbol=q.stock_symbol.\\nSELECT t.stock_symbol, t.trade_time, t.quantity, q.quote_time, q.price\\n  FROM trades t ASOF JOIN quotes q\\n    MATCH_CONDITION(t.trade_time <= quote_time)\\n    USING(stock_symbol)\\n  ORDER BY t.stock_symbol;\\n\\nCopy\\n\\n\\nInner join to a third table¶\\nThe following example adds a third companies table to the join in order to pick the company name for each stock symbol.\\nYou can use a regular INNER JOIN with an ON condition (or some other standard join syntax) to add the third table.\\nHowever, note that USING(stock_symbol) would not work here because the reference to stock_symbol would be ambiguous.\\nCREATE OR REPLACE TABLE companies(\\n  stock_symbol VARCHAR(4),\\n  company_name VARCHAR(100)\\n);\\n\\n INSERT INTO companies VALUES\\n  (\\'NVDA\\',\\'NVIDIA Corp\\'),\\n  (\\'TSLA\\',\\'Tesla Inc\\'),\\n  (\\'SNOW\\',\\'Snowflake Inc\\'),\\n  (\\'AAPL\\',\\'Apple Inc\\')\\n;\\n\\nCopy\\nSELECT t.stock_symbol, c.company_name, t.trade_time, t.quantity, q.quote_time, q.price\\n  FROM trades t ASOF JOIN quotes q\\n    MATCH_CONDITION(t.trade_time >= quote_time)\\n    ON t.stock_symbol=q.stock_symbol\\n    INNER JOIN companies c ON c.stock_symbol=t.stock_symbol\\n  ORDER BY t.stock_symbol;\\n\\nCopy\\n+--------------+---------------+-------------------------+----------+-------------------------+--------------+\\n| STOCK_SYMBOL | COMPANY_NAME  | TRADE_TIME              | QUANTITY | QUOTE_TIME              |        PRICE |\\n|--------------+---------------+-------------------------+----------+-------------------------+--------------|\\n| AAPL         | Apple Inc     | 2023-10-01 09:00:05.000 |     2000 | 2023-10-01 09:00:03.000 | 139.00000000 |\\n| SNOW         | Snowflake Inc | 2023-09-30 12:02:55.000 |     3000 | NULL                    |         NULL |\\n| SNOW         | Snowflake Inc | 2023-10-01 09:00:05.000 |     1000 | 2023-10-01 09:00:02.000 | 163.00000000 |\\n| SNOW         | Snowflake Inc | 2023-10-01 09:00:10.000 |     1500 | 2023-10-01 09:00:08.000 | 165.00000000 |\\n+--------------+---------------+-------------------------+----------+-------------------------+--------------+\\n\\n\\n\\n\\nNumbers as timestamps¶\\nThe following example demonstrates that the match condition can compare numeric values.\\nIn this case, the tables have UNIX timestamp values stored in NUMBER(38,0) columns. 1696150805\\nis equivalent to 2023-10-30 10:20:05.000 (three seconds later than 1696150802).\\nSELECT * FROM trades_unixtime;\\n\\nCopy\\n+--------------+------------+----------+--------------+\\n| STOCK_SYMBOL | TRADE_TIME | QUANTITY |        PRICE |\\n|--------------+------------+----------+--------------|\\n| SNOW         | 1696150805 |      100 | 165.33300000 |\\n+--------------+------------+----------+--------------+\\n\\n\\nSELECT * FROM quotes_unixtime;\\n\\nCopy\\n+--------------+------------+----------+--------------+--------------+\\n| STOCK_SYMBOL | QUOTE_TIME | QUANTITY |          BID |          ASK |\\n|--------------+------------+----------+--------------+--------------|\\n| SNOW         | 1696150802 |      100 | 166.00000000 | 165.00000000 |\\n+--------------+------------+----------+--------------+--------------+\\n\\n\\nSELECT *\\n  FROM trades_unixtime tu\\n    ASOF JOIN quotes_unixtime qu\\n    MATCH_CONDITION(tu.trade_time>=qu.quote_time);\\n\\nCopy\\n+--------------+------------+----------+--------------+--------------+------------+----------+--------------+--------------+\\n| STOCK_SYMBOL | TRADE_TIME | QUANTITY |        PRICE | STOCK_SYMBOL | QUOTE_TIME | QUANTITY |          BID |          ASK |\\n|--------------+------------+----------+--------------+--------------+------------+----------+--------------+--------------|\\n| SNOW         | 1696150805 |      100 | 165.33300000 | SNOW         | 1696150802 |      100 | 166.00000000 | 165.00000000 |\\n+--------------+------------+----------+--------------+--------------+------------+----------+--------------+--------------+\\n\\n\\n\\n\\nTIME columns in the match condition¶\\nThe following examples join tables that contain weather observations. The observations in these tables are recorded in TIME columns.\\nYou can create and load the tables as follows:\\nCREATE OR REPLACE TABLE raintime(\\n  observed TIME(9),\\n  location VARCHAR(40),\\n  state VARCHAR(2),\\n  observation NUMBER(5,2)\\n);\\n\\nINSERT INTO raintime VALUES\\n  (\\'14:42:59.230\\', \\'Ahwahnee\\', \\'CA\\', 0.90),\\n  (\\'14:42:59.001\\', \\'Oakhurst\\', \\'CA\\', 0.50),\\n  (\\'14:42:44.435\\', \\'Reno\\', \\'NV\\', 0.00)\\n;\\n\\nCREATE OR REPLACE TABLE preciptime(\\n  observed TIME(9),\\n  location VARCHAR(40),\\n  state VARCHAR(2),\\n  observation NUMBER(5,2)\\n);\\n\\nINSERT INTO preciptime VALUES\\n  (\\'14:42:59.230\\', \\'Ahwahnee\\', \\'CA\\', 0.91),\\n  (\\'14:42:59.001\\', \\'Oakhurst\\', \\'CA\\', 0.51),\\n  (\\'14:41:44.435\\', \\'Las Vegas\\', \\'NV\\', 0.01),\\n  (\\'14:42:44.435\\', \\'Reno\\', \\'NV\\', 0.01),\\n  (\\'14:40:34.000\\', \\'Bozeman\\', \\'MT\\', 1.11)\\n;\\n\\nCREATE OR REPLACE TABLE snowtime(\\n  observed TIME(9),\\n  location VARCHAR(40),\\n  state VARCHAR(2),\\n  observation NUMBER(5,2)\\n);\\n\\nINSERT INTO snowtime VALUES\\n  (\\'14:42:59.199\\', \\'Fish Camp\\', \\'CA\\', 3.20),\\n  (\\'14:42:44.435\\', \\'Reno\\', \\'NV\\', 3.00),\\n  (\\'14:43:01.000\\', \\'Lake Tahoe\\', \\'CA\\', 4.20),\\n  (\\'14:42:45.000\\', \\'Bozeman\\', \\'MT\\', 1.80)\\n;\\n\\nCopy\\nWhen you run the first query, some of the TIME values appear to be exactly the same in the result set (14:42:59, 14:42:44).\\nSELECT * FROM preciptime p ASOF JOIN snowtime s MATCH_CONDITION(p.observed>=s.observed)\\n  ORDER BY p.observed;\\n\\nCopy\\n+----------+-----------+-------+-------------+----------+-----------+-------+-------------+\\n| OBSERVED | LOCATION  | STATE | OBSERVATION | OBSERVED | LOCATION  | STATE | OBSERVATION |\\n|----------+-----------+-------+-------------+----------+-----------+-------+-------------|\\n| 14:40:34 | Bozeman   | MT    |        1.11 | NULL     | NULL      | NULL  |        NULL |\\n| 14:41:44 | Las Vegas | NV    |        0.01 | NULL     | NULL      | NULL  |        NULL |\\n| 14:42:44 | Reno      | NV    |        0.01 | 14:42:44 | Reno      | NV    |        3.00 |\\n| 14:42:59 | Oakhurst  | CA    |        0.51 | 14:42:45 | Bozeman   | MT    |        1.80 |\\n| 14:42:59 | Ahwahnee  | CA    |        0.91 | 14:42:59 | Fish Camp | CA    |        3.20 |\\n+----------+-----------+-------+-------------+----------+-----------+-------+-------------+\\n\\n\\nTo return a more precise display of TIME values, including milliseconds, run the following ALTER SESSION command,\\nthen run the ASOF JOIN query again:\\nALTER SESSION SET TIME_OUTPUT_FORMAT = \\'HH24:MI:SS.FF3\\';\\n\\nCopy\\n+----------------------------------+\\n| status                           |\\n|----------------------------------|\\n| Statement executed successfully. |\\n+----------------------------------+\\n\\n\\nSELECT * FROM preciptime p ASOF JOIN snowtime s MATCH_CONDITION(p.observed>=s.observed)\\n  ORDER BY p.observed;\\n\\nCopy\\n+--------------+-----------+-------+-------------+--------------+-----------+-------+-------------+\\n| OBSERVED     | LOCATION  | STATE | OBSERVATION | OBSERVED     | LOCATION  | STATE | OBSERVATION |\\n|--------------+-----------+-------+-------------+--------------+-----------+-------+-------------|\\n| 14:40:34.000 | Bozeman   | MT    |        1.11 | NULL         | NULL      | NULL  |        NULL |\\n| 14:41:44.435 | Las Vegas | NV    |        0.01 | NULL         | NULL      | NULL  |        NULL |\\n| 14:42:44.435 | Reno      | NV    |        0.01 | 14:42:44.435 | Reno      | NV    |        3.00 |\\n| 14:42:59.001 | Oakhurst  | CA    |        0.51 | 14:42:45.000 | Bozeman   | MT    |        1.80 |\\n| 14:42:59.230 | Ahwahnee  | CA    |        0.91 | 14:42:59.199 | Fish Camp | CA    |        3.20 |\\n+--------------+-----------+-------+-------------+--------------+-----------+-------+-------------+\\n\\n\\n\\n\\nMultiple ASOF joins in a query¶\\nThe following example shows how to connect a sequence of two or more ASOF joins in a single query block.\\nThe three tables (snowtime, raintime, preciptime) all contain weather observations that were recorded in\\nspecific locations at specific times. The column of interest is the observation column. The rows are logically grouped by state.\\nALTER SESSION SET TIME_OUTPUT_FORMAT = \\'HH24:MI:SS.FF3\\';\\n\\nSELECT *\\n  FROM snowtime s\\n    ASOF JOIN raintime r\\n      MATCH_CONDITION(s.observed>=r.observed)\\n      ON s.state=r.state\\n    ASOF JOIN preciptime p\\n      MATCH_CONDITION(s.observed>=p.observed)\\n      ON s.state=p.state\\n  ORDER BY s.observed;\\n\\nCopy\\n+--------------+------------+-------+-------------+--------------+----------+-------+-------------+--------------+----------+-------+-------------+\\n| OBSERVED     | LOCATION   | STATE | OBSERVATION | OBSERVED     | LOCATION | STATE | OBSERVATION | OBSERVED     | LOCATION | STATE | OBSERVATION |\\n|--------------+------------+-------+-------------+--------------+----------+-------+-------------+--------------+----------+-------+-------------|\\n| 14:42:44.435 | Reno       | NV    |        3.00 | 14:42:44.435 | Reno     | NV    |        0.00 | 14:42:44.435 | Reno     | NV    |        0.01 |\\n| 14:42:45.000 | Bozeman    | MT    |        1.80 | NULL         | NULL     | NULL  |        NULL | 14:40:34.000 | Bozeman  | MT    |        1.11 |\\n| 14:42:59.199 | Fish Camp  | CA    |        3.20 | 14:42:59.001 | Oakhurst | CA    |        0.50 | 14:42:59.001 | Oakhurst | CA    |        0.51 |\\n| 14:43:01.000 | Lake Tahoe | CA    |        4.20 | 14:42:59.230 | Ahwahnee | CA    |        0.90 | 14:42:59.230 | Ahwahnee | CA    |        0.91 |\\n+--------------+------------+-------+-------------+--------------+----------+-------+-------------+--------------+----------+-------+-------------+\\n\\n\\n\\n\\nLess than and greater than comparison operators¶\\nFollowing on from the previous example, two ASOF joins are specified, but this time the first match condition uses the >\\noperator and the second uses the < operator. The result is a single row that returns data from all three tables, and three rows\\nthat return data from two of the tables. Many of the columns in the result set are null-padded.\\nLogically, the query finds only one row where the observed time from the snowtime table was later than the observed time from the\\nraintime table but earlier than the observed time from the preciptime table.\\nSELECT *\\n  FROM snowtime s\\n    ASOF JOIN raintime r\\n      MATCH_CONDITION(s.observed>r.observed)\\n      ON s.state=r.state\\n    ASOF JOIN preciptime p\\n      MATCH_CONDITION(s.observed<p.observed)\\n      ON s.state=p.state\\n  ORDER BY s.observed;\\n\\nCopy\\n+--------------+------------+-------+-------------+--------------+-----------+-------+-------------+--------------+----------+-------+-------------+\\n| OBSERVED     | LOCATION   | STATE | OBSERVATION | OBSERVED     | LOCATION  | STATE | OBSERVATION | OBSERVED     | LOCATION | STATE | OBSERVATION |\\n|--------------+------------+-------+-------------+--------------+-----------+-------+-------------+--------------+----------+-------+-------------|\\n| 14:42:44.435 | Reno       | NV    |        3.00 | 14:41:44.435 | Las Vegas | NV    |        0.00 | NULL         | NULL     | NULL  |        NULL |\\n| 14:42:45.000 | Bozeman    | MT    |        1.80 | NULL         | NULL      | NULL  |        NULL | NULL         | NULL     | NULL  |        NULL |\\n| 14:42:59.199 | Fish Camp  | CA    |        3.20 | 14:42:59.001 | Oakhurst  | CA    |        0.50 | 14:42:59.230 | Ahwahnee | CA    |        0.91 |\\n| 14:43:01.000 | Lake Tahoe | CA    |        4.20 | 14:42:59.230 | Ahwahnee  | CA    |        0.90 | NULL         | NULL     | NULL  |        NULL |\\n+--------------+------------+-------+-------------+--------------+-----------+-------+-------------+--------------+----------+-------+-------------+\\n\\n\\n\\n\\nExamples of expected error cases¶\\nThe following examples show queries that return expected syntax errors.\\nHaving declared that snowtime s is the left table, you cannot begin the match condition with a reference to the right table, preciptime p:\\nSELECT * FROM snowtime s ASOF JOIN preciptime p MATCH_CONDITION(p.observed>=s.observed);\\n\\nCopy\\n010002 (42601): SQL compilation error:\\nMATCH_CONDITION clause is invalid: The left side allows only column references from the left side table, and the right side allows only column references from the right side table.\\n\\n\\nOnly the >=, <=, >, and < operators are allowed in match conditions:\\nSELECT * FROM preciptime p ASOF JOIN snowtime s MATCH_CONDITION(p.observed=s.observed);\\n\\nCopy\\n010001 (42601): SQL compilation error:\\nMATCH_CONDITION clause is invalid: Only comparison operators \\'>=\\', \\'>\\', \\'<=\\' and \\'<\\' are allowed. Keywords such as AND and OR are not allowed.\\n\\n\\nThe ON clause for ASOF JOIN must contain equality conditions:\\nSELECT *\\n  FROM preciptime p ASOF JOIN snowtime s\\n  MATCH_CONDITION(p.observed>=s.observed)\\n  ON s.state>=p.state;\\n\\nCopy\\n010010 (42601): SQL compilation error:\\nON clause for ASOF JOIN must contain conjunctions of equality conditions only. Disjunctions are not allowed. Each side of an equality condition must only refer to either the left table or the right table. S.STATE >= P.STATE is invalid.\\n\\n\\nAn ON clause equality condition cannot contain disjunctions:\\nSELECT *\\n  FROM preciptime p ASOF JOIN snowtime s\\n  MATCH_CONDITION(p.observed>=s.observed)\\n  ON s.state=p.state OR s.location=p.location;\\n\\nCopy\\n010010 (42601): SQL compilation error:\\nON clause for ASOF JOIN must contain conjunctions of equality conditions only. Disjunctions are not allowed. Each side of an equality condition must only refer to either the left table or the right table. (S.STATE = P.STATE) OR (S.LOCATION = P.LOCATION) is invalid.\\n\\n\\nASOF joins cannot be used with LATERAL inline views:\\nSELECT t1.a \"t1a\", t2.a \"t2a\"\\n  FROM t1 ASOF JOIN\\n    LATERAL(SELECT a FROM t2 WHERE t1.b = t2.b) t2\\n    MATCH_CONDITION(t1.a >= t2.a)\\n  ORDER BY 1,2;\\n\\nCopy\\n010004 (42601): SQL compilation error:\\nASOF JOIN is not supported for joins with LATERAL table functions or LATERAL views.\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageSyntaxParametersUsage notesMore details on join behaviorExpected behavior when “ties” exist in the right tableRewriting ASOF JOIN queries to reduce scans on the right tableUsing ASOF and MATCH_CONDITION as object names and aliasesExamplesRelated contentAnalyzing time-series dataLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/sql-reference-classes', 'title': 'SQL class reference | Snowflake Documentation', 'language': 'en'}, page_content='SQL class reference | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSQL command referenceFunction and stored procedure referenceClass referenceANOMALY_DETECTIONBUDGETCLASSIFICATIONCUSTOM_CLASSIFIERDOCUMENT_INTELLIGENCEFORECASTScripting referenceGeneral referenceAPI referenceReferenceClass reference\\n\\nSQL class reference¶\\nThese topics provide reference information for Snowflake classes.\\nEach class supports one or more of the following SQL operations:\\n\\nALTER: Modifies the properties of an instance of a class.\\nCREATE: Creates an instance of a class.\\nDROP: Deletes an instance of a class.\\nSHOW: Lists instances of a class.\\n\\nAn instance of a class can have one or more methods. A method is a stored procedure or function and can be called by\\nusing the instance name and method name, and arguments (if any) required by the method. For example,\\nCALL instance_name!method_name(...).\\n\\nUpdating your search path¶\\nYou can add the schema for classes you use frequently to your search path to save typing and make your SQL statements\\nmore concise. For more information about updating your search path, see Update your search path.\\n\\n\\nAvailable classes¶\\nSnowflake provides the following system-defined (built-in) classes.\\n\\nANOMALY_DETECTION (SNOWFLAKE.ML)Allows you to detect outliers in your time series data.\\n\\nBUDGET (SNOWFLAKE.CORE)Allows you to monitor credit usage of supported objects.\\n\\nCLASSIFICATION (SNOWFLAKE.ML)Automatically sorts data into categories based on features in the data.\\n\\nCUSTOM_CLASSIFIER (SNOWFLAKE.DATA_PRIVACY)Allows you to define custom classifiers to extend your data classification capabilities.\\n\\nDOCUMENT_INTELLIGENCE (SNOWFLAKE.ML)Represents a Document AI model build.\\n\\nFORECAST (SNOWFLAKE.ML)Represents a forecast model that produces a forecast for a single or multiple time series.\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageUpdating your search pathAvailable classesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/release-notes/deprecated-features', 'title': 'Deprecated Features | Snowflake Documentation', 'language': 'en'}, page_content=\"Deprecated Features | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusWhat's NewBehavior ChangesDeprecated FeaturesPreview FeaturesPerformance ImprovementsSQL ImprovementsClients, Drivers, and LibrariesMonthly ReleasesClient versions & support policySnowflake ConnectorsSnowflake Connector for Google Analytics Raw DataSnowflake Connector for Google Analytics Aggregate DataSnowflake Connector for ServiceNowSnowflake Connector for ServiceNow V2Snowflake Connector for MySQLSnowflake Connector for PostgreSQLNative SDK for ConnectorsNative SDK for Connectors Java libraryNative SDK for Connectors Java Test libraryNative SDK for Connectors Java TemplateNative SDK Example Java GitHub ConnectorReleasesDeprecated Features\\n\\nDeprecated Features¶\\nThe following Snowflake features and operating systems/programming languages are deprecated or pending deprecation:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlanned Deprecation\\nCurrent Status\\nFunctional Area\\nDescription\\nAdditional Notes\\n\\n\\n\\nJan 2024\\nPending\\nEcosystem\\nPer the Snowflake Support policy, any client version lower than the following will be out of support as of January 31, 2024:\\n\\nSnowSQL 1.2.21\\nPython Connector 2.7.3\\nSQLAlchemy (for Python) 1.3.3\\nJDBC Driver 3.13.14\\nODBC Driver 2.24.5\\nNode.JS Driver 1.6.6\\nGo Driver 1.6.6\\n.NET Driver 2.0.9\\nPHP PDO Driver 1.2.0\\nKafka Connector 1.6.5\\nSpark Connector 2.9.3\\n\\n\\nThe latest versions of all the clients can be downloaded from\\nDrivers and Libraries.\\n\\nOct 2023\\nPending\\nEcosystem\\nPer the Snowflake Support policy, any client version lower than the following will be out of support as of October 31, 2023:\\n\\nSnowSQL 1.2.20\\nPython Connector 2.7.0\\nSQLAlchemy (for Python) 1.3.2\\nJDBC Driver 3.13.10\\nODBC Driver 2.24.2\\nNode.JS Driver 1.6.4\\nGo Driver 1.6.3\\n.NET Driver 2.0.7\\nPHP PDO Driver 1.2.0\\nKafka Connector 1.6.0\\nSpark Connector 2.9.1\\n\\n\\nThe latest versions of all the clients, except SnowSQL and the ODBC Driver, can be downloaded from\\nDrivers and Libraries.\\nThe latest version of SnowSQL can be downloaded from SnowSQL Download.\\nThe latest version of the ODBC Driver can be downloaded from ODBC Download.\\n\\n\\nJul 2023\\nDeprecated Jul 31, 2023\\nEcosystem\\nPer the Snowflake Support policy, any client version lower than the following will be out of support as of July 31, 2023:\\n\\nSnowSQL 1.2.17\\nPython Connector 2.5.0\\nSQLAlchemy (for Python) 1.2.5\\nJDBC Driver 3.13.6\\nODBC Driver 2.23.3\\nNode.JS Driver 1.6.2\\nGo Driver 1.6.0\\n.NET Driver 2.0.4\\nPHP PDO Driver 1.1.0\\nKafka Connector 1.6.0\\nSpark Connector 2.9.1\\n\\n\\n\\n\\nApr 2023\\nDeprecated Apr 30, 2023\\nEcosystem\\nPer the Snowflake Support policy, any client version lower than the following is out of support as of April 30, 2023:\\n\\nSnowSQL 1.2.14\\nPython Connector 2.4.3\\nSQLAlchemy (for Python) 1.2.5\\nJDBC Driver 3.13.1\\nODBC Driver 2.23.2\\nNode.JS Driver 1.6.1\\nGo Driver 1.4.3\\n.NET Driver 2.0.0\\nPHP PDO Driver 1.0.2\\nKafka Connector 1.5.3\\nSpark Connector 2.8.\\n\\n\\n\\n\\nJan 2023\\nDeprecated Jan 31, 2023\\nEcosystem\\nPer the Snowflake Support policy, any client version lower than the following is out of support as of January 31, 2023:\\n\\nSnowSQL 1.2.10\\nPython Connector 2.3.9\\nSQLAlchemy (for Python) 1.2.4\\nJDBC Driver 3.12.17\\nODBC Driver 2.22.4\\nNode.JS Driver 1.5.3\\nGo Driver 1.3.13\\n.NET Driver 1.1.4\\nPHP PDO Driver 1.0.1\\nKafka Connector 1.5.2\\nSpark Connector 2.8.3\\n\\n\\n\\n\\nOct 2022\\nDeprecated Oct 2022\\nPending Obsolete Jan 2023\\nSQL, Ecosystem, Security\\nThese functions are deprecated and are no longer supported:\\n\\nSYSTEM$WHITELIST\\nSYSTEM$WHITELIST_PRIVATELINK\\n\\nUse these functions instead:\\n\\nSYSTEM$ALLOWLIST\\nSYSTEM$ALLOWLIST_PRIVATELINK\\n\\n\\n\\n\\nMay 2022\\nDeprecated May 11-12, 2022\\nSQL\\nThe following keywords for the CREATE PROCEDURE command are deprecated:\\n\\nIMMUTABLE\\nVOLATILE\\n\\n\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/admin-trial-account', 'title': 'Trial accounts | Snowflake Documentation', 'language': 'en'}, page_content='Trial accounts | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsOrganizationsIntroductionGetting StartedManaging AccountsAccountsConnecting to Your AccountsAccount IdentifiersTrial AccountsParameter ManagementUser ManagementBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesOrganizations & AccountsTrial Accounts\\n\\nTrial accounts¶\\nA Snowflake trial account lets you evaluate/test Snowflake’s full range of innovative and powerful features with no cost or contractual obligations. To sign\\nup for a trial account, all you need is a valid email address; no payment information or other qualifying information is required.\\n\\nSigning up for a trial account¶\\nYou can sign up for a free trial using the self-service form (on the Snowflake website).\\nWhen you sign up for a trial account, you select your cloud platform, region,\\nand Snowflake Edition. These selections can affect how quickly you exhaust your free usage balance. For example, some features available in the Enterprise Edition consume additional credits.\\nThe balance of your free usage decreases as you consume credits to use compute resources and accrue costs associated with storage. You can track your remaining balance at any time.\\nThe trial continues for 30 days (from the sign-up date) or until you’ve depleted your free usage balance, whichever occurs first. At any time during the trial, you can cancel the trial or convert the account to a paid account.\\nAt the end of the trial, the account is suspended. You can still log into a suspended account, but you cannot use any features, such as running a virtual warehouse,\\nloading data, or performing queries.\\nTo reactivate a suspended trial account, you must enter a credit card, which converts it to a paid account.\\n\\n\\nUsing compute resources¶\\nVirtual warehouses provide the compute power to load data and\\nperform queries. These warehouses consume credits, which reduces your free usage balance. To begin, simply start a warehouse; any credits consumed by the warehouse will be deducted from your balance. If your credit consumption fully depletes your free usage balance, you must add a credit card to the account to continue using Snowflake.\\nFree credits are only consumed by the virtual warehouses you create in your account, and only when they are running.\\n\\nTip\\nTo prevent unintentional usage of your free credits:\\n\\nVerify the size of your virtual warehouses before you start/resume them. The larger the warehouse, the more credits it consumes while running.\\nIn many situations, Small or Medium size warehouses are sufficient for evaluating Snowflake’s loading and querying capabilities.\\nDo not disable auto-suspend when creating a warehouse. Choosing a short auto-suspend time period (e.g. 5 minutes or less) can reduce credit consumption.\\n\\nFor additional tips on using your trial account:\\n\\nIn the left navigation bar, find the tile showing your remaining balance.\\nSelect … » Using your trial credits.\\n\\n\\nTrial accounts without a valid payment methods are limited to roughly one credit per day of usage of\\nSnowflake Cortex LLM functions.\\nTo remove this restriction, convert your trial account to a paid account.\\n\\n\\nUsing storage¶\\nAs you load data into your trial account, the cost of that storage is subtracted from your free usage balance based on the standard On-Demand cost of a TB in your cloud platform and region. In addition to the cost of storage, loading data also consumes credits as it uses the compute resources of a warehouse.\\n\\n\\nTutorials for trial accounts¶\\nThe following tutorials are available for trial accounts:\\n\\nCreate users and grant roles\\nLoad and query sample data using SQL\\nLoad and query sample data using Snowpark Python\\nLoad data from cloud storage: Amazon S3\\nLoad data from cloud storage: Microsoft Azure\\nLoad data from cloud storage: GCS\\n\\n\\n\\nTracking your remaining balance¶\\nUsers with the ACCOUNTADMIN role can track the remaining balance of their trial using a tile in the left navigation bar of\\nSnowsight.\\nFrom this tile you can also:\\n\\nSelect Upgrade to convert the trial account to a paid account.\\nSelect … » see organization usage details to access the Usage page, which allows you to drill down into your credit\\nconsumption and storage costs.\\nSelect the … button to access resources that help you get the most out of your trial account.\\n\\n\\n\\nConverting to a paid account¶\\nYou can add a credit card to a trial account at any time to convert it to a paid account.\\n\\nSign in to Snowsight.\\nDo one of the following:\\n\\nSelect Upgrade in the left navigation.\\nSelect Admin » Billing & Terms.\\n\\n\\nSelect Payment Methods.\\nSelect + Credit Card.\\nEnter the required information and select Add Card.\\n\\nWhen you enter a credit card, Snowflake verifies that the card is valid by charging $1 (USD). No other charges are billed at that time.\\nNote that you can also change the credit card for a trial account, at any time, using the same interface in which you added the card. Each time you\\nenter a new credit card, the new card will be charged $1 (USD).\\n\\nNote\\nAdding a credit card to a trial account converts it to a paid account without ending the trial period. During the remainder of the trial period, you can continue using your free credits and storage until the balance is exhausted, after which all additional credit consumption and storage costs will be charged.\\nUnused balances expire when the trial period ends, at which time costs (for consuming credits and storing data) are charged to the credit card on file at the end of each billing cycle (typically monthly).\\nFor pricing details, see the pricing page (on the Snowflake website).\\n\\n\\n\\nCanceling a trial account¶\\nYou can cancel a trial account at any time by contacting Snowflake Support\\nand requesting the account to be canceled.\\n\\nNote\\nCurrently, trial accounts cannot be canceled through the web interface. To cancel an account, you must contact Snowflake Support.\\n\\n\\n\\nCurrent limitations for trial accounts¶\\nThe following features are not available for trial accounts:\\n\\nTrust Center\\nBudgets\\nExternal network access\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageSigning up for a trial accountUsing compute resourcesUsing storageTutorials for trial accountsTracking your remaining balanceConverting to a paid accountCanceling a trial accountCurrent limitations for trial accountsRelated contentUnderstanding compute costRelated info\\n\\nGetting Started with Snowflake - Zero to Snowflake\\n\\nLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/', 'title': 'Snowflake Documentation', 'language': 'en'}, page_content='Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusSnowflake DocumentationStreamline your Snowflake journey with comprehensive documentation and learning resourcesFeatured ResourcesDive into our top picksuser guideConnecting to SnowflakeLearn about the applications and tools that you can use to access Snowflakeuser guideWorking with Virtual WarehousesLearn how to create and manage warehouses, which are used to process queriesuser guideDatabases, Tables and ViewsLearn how to create and manage databases, tables, and views for storing and accessing your datauser guideLoad Data into SnowflakeLearn about the different options for getting data into Snowflake and setting up a pipeline to transform your dataDiscover the Most Popular ResourcesGetting StartedBasic information and instructions for first-time users of Snowflake.Getting a Trial AccountSnowflake in 20 MinutesKey Concepts and ArchitectureConnecting to SnowflakeSee allUser GuidesInstructions on performing various Snowflake operations.Understanding & Using Time TravelWorking with Temporary and Transient TablesWorking with Materialized ViewsSee allDeveloper GuidesWrite applications that extend Snowflake, act as a client, or act as an integrating component.Snowflake Native App FrameworkSnowpark APIUser-defined Functions (UDFs) and Stored ProceduresDriversSee allReferenceReference for SQL data types, SQL commands, SQL functions, SQL classes, scripting, views, and other areasSQL Data TypesSQL CommandsSQL FunctionsSQL ClassesSnowflake ScriptingSee allTutorialsTutorials to help you learn the basics of using SnowflakeSnowflake in 20 MinutesBulk Loading from a Local File SystemBulk Loading from Amazon S3 Using COPYSee allReleasesOverview of the new features, enhancements, and important fixes introduced in the most recent releases of Snowflake.New featuresBehavior change logSnowflake connector, driver, and library monthly releasesSee allWhat’s NewOverview of the new features, enhancements, and important behavior changes introduced in the most recent releases of SnowflakeSee allRecent Product Updatesgenerally availableOn April 29, 2024Dynamic TablesWith this release, we are pleased to announce the general availability of dynamic tables, a new table type for continuous processing pipelines.generally availableOn April 11th, 2024BudgetsWith this release, we are pleased to announce the general availability of Budgets which enables account-level monitoring and notification of Snowflake credit usage for a group of specific Snowflake objects.generally availableOn April 23, 2024Snowflake Connector for ServiceNow® V2generally availableOn May 13, 2024ASOF JOINpreviewOn May 09, 2024Python user-defined aggregate functionspreviewOn May 03, 2024Snowflake Model RegistryAnnouncementsStay informed and discover the latest updates, improvements, and enhancementsBehavior ChangesBehavior changes that may impact your usageDeprecated FeaturesFeatures and operating systems/programming languages that are deprecated or pending deprecationPerformance ImprovementsAll features that can improve performance and make queries run fasterVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer', 'title': 'Developer - Snowflake Documentation', 'language': 'en'}, page_content='Developer - Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIMachine LearningSnowflake MLSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewReferenceAPI ReferenceDevelop Apps and ExtensionsWrite applications that extend Snowflake, act as a client, or act as an integrating component.SNOWPARK APIRun Python, Java, and Scala Code in SnowparkUsing Snowpark libraries and code execution environments, you can run Python and other programming languages next to your data in Snowflake.BuildEnable all data users to bring their work to a single platform with native support for Python, Java, Scala, and more.SecureApply consistent controls trusted by over 500 of the Forbes Global 2000 across all workloads.OptimizeBenefit from the Snowflake Data Cloud with super price/performance and near-zero maintenance.Get to know Snowpark APISnowpark is the set of libraries and code execution environments that run Python and other programming languages next to your data in Snowflake. Snowpark can be used to build data pipelines, ML models, apps, and other data processing tasks.Learn moreCode in Snowpark with multiple languagesRun custom Python, Java, or Scala code directly in Snowflake with Snowpark user-defined functions (UDFs) and stored procedures. There are no separate clusters to manage, scale, or operate.PythonJavaScalafrom snowflake.snowpark import Sessionfrom snowflake.snowpark.functions import col# Create a new session, using the connection properties specified in a file.new_session = Session.builder.configs(connection_parameters).create()# Create a DataFrame that contains the id, name, and serial_number# columns in the “sample_product_data” table.df = session.table(\"sample_product_data\").select(col(\"id\"), col(\"name\"), col(\"name\"), col(\"serial_number\"))# Show the results df.show()Developer GuideAPI Reference Try SnowparkUse the following quickstart tutorials to get a hands-on introduction to SnowparkTUTORIALGetting Started with Data Engineering and ML using Snowpark for PythonFollow this step-by-step guide to transform raw data into an interactive application using Python with Snowpark and Streamlit.TUTORIALData Engineering Pipelines with Snowpark PythonLearn how to build end-to-end data engineering pipelines using Snowpark with Python.TUTORIALIntro to Machine Learning with Snowpark MLBuild an end-to-end ML workflow from feature engineering to model training and batch inference using Snowpark ML.Snowflake MLEnd-to-End Machine Learning in SnowflakePre-process data and train, manage, and deploy machine learning models all within Snowflake.Developer GuideModel developmentTransform data and train models. Run your ML pipeline within security and governance frameworks.Developer GuideModel RegistrySecurely manage models and their metadata in Snowflake regardless of origin.Developer GuideFeature StoreMake creating, storing, and managing features for machine learning workloads easier and more efficient.Developer GuideDatasetsImmutable, versioned snapshots of data ready to be fed to popular machine learning frameworks.Developer GuideFramework ConnectorsProvide Snowflake data to PyTorch and Tensorflow in their own formats.API ReferenceSnowpark MLThe Python API for Snowflake ML modeling and ML Ops features.Snowflake Python APIManage Snowflake resources, apps, and data pipelinesCreate and manage Snowflake resources across data engineering, Snowpark, Snowpark ML, and application workloads using a unified, first-class Python API.Developer GuideSnowflake Python API overviewLearn about the Snowflake Python API and how to get started.TutorialGetting started with the Snowflake Python APILearn the fundamentals for creating and managing Snowflake resources using the Snowflake Python API.API ReferenceSnowflake Python API referenceReference for the Snowflake Python API.NATIVE APPS FRAMEWORKBuild secure data applicationsExpand the capabilities of other Snowflake features by sharing data and related business logic with other Snowflake accounts.TutorialDeveloping an Application with the Native Apps FrameworkFollow this step-by-step tutorial to create a secure data application using the Native Apps Framework.Developer GuideAbout the Native Apps FrameworkLearn about the building blocks of the Native Apps Framework, including key terms and components.Developer GuideNative Apps Framework WorkflowsUnderstand the end-to-end workflows for developing, publishing, and using applications.SQL ReferenceNative Apps Framework CommandsView the SQL commands used to create and use database objects supported by the Native Apps Framework.SNOWPARK CONTAINER SERVICESDeploy, manage, and scale containerized applicationsBuild atop a fully-managed service that comes with Snowflake security, configuration, and operational best practices built in.Developer GuideSnowpark Container Services OverviewLearn about Snowpark Container Services, including how it works and how to get started.TutorialIntroductory tutorialsLearn the basics of creating a Snowpark Container Services service.TutorialAdvanced tutorialsLearn advanced concepts such as service-to-service communications.STREAMLIT IN SNOWFLAKEDevelop custom web apps for machine learning and data scienceSecurely build, deploy, and share Streamlit apps on Snowflake’s data cloud.Developer GuideAbout Streamlit in SnowflakeLearn about deploying Streamlit apps by using Streamlit in Snowflake.Developer GuideExample - Accessing Snowflake data from Streamlit in Snowflake Learn how to securely access Snowflake data from a Streamlit app.Developer GuideDeveloping a Streamlit app by using SnowsightLearn how to quickly create, use, and share a Streamlit app in Snowsight. FUNCTIONS AND PROCEDURESExtend Snowflake CapabilitiesEnhance and extend Snowflake by writing procedures and user-defined functions. In both cases, you write the logic in one of the supported programming languages.Developer GuideStored Procedures or UDFsUnderstand key differences between procedures and UDFs.Developer GuideStored ProceduresPerform scheduled or on-demand operations by executing code or SQL statements.Developer GuideUser-Defined Functions (UDFs)Run logic to calculate and return data for batch processing and integrating custom logic into SQL.Developer GuideDesign GuidelinesGeneral guidelines on security, conventions, and more.Developer GuidePackaging Handler CodeBuild a JAR file that contains the handler and its dependencies. Reference the handler JAR on a stage.Developer GuideWriting External FunctionsWriting external functions you can use to invoke code on other systems.Developer GuideLogging and TracingCapture log and trace messages in an event table that you can query for analysis later.Developer GuideExternal Network AccessA guide for accessing network locations external to Snowflake. KAFKA AND SPARK CONNECTORSIntegrate with Other SystemsSnowflake includes connectors with APIs for integrating with systems outside Snowflake.User GuideSnowflake EcosystemIntegrate Snowflake with many other systems for exchanging data, performing analysis, and more.User GuideApache KafkaSend events from the Kafka event streaming platform to Snowflake.User GuideApache SparkIntegrate the Apache Spark analytics engine in Spark workloads for data processing directly on Snowflake. DRIVERSBuild a Client App with Drivers and APIsIntegrate Snowflake operations into a client app. In addition to the Snowpark API, you can also use language and platform specific drivers.DriversDrivers allow you to connect from your code or apps to Snowflake. Using languages such as C#, Go, and Python, you can write applications that perform operations on Snowflake.Go Snowflake DriverJDBC Driver.NET DriverNode.js DriverODBC DriverPHP PDO DriverPython ConnectorRESTful APIUsing the Snowflake RESTful SQL API, you can access and update data over HTTPS and REST. For example, you can submit SQL statements, create and execute stored procedures, provision users, and so on.In the SQL REST API, you submit a SQL statement for execution in the body of a POST request. You then check execution status and fetch results with GET requests.DEVELOPER GUIDESnowflake SQL REST APIGet started with the Snowflake SQL REST API.TOOLSDevelop more efficientlyWork with Snowflake using tools that integrate well with your existing workflow. Work with Snowflake from the command lineUse the command line to create, manage, update, and view apps running on Snowflake across workloads.DEVELOPER GUIDEIntroducing Snowflake CLILearn about Snowflake CLI benefits and how it differs from SnowSQL.DEVELOPER GUIDEInstalling Snowflake CLIInstall Snowflake CLI using common package managers.REFERENCESnowflake CLI command referenceExplore commands for connecting, managing apps, objects, and other Snowflake features. Use Git from SnowflakeExecute and use Git repository code directly from Snowflake.DEVELOPER GUIDEUsing a Git repository in SnowflakeIntegrate your Git repository with Snowflake and fetch repository files to a repository stage that is a Git client with a full clone of the repository.DEVELOPER GUIDESetting up Snowflake to use GitSet up Snowflake to securely interact with your Git repository.DEVELOPER GUIDEGit operations in SnowflakePerform common Git operations from within Snowflake, including fetching files, viewing branches or tags, and executing repository code.Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/sql-reference-data-types', 'title': 'SQL data types reference | Snowflake Documentation', 'language': 'en'}, page_content='SQL data types reference | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSummaryNumericString & binaryLogicalDate & timeSemi-structuredStructuredGeospatialVectorUnsupportedConversionSQL command referenceFunction and stored procedure referenceClass referenceScripting referenceGeneral referenceAPI referenceReferenceSQL data types reference\\n\\nSQL data types reference¶\\nSnowflake supports most basic SQL data types (with some restrictions) for use in columns, local variables, expressions, parameters,\\nand any other appropriate/suitable locations.\\n\\nNote\\nYou can also load unstructured data into Snowflake. For more information, see Introduction to unstructured data.\\n\\nIn some cases, data of one type can be converted to another type. For example, INTEGER data can be converted to FLOAT.\\nSome conversions are lossless, but others might lose information. The amount of loss depends upon the data types and the specific\\nvalue. For example, converting FLOAT to INTEGER removes the digits after the decimal place. (The value is rounded to the nearest\\ninteger.)\\nIn some cases, the user must specify the desired conversion, such as when passing a VARCHAR to the TIME_SLICE() function,\\nwhich expects a TIMESTAMP. We call this “explicit casting”. For more information about explicit casting, see\\nConversion functions.\\nIn other cases, data types are converted automatically, such as when adding a float and an integer. We call this\\n“implicit coercion”. In Snowflake, data types are automatically coerced whenever necessary and possible. For more\\ninformation about implicit coercion, see Data type conversion.\\nFor more information about Snowflake data types, see the following topics:\\n\\nSummary of data types\\nNumeric data types\\nString & binary data types\\nLogical data types\\nDate & time data types\\nSemi-structured data types\\nStructured data types\\nGeospatial data types\\nVector data types\\nUnsupported data types\\nData type conversion\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentString & binary functionsDate & time functionsSemi-structured and structured data functionsGeospatial functionsConversion functionsData type conversionUnstructured DataLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/tables-temp-transient', 'title': 'Working with Temporary and Transient Tables | Snowflake Documentation', 'language': 'en'}, page_content='Working with Temporary and Transient Tables | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsTable StructuresTemporary and Transient TablesExternal TablesIceberg TablesHybrid TablesWorking with tables in SnowsightSearch optimization serviceViewsViewsSecure ViewsMaterialized ViewsWorking with ViewsConsiderationsViews, Materialized Views, and Dynamic TablesTable DesignCloningData StorageData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesDatabases, Tables, & ViewsTemporary and Transient Tables\\n\\nWorking with Temporary and Transient Tables¶\\nIn addition to permanent tables, which is the default table type when creating tables, Snowflake supports defining tables as either temporary or\\ntransient. These types of tables are especially useful for storing data that does not need to be maintained for extended periods of time\\n(i.e. transitory data).\\n\\nNote\\nYou cannot create hybrid tables that are temporary or transient. In turn, you cannot create hybrid tables within transient schemas or databases.\\n\\n\\nTemporary Tables¶\\nSnowflake supports creating temporary tables for storing non-permanent, transitory data (e.g. ETL data, session-specific data). Temporary tables\\nonly exist within the session in which they were created and persist only for the remainder of the session. As such, they are not visible to other\\nusers or sessions. Once the session ends, data stored in the table is purged completely from the system and, therefore, is not recoverable, either\\nby the user who created the table or Snowflake.\\n\\nNote\\nIn addition to tables, Snowflake supports creating certain other database objects as temporary (e.g. stages). These objects follow the same\\nsemantics (i.e. they are session-based, persisting only for the remainder of the session).\\n\\n\\nData Storage Usage for Temporary Tables¶\\nFor the duration of the existence of a temporary table, the data stored in the table contributes to the overall storage charges that Snowflake bills\\nyour account. To prevent any unexpected storage changes, particularly if you create large temporary tables in sessions that you maintain for periods\\nlonger than 24 hours, Snowflake recommends explicitly dropping these tables once they are no longer needed. You can also explicitly exit the session\\nin which the table was created to ensure no additional charges are accrued.\\nFor more information, see Comparison of Table Types (in this topic).\\n\\n\\nPotential Naming Conflicts with Other Table Types¶\\nSimilar to the other table types (transient and permanent), temporary tables belong to a specified database and schema; however, because they are\\nsession-based, they aren’t bound by the same uniqueness requirements. This means you can create temporary and non-temporary tables with the same name\\nwithin the same schema.\\nHowever, note that the temporary table takes precedence in the session over any other table with the same name in the same schema. This can lead to\\npotential conflicts and unexpected behavior, particularly when performing DDL on both temporary and non-temporary tables. For example:\\n\\nYou can create a temporary table that has the same name as an existing table in the same schema, effectively hiding the existing table.\\nYou can create a table that has the same name as an existing temporary table in the same schema; however, the newly-created table is hidden by the\\ntemporary table.\\n\\nSubsequently, all queries and other operations performed in the session on the table affect only the temporary table.\\n\\nImportant\\nThis behavior is particularly important to note when dropping a table in a session and then using Time Travel to restore the table. It is also\\nimportant to note this behavior when using CREATE OR REPLACE to create a table because this essentially drops a table (if it exists) and creates a\\nnew table with the specified definition.\\n\\n\\n\\nCreating a Temporary Table¶\\nTo create a temporary table, simply specify the TEMPORARY keyword (or TEMP abbreviation) in CREATE TABLE.\\nNote that creating a temporary table does not require the CREATE TABLE privilege on the schema in which the object is created.\\nFor example:\\n\\nCREATE TEMPORARY TABLE mytemptable (id NUMBER, creation_date DATE);\\n\\nCopy\\n\\n\\nNote\\nAfter creation, temporary tables cannot be converted to any other table type.\\n\\n\\n\\n\\nTransient Tables¶\\nSnowflake supports creating transient tables that persist until explicitly dropped and are available to all users with the appropriate privileges.\\nTransient tables are similar to permanent tables with the key difference that they do not have a Fail-safe period. As a result, transient tables\\nare specifically designed for transitory data that needs to be maintained beyond each session (in contrast to temporary tables), but does not\\nneed the same level of data protection and recovery provided by permanent tables.\\n\\nData Storage Usage for Transient Tables¶\\nSimilar to permanent tables, transient tables contribute to the overall storage charges that Snowflake bills your account; however, because\\ntransient tables do not utilize Fail-safe, there are no Fail-safe costs (i.e. the costs associated with maintaining the data required for\\nFail-safe disaster recovery).\\nFor more information, see Comparison of Table Types (in this topic).\\n\\n\\nTransient Tables Created as Clones of Permanent Tables¶\\nWhen you create a transient table as a clone of a permanent table, Snowflake creates a zero-copy clone.\\nThis means when the transient table is created, it utilizes no data storage because it shares all of the existing\\nmicro-partitions of the original permanent table.\\nWhen rows are added, deleted, or updated in the clone, it results in new micro-partitions that belong exclusively\\nto the clone (in this case, the transient table).\\nWhen a permanent table is deleted, it enters Fail-safe for a 7-day period. Fail-safe bytes incur\\nstorage costs. If a transient table is created as a clone of a permanent table, this\\nmight delay the time between when the permanent table is deleted and when all of its bytes enter Fail-safe. If the transient\\ntable clone shares any micro-partitions with the permanent table when it is deleted, those shared bytes will only enter\\nFail-safe when the transient table is deleted.\\n\\n\\nTransient Databases and Schemas¶\\nSnowflake also supports creating transient databases and schemas. All tables created in a transient schema, as well as all schemas created in\\na transient database, are transient by definition.\\n\\n\\nCreating a Transient Table, Schema, or Database¶\\nTo create a transient table, schema, database, simply specify the TRANSIENT keyword when creating the object:\\n\\nCREATE TABLE\\nCREATE SCHEMA\\nCREATE DATABASE\\n\\nFor example, to create a transient table:\\n\\nCREATE TRANSIENT TABLE mytranstable (id NUMBER, creation_date DATE);\\n\\nCopy\\n\\n\\nNote\\nAfter creation, transient tables cannot be converted to any other table type.\\n\\n\\n\\n\\nComparison of Table Types¶\\nThe following table summarizes the differences between the three table types, particularly with regard to their impact on Time Travel and\\nFail-safe:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nType\\nPersistence\\nCloning (source type => target type)\\nTime\\xa0Travel\\xa0Retention\\xa0Period\\xa0(Days)\\nFail-safe\\xa0Period\\xa0(Days)\\n\\n\\n\\nTemporary\\nRemainder of session\\nTemporary => Temporary . . Temporary => Transient\\n0 or 1 (default is 1)\\n0\\n\\nTransient\\nUntil explicitly dropped\\nTransient => Temporary . . Transient => Transient\\n0 or 1 (default is 1)\\n0\\n\\nPermanent (Standard Edition)\\nUntil explicitly dropped\\nPermanent => Temporary . . Permanent => Transient . . Permanent => Permanent\\n0 or 1 (default is 1)\\n7\\n\\nPermanent (Enterprise Edition and higher)\\nUntil explicitly dropped\\nPermanent => Temporary . . Permanent => Transient . . Permanent => Permanent\\n0 to 90 (default is configurable)\\n7\\n\\n\\n\\n\\nTime Travel Notes¶\\n\\nThe Time Travel retention period for a table can be specified when the table is created or any time afterwards. Within the retention period,\\nall Time Travel operations can be performed on data in the table (e.g. queries) and the table itself (e.g. cloning and restoration).\\nIf the Time Travel retention period for a permanent table is set to 0, it will immediately enter the Fail-safe period when it is dropped.\\nTemporary tables can have a Time Travel retention period of 1 day; however, a temporary table is purged once the session (in which the table\\nwas created) ends so the actual retention period is for 24 hours or the remainder of the session, whichever is shorter.\\nA long-running Time Travel query will delay the purging of temporary and transient tables until the query completes.\\n\\n\\n\\nFail-safe Notes¶\\n\\nThe Fail-safe period is not configurable for any table type.\\nTransient and temporary tables have no Fail-safe period. As a result, no additional data storage charges are incurred beyond the\\nTime Travel retention period.\\n\\n\\nImportant\\nBecause transient tables do not have a Fail-safe period, they provide a good option for managing the cost of very large tables used to store\\ntransitory data; however, the data in these tables cannot be recovered after the Time Travel retention period passes.\\nFor example, if a system failure occurs in which a transient table is dropped or lost, after 1 day, the data is not recoverable by you or\\nSnowflake. As such, we recommend using transient tables only for data that does not need to be protected against failures or data that\\ncan be reconstructed outside of Snowflake.\\nFor more information, see Data storage considerations.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageTemporary TablesTransient TablesComparison of Table TypesRelated contentUnderstanding & using Time TravelUnderstanding and viewing Fail-safeStorage Costs for Time Travel and Fail-safeLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/guides-overview-loading-data', 'title': 'Load Data into Snowflake | Snowflake Documentation', 'language': 'en'}, page_content='Load Data into Snowflake | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingLoading Semi-Structured DataIntroductionSupported FormatsConsiderationsAccessing Unstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData Loading\\n\\nLoad Data into Snowflake¶\\nData can be loaded into Snowflake in a number of ways.\\nThe following topics provide an overview of data loading concepts, tasks, tools, and techniques to quick and easily load data into your Snowflake database.\\n\\nOverview of data loadingOptions available to load data into Snowflake.\\n\\nSummary of data loading featuresReference of the supported features for using the COPY INTO <table> command to load data from files.\\n\\nData loading considerationsBest practices, general guidelines, and important considerations for bulk data loading.\\n\\nWorking with Amazon S3-compatible storageInstructions for accessing data in other storage.\\n\\nLoading data using the web interfaceInstructions for loading limited amounts of data using the web interface.\\n\\nIntroduction to Loading Semi-structured DataConsiderations for loading semi-structured data.\\n\\nIntroduction to unstructured dataConsiderations for loading unstructured data.\\n\\nBulk loading from a local file systemInstructions for loading data in bulk using the COPY command.\\n\\nSnowpipeInstructions for loading data continuously using Snowpipe.\\n\\nSnowpipe StreamingInstructions for loading data streams continuously using Snowpipe Streaming.\\n\\nTransforming data during a loadInstructions for transforming data while loading it into a table using the COPY INTO command.\\n\\nQuerying Data in Staged FilesInstructions on using standard SQL to query internal and external named stages.\\n\\nQuerying Metadata for Staged FilesInstructions on querying metadata in internal and external stages.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentSnowflake Partner ConnectOverview of data unloadingData Manipulation Language (DML) commandsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview', 'title': 'Snowflake Model Registry | Snowflake Documentation', 'language': 'en'}, page_content='Snowflake Model Registry | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIMachine LearningSnowflake MLBuilt-in ML FunctionsSnowpark MLModelingSnowflake Model RegistryModel Registry SQL ReferenceLogging Custom ModelsPartitioned Custom ModelsSpecifying Model SignaturesHugging Face Pipeline SignaturesModel Registry UIMigrating from Preview APISnowflake Feature StoreDatasetsFileSystem and FileSetFramework ConnectorsSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewReferenceAPI ReferenceDeveloperSnowflake MLSnowflake Model Registry\\n\\nSnowflake Model Registry¶\\n\\nNote\\nThe model registry API described in this topic is generally available as of package version 1.5.0.\\n\\nThe Snowflake Model Registry lets you securely manage models and their metadata in Snowflake, regardless of origin. The\\nmodel registry stores machine learning models as first-class schema-level objects in Snowflake so they can easily be\\nfound and used by others in your organization. You can create registries, and store models in them, using classes in the\\nSnowpark ML library. Models can have multiple versions, and you can designate a version as the default.\\nOnce you have stored a model, you can invoke its methods (equivalent to functions or stored procedures) to perform\\nmodel operations, such as inference, in a Snowflake virtual warehouse.\\n\\nTip\\nSee Introduction to Machine Learning with Snowpark ML\\nfor an example of an end-to-end workflow in Snowpark ML, including the Snowflake Model Registry.\\nIf you have models in Microsoft Azure Machine Learning or in Amazon SageMaker, see Deploying Models from Azure ML and SageMaker to Snowpark ML.\\n\\nThe most important classes in the Snowflake Model Registry API are:\\n\\nsnowflake.ml.registry.Registry:\\nManages models within a schema.\\nsnowflake.ml.model.Model:\\nRepresents a model.\\nsnowflake.ml.model.ModelVersion:\\nRepresents a version of a model.\\n\\nThe Snowflake Model Registry supports the following types of models.\\n\\nSnowpark ML Modeling\\nscikit-learn\\nXGBoost\\nLightGBM\\nCatBoost\\nPyTorch\\nTensorFlow\\nMLFlow PyFunc\\nSentence Transformer\\nHugging Face pipeline\\nOther types of models via the snowflake.ml.model.CustomModel class (see Storing Custom Models in the Snowflake Model Registry).\\n\\nThis topic describes how to perform registry operations in Python using Snowpark ML. You can also perform many registry\\noperations in SQL; see Model commands.\\n\\nDifferences from Private Preview¶\\nSnowflake previously made a model registry available privately to select customers. The registry feature described in\\nthis topic has significant changes to functionality and APIs compared to the private preview version. Most notably, the\\nmodel registry functionality is now hosted natively inside Snowflake using a new schema-level object.\\nFor details on the differences between these two APIs, see Migrating from the Snowflake Model Registry Preview API.\\n\\n\\nRequired Privileges¶\\nTo create a model, you must either own the schema where the model is created or have the CREATE MODEL privilege on it.\\nTo use a model, you must either own the model or have the USAGE privilege on it. The USAGE privilege allows grantees to\\nuse the model for inference without being able to see any of its internals.\\nIf a user’s role has USAGE on a model, it appears in Snowsight’s model registry page.\\nFor details, see Access control privileges.\\n\\nNote\\nModels currently do not support replication.\\n\\n\\n\\nCurrent Limitations and Issues¶\\nThe Snowflake Model Registry currently has the following limitations:\\n\\nThe registry cannot be used in Snowflake Native Apps.\\nModels cannot be shared or cloned, and are skipped during replication.\\n\\nVersions 1.5.0 and 1.5.1 of the snowflake-ml-python package have the following known issues. Until these are\\naddressed, use the provided workaround.\\n\\nIn Snowflake release 8.23 and earlier, the library does not work in\\nowner’s rights stored procedures.\\nUse caller’s rights stored procedures instead.\\nIn stored procedures, logging a model requires embedding a copy of the local Snowpark ML\\nlibrary in the model. Specify the embed_local_ml_library option in the log_model call as follows.\\nregistry.log_model(..., options={\"embed_local_ml_library\": True, ...})\\n\\nCopy\\n\\n\\nThe following limits apply to models and model versions.\\n\\n\\nModels\\n\\nMaximum of 50 versions\\n\\n\\n\\nModel versions\\n\\nMaximum of 10 methods\\nMaximum of 10 imports\\nMaximum of 500 arguments per method\\nMaximum metadata (including metrics) of 100 KB\\nMaximum total model size of 5 GB\\nMaximum config file size of 250 KB, including conda.yml and other manifest files that log_model generates internally.\\n(If a model has many functions and all of them have many arguments, for example, this limit might be exceeded.)\\n\\n\\n\\n\\n\\n\\n\\nOpening the Snowflake Model Registry¶\\nModels are first-class Snowflake objects and can be organized within a database and schema along with other Snowflake\\nobjects. The Snowflake Model Registry provides a Python class for managing models within a schema. Thus, any Snowflake schema\\ncan be used as a registry. It is not necessary to initialize or otherwise prepare a schema for this purpose. Snowflake\\nrecommends creating one or more dedicated schemas for this purpose, such as ML.REGISTRY. You can create the schema using\\nCREATE SCHEMA.\\nBefore you can create or modify models in the registry, you must open the registry. Opening the registry returns a\\nreference to it, which you can then use to add new models and obtain references to existing models.\\nfrom snowflake.ml.registry import Registry\\n\\nreg = Registry(session=sp_session, database_name=\"ML\", schema_name=\"REGISTRY\")\\n\\nCopy\\n\\n\\nRegistering Models and Versions¶\\nAdding a model to the registry is called logging the model. Log a model by calling the registry’s log_model\\nmethod. This method:\\n\\nSerializes the model, a Python object, and creates a Snowflake model object from it.\\nAdds metadata, such as a description, to the model as specified in the log_model call.\\n\\nEach model may have any number of versions. To log additional versions of the model, call log_model again with\\nthe same model_name but a different version_name.\\nYou cannot add tags to a model when it is added to the registry, because tags are attributes of the model, and\\nlog_model adds a specific model version, only creating a model when adding its first version. You can update\\nthe model’s tags after logging the first version of the model.\\nIn the example below, clf, short for “classifier,” is the Python model object, which has already been\\ncreated elsewhere in your code. You can add a comment at registration time, as shown here. The combination of\\nname and version must be unique in the schema. You may specify conda_dependencies lists; the\\nspecified packages will be deployed with the model.\\nmv = reg.log_model(clf,\\n                   model_name=\"my_model\",\\n                   version_name=\"v1\",\\n                   conda_dependencies=[\"scikit-learn\"],\\n                   comment=\"My awesome ML model\",\\n                   metrics={\"score\": 96},\\n                   sample_input_data=train_features)\\n\\nCopy\\nThe arguments of log_model are described here.\\nRequired Arguments\\n\\n\\nArgument\\nDescription\\n\\n\\n\\nmodel\\nThe Python model object of a supported model type. Must be serializable (“pickleable”).\\n\\nmodel_name\\nThe model’s name, used with version_name to identify the model in the registry. The name cannot be\\nchanged after the model has been logged. Must be a valid Snowflake identifier.\\n\\n\\n\\n\\nNote\\nThe combination of model name and version must be unique in the schema.\\n\\nOptional Arguments\\n\\n\\nArgument\\nDescription\\n\\n\\n\\nversion_name\\nString specifying the model’s version, used with model_name to identify the model in the registry.\\nMust be a valid Snowflake identifier. If missing, a human-readable\\nversion name is generated automatically.\\n\\ncode_paths\\nList of paths to directories of code to import when loading or deploying the model.\\n\\ncomment\\nComment, for example a description of the model.\\n\\nconda_dependencies\\nList of Conda packages required by your model. This argument specifies package names\\nand optional versions in Conda format,\\nthat is, \"[channel::]package [operator version]\". If you do not specify a channel, the Snowflake channel is assumed.\\n\\next_modules\\nList of external modules to pickle with the model. Supported with scikit-learn, Snowpark ML, PyTorch, TorchScript, and custom models.\\n\\nmetrics\\nDictionary containing metrics linked to the model version.\\n\\noptions\\nDictionary containing options for model creation. The following options are available for all model types.\\n\\nembed_local_ml_library: whether to embed a copy of the local Snowpark ML library into the model. Default: False.\\nrelax_version: whether to relax the version constraints of the dependencies. This replaces version specifiers like\\n==x.y.z with specifiers like <=x.y, <(x+1). Default: True.\\nmethod_options: A dictionary of per-method options, where the key is the name of a method and the value is a dictionary\\ncontaining one or more of the options described here. The available options are:\\n\\ncase_sensitive: Indicates whether the method and its signature are case-sensitive. Case-sensitive methods must be double-quoted\\nwhen used in SQL. This option also allows non-alphabetic characters in method names. Default: False.\\nmax_batch_size: Maximum batch size that the method will accept when called in the warehouse. Default: None (the batch\\nsize is automatically determined)\\n\\n\\n\\nIndividual model types may support additional options. See Notes on Specific Model Types.\\n\\n\\npip_requirements\\nList of package specs for PyPI packages required by your model.\\n\\npython_version\\nThe version of Python in which the model will run. Defaults to None, which designates the latest version\\navailable in the warehouse.\\n\\nsample_input_data\\nA DataFrame containing sample input data. The feature names required by the model, and their types, are\\nextracted from this DataFrame. Either this argument or signatures must be provided for all models except Snowpark ML\\nand MLFlow models and Hugging Face pipelines.\\n\\nsignatures\\nModel method signatures as a mapping from target method name to signatures of input and output. Either this argument or\\nsample_input_data must be provided for all models except Snowpark ML and MLFlow models and Hugging Face pipelines.\\n\\n\\n\\nlog_model returns a snowflake.ml.model.ModelVersion object, which represents the version of the model\\nthat was added to the registry.\\nOnce registered, the model itself cannot be modified (although you can change its metadata). To delete a model and all\\nits versions, use the registry’s delete_model method.\\n\\n\\nWorking with Model Artifacts¶\\nAfter a model has been logged, its artifacts (the files backing the model, including its serialized Python objects and\\nvarious metadata files such as its manifest) are available on an internal stage. Artifacts cannot be modified, but you\\ncan view or download the artifacts of models you own.\\n\\nNote\\nHaving the USAGE privilege on a model does not allow you to access its artifacts; ownership is required.\\n\\nYou can access model artifacts from a stage using, for example, the GET command\\nor its equivalent in Snowpark Python,\\nFileOperation.get.\\nHowever, you cannot address model artifacts using the usual stage path syntax. Instead, use a snow:// URL, a more\\ngeneral way to specify the location of objects in Snowflake. For example, a version inside a model can be specified by a\\nURL of the form snow://model/<model_name>/versions/<version_name>/.\\nKnowing the of name of the model and the version you want, you can use the\\nLIST command to view the artifacts of the model as follows:\\nLIST \\'snow://model/my_model/versions/V3/\\';\\n\\nCopy\\nThe output resembles:\\nname                                      size                  md5                      last_modified\\nversions/V3/MANIFEST.yml           30639    2f6186fb8f7d06e737a4dfcdab8b1350        Thu, 18 Jan 2024 09:24:37 GMT\\nversions/V3/functions/apply.py      2249    e9df6db11894026ee137589a9b92c95d        Thu, 18 Jan 2024 09:24:37 GMT\\nversions/V3/functions/predict.py    2251    132699b4be39cc0863c6575b18127f26        Thu, 18 Jan 2024 09:24:37 GMT\\nversions/V3/model.zip             721663    e92814d653cecf576f97befd6836a3c6        Thu, 18 Jan 2024 09:24:37 GMT\\nversions/V3/model/env/conda.yml          332        1574be90b7673a8439711471d58ec746        Thu, 18 Jan 2024 09:24:37 GMT\\nversions/V3/model/model.yaml       25718    33e3d9007f749bb2e98f19af2a57a80b        Thu, 18 Jan 2024 09:24:37 GMT\\n\\n\\nTo retrieve one of these artifacts, use the SQL GET command.\\nGET \\'snow://model_my_model/versions/V3/MANIFEST.yml\\'\\n\\nCopy\\nOr the equivalent with Snowpark Python:\\nsession.file.get(\\'snow://model_my_model/versions/V3/MANIFEST.yml\\', \\'model_artifacts\\')\\n\\nCopy\\n\\nNote\\nThe names and organization of a model’s artifacts can vary depending on the type of the model and might change.\\nThe example artifact list above is intended to be illustrative, not authoritative.\\n\\n\\n\\nDeleting Models¶\\nUse the registry’s delete_model method to delete a model and all its versions.\\nreg.delete_model(\"mymodel\")\\n\\nCopy\\n\\nTip\\nYou can also delete models in SQL using DROP MODEL.\\n\\n\\n\\nGetting Models from the Registry¶\\nTo get information about each model, use the show_models method.\\nmodel_df = reg.show_models()\\n\\nCopy\\n\\nTip\\nIn SQL, use SHOW MODELS to get a list of models.\\n\\nThe result of show_models is a pandas DataFrame. The available columns are shown below.\\n\\n\\n\\n\\n\\n\\nColumn\\nDescription\\n\\n\\n\\ncreated_on\\nDate and time when the model was created.\\n\\nname\\nName of the model.\\n\\ndatabase_name\\nDatabase in which the model is stored.\\n\\nschema_name\\nSchema in which the model is stored.\\n\\nowner\\nRole that owns the model.\\n\\ncomment\\nComment for the model.\\n\\nversions\\nJSON array listing versions of the model.\\n\\ndefault_version_name\\nVersion of the model used when referring to the model without a version.\\n\\n\\n\\nTo get a list of the models in the registry instead, each as a Model instance, use the models method.\\nmodel_list = reg.models()\\n\\nCopy\\nTo get a reference to a specific model from the registry by name, use the registry’s get_model method.\\nm = reg.get_model(\"MyModel\")\\n\\nCopy\\n\\nNote\\nModel instances are not copies of the original logged Python model object, but references to the underlying\\nmodel object in the registry.\\n\\nOnce you have a reference to a model, either one from the list returned by models method or one retrieved using\\nget_model, you can work with its metadata and\\nits versions.\\n\\n\\nViewing and Updating a Model’s Metadata¶\\nYou can view and update a model’s metadata attributes in the registry, including its name, comment, tags, and metrics.\\n\\nRetrieving and Updating Comments¶\\nUse the model’s comment attribute to retrieve and update the model’s comment.\\nprint(m.comment)\\nm.comment = \"A better description than the one I provided originally\"\\n\\nCopy\\n\\nNote\\nThe description attribute is a synonym for comment. The above code can also be written:\\nprint(m.description)\\nm.description = \"A better description than the one I provided originally\"\\n\\nCopy\\n\\n\\nTip\\nYou can also set a model’s comment in SQL using ALTER MODEL.\\n\\n\\n\\nRetrieving and Updating Tags¶\\nTags are metadata used to record a model’s purpose, algorithm, training data set, lifecycle stage, or other information\\nyou choose. You can set tags when the model is registered or at any time afterward. You can also update the values of\\nexisting tags or remove tags entirely.\\n\\nNote\\nThe names of all tags (and potentially their possible values) must be defined ahead of time using CREATE TAG. See\\nObject Tagging.\\n\\nTo get all of a model’s tags as a Python dictionary, use show_tags.\\nprint(m.show_tags())\\n\\nCopy\\nTo add a new tag or change the value of an existing tag, use set_tag.\\nm.set_tag(\"live_version\", \"v1\")\\n\\nCopy\\nTo retrieve the value of a tag, use get_tag:\\nm.get_tag(\"live_version\")\\n\\nCopy\\nTo remove a tag, use unset_tag.\\nm.unset_tag(\"live_version\")\\n\\nCopy\\n\\nTip\\nYou can also set a model’s comment in SQL using ALTER MODEL.\\n\\n\\n\\nRenaming a Model¶\\nUse the rename method to rename or move a model. Specify a fully qualified name as the new name to move the model to\\na different database or schema.\\nm.rename(\"MY_MODEL_TOO\")\\n\\nCopy\\n\\nTip\\nYou can also rename a model in SQL using ALTER MODEL.\\n\\n\\n\\n\\nWorking with Model Versions¶\\nA model may have any number of versions, each identified by a string. You can use any version naming convention that you\\nlike. Logging a model actually logs a specific version of the model. To log additional versions of a model, call\\nlog_model again with the same model_name but a different version_name.\\n\\nTip\\nIn SQL, use SHOW VERSIONS IN MODEL to see the versions of a model.\\n\\nA version of a model is represented by an instance of the snowflake.ml.model.ModelVersion class.\\nTo get a list of all the versions of a model, call the model object’s versions method. The result is a list of\\nModelVersion instances.\\nversion_list = m.versions()\\n\\nCopy\\nTo get information about each model as a DataFrame instead, call the model’s show_versions method.\\nversion_df = m.show_versions()\\n\\nCopy\\nThe resulting DataFrame contains the following columns.\\n\\n\\n\\n\\n\\n\\nColumn\\nDescription\\n\\n\\n\\ncreated_on\\nDate and time when the model version was created.\\n\\nname\\nName of the version.\\n\\ndatabase_name\\nDatabase in which the version is stored.\\n\\nschema_name\\nSchema in which the version is stored.\\n\\nmodel_name\\nName of the model that this version belongs to.\\n\\nis_default_version\\nBoolean value indicating whether this version is the model’s default version.\\n\\nfunctions\\nJSON array of the names of the functions available in this version.\\n\\nmetadata\\nJSON object containing metadata as key-value pairs ({} if no metadata is specified).\\n\\nuser_data\\nJSON object from the user_data section of the model definition manifest ({} if no user data is specified).\\n\\n\\n\\n\\nDeleting Model Versions¶\\nYou can delete a model version using the model’s delete_version method.\\nm.delete_version(\"rc1\")\\n\\nCopy\\n\\nTip\\nYou can also delete a model version in SQL using ALTER MODEL … DROP VERSION.\\n\\n\\n\\nDefault Version¶\\nA version of a model may be designated as the default model. Retrieve or set the model’s default attribute to obtain\\nthe current default version (as a ModelVersion object) or to change it (using a string).\\ndefault_version = m.default\\nm.default = \"v2\"\\n\\nCopy\\n\\nTip\\nIn SQL, use ALTER MODEL to set the default version.\\n\\n\\n\\nModel Version Aliases¶\\nYou can assign an alias to a model version using the SQL ALTER MODEL command.\\nYou can use an alias wherever a version name is required, such as when getting a reference to a model version, in Python\\nor in SQL.\\nIn addition to aliases you create, the following system aliases are available in all models.\\n\\nDEFAULT refers to the default version of the model.\\nFIRST refers to the oldest version of the model by creation time.\\nLAST refers to the newest version of the model by creation time.\\n\\nAlias names you create may not be the same as any existing version name or alias in the model, including system aliases.\\n\\n\\nGetting a Reference to a Model Version¶\\nTo get a reference to a specific version of a model as a ModelVersion instance, use the model’s version method.\\nUse the model’s default attribute to get the default version of the model.\\nmv = m.version(\"v1\")\\nmv = m.default\\n\\nCopy\\nOnce you have a reference to a specific version of a model (such as the variable mv in this example), you can\\nretrieve or update its comments or metrics and call the model’s methods (or functions) as shown in the following sections.\\n\\n\\nRetrieving and Updating Comments¶\\nAs with models, model versions can have comments, which can be accessed and set via the model version’s comment or\\ndescription attribute.\\nprint(mv.comment)\\nprint(mv.description)\\n\\nmv.comment = \"A model version comment\"\\nmv.description = \"Same as setting the comment\"\\n\\nCopy\\n\\nTip\\nYou can also change a model version’s comment in SQL using ALTER MODEL … MODIFY VERSION.\\n\\n\\n\\nRetrieving and Updating Metrics¶\\nMetrics are key-value pairs used to track prediction accuracy and other model version characteristics. You can set\\nmetrics when creating a model version or set them using the set_metric method. A metric value can be any Python\\nobject that can be serialized to JSON, including numbers, strings, lists, and dictionaries. Unlike tags, metric names\\nand possible values do not need to be defined in advance.\\nA test accuracy metric might be generated using sklearn’s accuracy_score:\\nfrom sklearn import metrics\\n\\ntest_accuracy = metrics.accuracy_score(test_labels, prediction)\\n\\nCopy\\nThe confusion matrix can be generated similarly using sklearn:\\ntest_confusion_matrix = metrics.confusion_matrix(test_labels, prediction)\\n\\nCopy\\nThen we can set these values as metrics as follows.\\n# scalar metric\\nmv.set_metric(\"test_accuracy\", test_accuracy)\\n\\n# hierarchical (dictionary) metric\\nmv.set_metric(\"evaluation_info\", {\"dataset_used\": \"my_dataset\", \"accuracy\": test_accuracy, \"f1_score\": f1_score})\\n\\n# multivalent (matrix) metric\\nmv.set_metric(\"confusion_matrix\", test_confusion_matrix)\\n\\nCopy\\nTo retrieve a model version’s metrics as a Python dictionary, use show_metrics.\\nmetrics = mv.show_metrics()\\n\\nCopy\\nTo delete a metric, call delete_metric.\\nmv.delete_metric(\"test_accuracy\")\\n\\nCopy\\n\\nTip\\nYou can also modify a model version’s metrics (which are stored in as metadata) in SQL using\\nALTER MODEL … MODIFY VERSION.\\n\\n\\n\\nExporting a Model Version¶\\nUse mv.export to export a model’s files to a local directory. The directory is created if it does not exist.\\nmv.export(\"~/mymodel/\")\\n\\nCopy\\nBy default, the exported files include the code, the environment to load the model, and model weights. To also\\nexport the files needed to run the model in a warehouse, specify export_mode = ExportMode.FULL.\\nmv.export(\"~/mymodel/\", export_mode=ExportMode.FULL)\\n\\nCopy\\n\\n\\nLoading a Model Version¶\\nUse mv.load to load the original Python model object that was originally added to the registry. You can then\\nuse the model for inference just as though you had defined it in your Python code.\\nclf = mv.load()\\n\\nCopy\\n\\nImportant\\nTo ensure proper functionality of the model, the target Python environment (that is, the versions of the Python\\ninterpreter and of all libraries) should be identical to the environment from which the model was logged. Specify\\nforce=True in the load call to force the model to be loaded even if the environment is different.\\n\\nThe optional options argument is a dictionary of options for loading the model. Currently, the argument supports\\nonly the use_gpu option.\\n\\n\\nOption\\nType\\nDescription\\nDefault\\n\\n\\n\\nuse_gpu\\nbool\\nEnables GPU-specific loading logic.\\nFalse\\n\\n\\n\\nThe following example illustrates the use of the options argument.\\nclf = mv.load(options={\"use_gpu\": True})\\n\\nCopy\\n\\n\\n\\nCalling Model Methods¶\\nModel versions can have methods, which are attached functions that can be executed to perform inference or other model\\noperations. The versions of a model can have different methods, and the signatures of these methods can also differ.\\nTo call a method of a model version, use mv.run, specifying the name of the function to be called and passing a\\nSnowpark or pandas DataFrame containing the inference data and any other required parameters. The method executes in a\\nSnowflake warehouse.\\nThe return value of the method is a Snowpark or pandas DataFrame, depending on the type of DataFrame passed in.\\nSnowpark DataFrames are evaluated lazily, so the method is run only when the DataFrame’s collect, show,\\nor to_pandas method is called.\\n\\nNote\\nInvoking a method runs it in the warehouse specified in the session you’re using to connect to the registry.\\nSee Specifying a Warehouse.\\n\\nThe following example illustrates running the predict method of a model. This model’s predict method does not\\nrequire any parameters besides the inference data (test_features here). If it did, they would be passed as\\nadditional arguments after the inference data.\\nremote_prediction = mv.run(test_features, function_name=\"predict\")\\nremote_prediction.show()   # assuming test_features is Snowpark DataFrame\\n\\nCopy\\nTo see what methods can be called on a given model, call mv.show_functions. The return value of this method is a\\nlist of ModelFunctionInfo objects. Each of these objects includes the following attributes:\\n\\nname: The name of the function that can be called from Python or SQL.\\ntarget_method: The name of the Python method in the original logged model.\\n\\n\\nTip\\nYou can also call model methods in SQL. See Model methods.\\n\\n\\n\\nCost Considerations¶\\nUsing the Snowflake Model Registry incurs standard Snowflake consumption-based costs. These include:\\n\\nCost of storing model artifacts, metadata, and functions. See Exploring storage cost\\nfor general information about storage costs.\\nCost of copying files between stages to Snowflake. See COPY FILES.\\nCost of serverless model object operations through the Snowsight UI or the SQL or Python interface, such as\\nshowing models and model versions, and altering model comments, tags, and metrics.\\nWarehouse compute costs, which vary depending on the type of model and the quantity of data used in inference.\\nSee Understanding compute cost for general information about Snowflake compute costs.\\nWarehouse compute costs are incurred for:\\n\\nModel and version creation operations.\\nInvoking a model’s methods.\\n\\n\\n\\n\\n\\nNotes on Specific Model Types¶\\nThis section provides additional information on logging specific types of models into the Snowflake Model Registry.\\n\\nSnowpark ML¶\\nThe registry supports models created using Snowpark ML modeling APIs (models derived from\\nsnowpark.ml.modeling.framework.base.BaseEstimator). The following additional options may be used in the options dictionary\\nwhen calling log_model.\\n\\n\\nOption\\nDescription\\n\\n\\n\\ntarget_methods\\nA list of the names of the methods available on the model object. Snowpark ML models have the following target methods by default, assuming the method exists:\\npredict, transform, predict_proba, predict_log_proba, decision_function\\n\\n\\n\\nYou do not need to specify sample_input_data or signatures when logging a Snowpark ML model;\\nthese are automatically inferred during fitting.\\n\\nExample¶\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn import datasets\\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\\n\\niris = datasets.load_iris()\\ndf = pd.DataFrame(data=np.c_[iris[\"data\"], iris[\"target\"]], columns=iris[\"feature_names\"] + [\"target\"])\\ndf.columns = [s.replace(\" (CM)\", \"\").replace(\" \", \"\") for s in df.columns.str.upper()]\\n\\ninput_cols = [\"SEPALLENGTH\", \"SEPALWIDTH\", \"PETALLENGTH\", \"PETALWIDTH\"]\\nlabel_cols = \"TARGET\"\\noutput_cols = \"PREDICTED_TARGET\"\\n\\nclf_xgb = XGBClassifier(\\n        input_cols=input_cols, output_cols=output_cols, label_cols=label_cols, drop_input_cols=True\\n)\\nclf_xgb.fit(df)\\nmodel_ref = registry.log_model(\\n    clf_xgb,\\n    model_name=\"XGBClassifier\",\\n    version_name=\"v1\",\\n)\\nmodel_ref.run(df.drop(columns=label_cols).head(10), function_name=\\'predict_proba\\')\\n\\nCopy\\n\\n\\n\\nscikit-learn¶\\nThe registry supports models created using scikit-learn (models derived from sklearn.base.BaseEstimator or\\nsklearn.pipeline.Pipeline). The following additional options may be used in the options dictionary\\nwhen calling log_model.\\n\\n\\nOption\\nDescription\\n\\n\\n\\ntarget_methods\\nA list of the names of the methods available on the model object. scikit-learn models have the following target methods by default, assuming the method exists:\\npredict, transform, predict_proba, predict_log_proba, decision_function\\n\\n\\n\\nYou must specify either the sample_input_data or signatures parameter when logging a scikit-learn model\\nso that the registry knows the signatures of the target methods.\\n\\nExample¶\\nfrom sklearn import datasets, ensemble\\n\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\nclf = ensemble.RandomForestClassifier(random_state=42)\\nclf.fit(iris_X, iris_y)\\nmodel_ref = registry.log_model(\\n    clf,\\n    model_name=\"RandomForestClassifier\",\\n    version_name=\"v1\",\\n    sample_input_data=iris_X,\\n    options={\\n        \"method_options\": {\\n            \"predict\": {\"case_sensitive\": True},\\n            \"predict_proba\": {\"case_sensitive\": True},\\n            \"predict_log_proba\": {\"case_sensitive\": True},\\n        }\\n    },\\n)\\nmodel_ref.run(iris_X[-10:], function_name=\\'\"predict_proba\"\\')\\n\\nCopy\\n\\n\\n\\nXGBoost¶\\nThe registry supports models created using XGBoost (models derived from xgboost.XGBModel or xgboost.Booster).\\nThe following additional options may be used in the options dictionary when calling log_model.\\n\\n\\nOption\\nDescription\\n\\n\\n\\ntarget_methods\\nA list of the names of the methods available on the model object. Models derived from XGBModel have the following target methods by default, assuming the method exists:\\npredict, predict_proba. (Before v1.4.0, apply was also included.) Models derived from Booster have the predict method by default.\\n\\ncuda_version\\nThe version of the CUDA runtime to be used when deploying to a platform with GPU; defaults to 11.7. If manually set\\nto None, the model cannot be deployed to a platform having a GPU.\\n\\n\\n\\nYou must specify either the sample_input_data or signatures parameter when logging an XGBoost model so\\nthat the registry knows the signatures of the target methods.\\n\\nExample¶\\nimport xgboost\\nfrom sklearn import datasets, model_selection\\n\\ncal_X, cal_y = datasets.load_breast_cancer(as_frame=True, return_X_y=True)\\ncal_X_train, cal_X_test, cal_y_train, cal_y_test = model_selection.train_test_split(cal_X, cal_y)\\nparams = dict(n_estimators=100, reg_lambda=1, gamma=0, max_depth=3, objective=\"binary:logistic\")\\nregressor = xgboost.train(params, xgboost.DMatrix(data=cal_X_train, label=cal_y_train))\\nmodel_ref = registry.log_model(\\n    regressor,\\n    model_name=\"xgBooster\",\\n    version_name=\"v1\",\\n    sample_input_data=cal_X_test,\\n    options={\\n        \"target_methods\": [\"predict\"],\\n        \"method_options\": {\\n            \"predict\": {\"case_sensitive\": True},\\n        },\\n    },\\n)\\nmodel_ref.run(cal_X_test[-10:])\\n\\nCopy\\n\\n\\n\\nPyTorch¶\\nThe registry supports PyTorch models (classes derived from torch.nn.Module or torch.jit.ModuleScript) if the\\nmodel’s forward method accepts one or more torch.Tensor instances as input and returns a torch.Tensor or a\\ntuple of them. The registry converts between pandas DataFrames and tensors when calling the model and returning the results.\\nTensors correspond to columns in the dataframe.\\nFor example, suppose your model accepts two tensors like this:\\nimport torch\\n\\nclass TorchModel(torch.nn.Module):\\n    def __init__(self, n_input: int, n_hidden: int, n_out: int, dtype: torch.dtype = torch.float32) -> None:\\n        super().__init__()\\n        self.model = torch.nn.Sequential(\\n            torch.nn.Linear(n_input, n_hidden, dtype=dtype),\\n            torch.nn.ReLU(),\\n            torch.nn.Linear(n_hidden, n_out, dtype=dtype),\\n            torch.nn.Sigmoid(),\\n        )\\n\\n    def forward(self, tensor_1: torch.Tensor, tensor_2: torch.Tensor) -> torch.Tensor:\\n        return self.model(tensor_1) + self.model(tensor_2)\\n\\nCopy\\nIf you want to pass torch.Tensor([[1,2],[3,4]]) as tensor_1 and torch.Tensor([[5,6], [7,8]]) as\\ntensor_2, create a DataFrame as follows to pass to the model.\\nimport pandas as pd\\ntensors = pd.DataFrame([[[1,2],[5,6]],[[3,4],[7,8]]])\\n\\nCopy\\nThen the tensors DataFrame looks like this.\\n        0       1\\n0  [1, 2]  [5, 6]\\n1  [3, 4]  [7, 8]\\n\\nCopy\\nSimilarly, if your model returns two tensors, such as (torch.Tensor([[1,2],[3,4]]), torch.Tensor([[5,6], [7,8]])),\\nthe result is a DataFrame like the one above.\\nWhen providing sample input data for a PyTorch model, you must provide either a list of tensors (which will be\\nconverted ta a pandas DataFrame) or a DataFrame. A list may contain a single tensor, but a tensor on its own is not\\naccepted.\\n\\nLogging the model¶\\nThe following additional options may be used in the options dictionary when calling log_model.\\n\\n\\nOption\\nDescription\\n\\n\\n\\ntarget_methods\\nA list of the names of the methods available on the model object. PyTorch models default to forward.\\n\\ncuda_version\\nThe version of the CUDA runtime to be used when deploying to a platform with GPU; defaults to 11.7. If manually set\\nto None, the model cannot be deployed to a platform having a GPU.\\n\\n\\n\\nYou must specify either the sample_input_data or signatures parameter when logging a PyTorch model\\nso that the registry knows the signatures of the target methods.\\n\\n\\nExample¶\\nimport torch\\nimport numpy as np\\n\\nclass TorchModel(torch.nn.Module):\\n        def __init__(self, n_input: int, n_hidden: int, n_out: int, dtype: torch.dtype = torch.float32) -> None:\\n                super().__init__()\\n                self.model = torch.nn.Sequential(\\n                        torch.nn.Linear(n_input, n_hidden, dtype=dtype),\\n                        torch.nn.ReLU(),\\n                        torch.nn.Linear(n_hidden, n_out, dtype=dtype),\\n                        torch.nn.Sigmoid(),\\n                )\\n\\n        def forward(self, tensor: torch.Tensor) -> torch.Tensor:\\n                return self.model(tensor)\\n\\nn_input, n_hidden, n_out, batch_size, learning_rate = 10, 15, 1, 100, 0.01\\ndtype = torch.float32\\nx = np.random.rand(batch_size, n_input)\\ndata_x = torch.from_numpy(x).to(dtype=dtype)\\ndata_y = (torch.rand(size=(batch_size, 1)) < 0.5).to(dtype=dtype)\\n\\nmodel = TorchModel(n_input, n_hidden, n_out, dtype=dtype)\\nloss_function = torch.nn.MSELoss()\\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\\nfor _epoch in range(100):\\n        pred_y = model.forward(data_x)\\n        loss = loss_function(pred_y, data_y)\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n\\nmodel_ref = registry.log_model(\\n    model,\\n    model_name=\"torchModel\",\\n    version_name=\"v1\",\\n    sample_input_data=[data_x],\\n)\\nmodel_ref.run([data_x])\\n\\nCopy\\n\\n\\n\\nTensorFlow¶\\nModels that extend tensorflow.Module or tensorflow.keras.Model are supported when they accept and return tensors\\nand are compilable or compiled.\\n\\nThe __call__ method for a tensorflow.Module or the call method for a tensorflow.keras.Model accepts one\\nor more tensorflow.Tensor or tensorflow.Variable as input and returns a tensorflow.Tensor or\\ntensorflow.Variable or a tuple of one of these types.\\nIf your model extends Module, it must be compilable, meaning the __call__ method is decorated with\\n@tensorflow.function; see tf.function documentation. If it extends\\nModel, it must be compiled; see compile documentation.\\n\\nThe registry converts between pandas DataFrames and tensors when calling the model and returning the results.\\nTensors correspond to columns in the dataframe.\\nFor example, suppose your model accepts two tensors like this:\\nimport tensorflow as tf\\n\\nclass KerasModel(tf.keras.Model):\\n    def  __init__(self, n_hidden: int, n_out: int) -> None:\\n        super().__init__()\\n        self.fc_1 = tf.keras.layers.Dense(n_hidden, activation=\"relu\")\\n        self.fc_2 = tf.keras.layers.Dense(n_out, activation=\"sigmoid\")\\n\\n    def call(self, tensor_1: tf.Tensor, tensor_2: tf.Tensor) -> tf.Tensor:\\n        input = tensor_1 + tensor_2\\n        x = self.fc_1(input)\\n        x = self.fc_2(x)\\n        return x\\n\\nCopy\\nIf you want to pass tf.Tensor([[1,2],[3,4]]) as tensor_1 and tf.Tensor([[5,6], [7,8]]) as\\ntensor_2, create a DataFrame as follows to pass to the model.\\nimport pandas as pd\\ntensors = pd.DataFrame([[[1,2],[5,6]],[[3,4],[7,8]]])\\n\\nCopy\\nThen the tensors DataFrame looks like this.\\n        0       1\\n0  [1, 2]  [5, 6]\\n1  [3, 4]  [7, 8]\\n\\nCopy\\nSimilarly, if your model returns two tensors, such as (tf.Tensor([[1,2],[3,4]]), tf.Tensor([[5,6], [7,8]])),\\nthe result is a DataFrame like the one above.\\nWhen providing sample input data for a TensorFlow model, you must provide either a list of tensors (which will be\\nconverted ta a pandas DataFrame) or a DataFrame. A list may contain a single tensor, but a tensor on its own is not\\naccepted.\\n\\nLogging the model¶\\nThe following additional options may be used in the options dictionary when calling log_model.\\n\\n\\nOption\\nDescription\\n\\n\\n\\ntarget_methods\\nA list of the names of the methods available on the model object. TensorFlow models default to forward.\\n\\ncuda_version\\nThe version of the CUDA runtime to be used when deploying to a platform with GPU; defaults to 11.7. If manually set\\nto None, the model cannot be deployed to a platform having a GPU.\\n\\n\\n\\nYou must specify either the sample_input_data or signatures parameter when logging a TensorFlow model\\nso that the registry knows the signatures of the target methods.\\n\\n\\nExample¶\\nimport tensorflow as tf\\nimport numpy as np\\n\\nclass KerasModel(tf.keras.Model):\\n        def __init__(self, n_hidden: int, n_out: int) -> None:\\n                super().__init__()\\n                self.fc_1 = tf.keras.layers.Dense(n_hidden, activation=\"relu\")\\n                self.fc_2 = tf.keras.layers.Dense(n_out, activation=\"sigmoid\")\\n\\n        def call(self, tensor: tf.Tensor) -> tf.Tensor:\\n                input = tensor\\n                x = self.fc_1(input)\\n                x = self.fc_2(x)\\n                return x\\n\\nn_input, n_hidden, n_out, batch_size, learning_rate = 10, 15, 1, 100, 0.01\\ndtype = tf.float32\\nx = np.random.rand(batch_size, n_input)\\ndata_x = tf.convert_to_tensor(x, dtype=dtype)\\nraw_data_y = tf.random.uniform((batch_size, 1))\\nraw_data_y = tf.where(raw_data_y > 0.5, tf.ones_like(raw_data_y), tf.zeros_like(raw_data_y))\\ndata_y = tf.cast(raw_data_y, dtype=dtype)\\n\\nmodel = KerasModel(n_hidden, n_out)\\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate), loss=tf.keras.losses.MeanSquaredError())\\nmodel.fit(data_x, data_y, batch_size=batch_size, epochs=100)\\n\\nmodel_ref = registry.log_model(\\n    model,\\n    model_name=\"tfModel\",\\n    version_name=\"v1\",\\n    sample_input_data=[data_x],\\n)\\nmodel_ref.run([data_x])\\n\\nCopy\\n\\n\\n\\nMLFlow¶\\nMLFlow models that provide a PyFunc flavor are supported. If your MLFlow model has a signature, the signature\\nargument is inferred from the model. Otherwise, you must provide either signature or sample_input_data.\\nThe following additional options may be used in the options dictionary when calling log_model.\\n\\n\\nOption\\nDescription\\n\\n\\n\\nmodel_uri\\nThe URI of the artifacts of the MLFlow model. Must be provided if it is not available in the model’s metadata as\\nmodel.metadata.get_model_info().model_uri.\\n\\nignore_mlflow_metadata\\nIf True, the model’s metadata is not imported to the model object in the registry. Default: False\\n\\nignore_mlflow_dependencies\\nIf True, the dependencies in the model’s metadata are ignored, which is useful due to package available\\nlimitations in Snowflake warehouses. Default: False\\n\\n\\n\\n\\nExample¶\\nimport mlflow\\nfrom sklearn import datasets, model_selection, ensemble\\n\\ndb = datasets.load_diabetes(as_frame=True)\\nX_train, X_test, y_train, y_test = model_selection.train_test_split(db.data, db.target)\\nwith mlflow.start_run() as run:\\n    rf = ensemble.RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\\n    rf.fit(X_train, y_train)\\n\\n    # Use the model to make predictions on the test dataset.\\n    predictions = rf.predict(X_test)\\n    signature = mlflow.models.signature.infer_signature(X_test, predictions)\\n    mlflow.sklearn.log_model(\\n        rf,\\n        \"model\",\\n        signature=signature,\\n    )\\n    run_id = run.info.run_id\\n\\n\\nmodel_ref = registry.log_model(\\n    mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\"),\\n    model_name=\"mlflowModel\",\\n    version_name=\"v1\",\\n    conda_dependencies=[\"mlflow<=2.4.0\", \"scikit-learn\", \"scipy\"],\\n    options={\"ignore_mlflow_dependencies\": True}\\n)\\nmodel_ref.run(X_test)\\n\\nCopy\\n\\n\\n\\nHugging Face Pipeline¶\\n\\nNote\\nFor details on the expected input and output of specific types of Hugging Face pipelines, see\\nInferred Signatures for Hugging Face Pipelines.\\n\\nThe registry supports Hugging Face model classes defined as transformers that derive from transformers.Pipeline.\\nThe following code is an example of logging a compatible model.\\nlm_hf_model = transformers.pipeline(\\n    task=\"text-generation\",\\n    model=\"bigscience/bloom-560m\",\\n    token=\"...\",  # Put your HuggingFace token here.\\n    return_full_text=False,\\n    max_new_tokens=100,\\n)\\n\\nlmv = reg.log_model(lm_hf_model, model_name=\\'bloom\\', version_name=\\'v560m\\')\\n\\nCopy\\n\\nImportant\\nA model based on huggingface_pipeline.HuggingFacePipelineModel contains only configuration data; the model\\nweights are downloaded from the Hugging Face Hub each time the model is used.\\nThe model registry currently supports deploying models only to warehouses. Warehouses do not support external network\\naccess without special configuration.\\nEven if the required external access integrations have been created, there is no way at this time to specify which\\nintegrations a particular model needs.\\nThe current best practice is to instead use transformers.Pipeline as shown in the example above. This downloads\\nmodel weights to your local system and uploads the entire model to the warehouse. This results in a self-contained\\nmodel that does not need Internet access.\\n\\nThe registry infers the signatures argument as long as the pipeline contains only a task from the following list.\\n\\nconversational\\nfill-mask\\nquestion-answering\\nsummarization\\ntable-question-answering\\ntext2text-generation\\ntext-classification (also called sentiment-analysis)\\ntext-generation\\ntoken-classification (also called ner)\\ntranslation\\ntranslation_xx_to_yy\\nzero-shot-classification\\n\\nThe sample_input_data argument is completely ignored for Hugging Face models. Specify the signatures argument\\nwhen logging a Hugging Face model that is not in the above list so that the registry knows the signatures of the target\\nmethods.\\nTo see the inferred signature, use the show_functions method. The following, for example, is the result of\\nlmv.show_functions() where lmv is the model logged above.\\n{\\'name\\': \\'__CALL__\\',\\n  \\'target_method\\': \\'__call__\\',\\n  \\'signature\\': ModelSignature(\\n                      inputs=[\\n                          FeatureSpec(dtype=DataType.STRING, name=\\'inputs\\')\\n                      ],\\n                      outputs=[\\n                          FeatureSpec(dtype=DataType.STRING, name=\\'outputs\\')\\n                      ]\\n                  )}]\\n\\nCopy\\nWith this information, you can call the model as follows.\\nimport pandas as pd\\nremote_prediction = lmv.run(pd.DataFrame([\"Hello, how are you?\"], columns=[\"inputs\"]))\\n\\nCopy\\n\\nUsage Notes¶\\n\\nMany Hugging Face models are large and do not fit in a standard warehouse. Use a Snowpark-optimized warehouse or choose\\na smaller version of the model. For example, instead of using the Llama-2-70b-chat-hf model, try\\nLlama-2-7b-chat-hf.\\nSnowflake warehouses do not have GPUs. Use only CPU-optimized Hugging Face models.\\nSome Hugging Face transformers return an array of dictionaries per input row. The registry converts such output to a\\nstring containing a JSON representation of the array. For example, multi-output Question Answering output looks like this:\\n[{\"score\": 0.61094731092453, \"start\": 139, \"end\": 178, \"answer\": \"learn more about the world of athletics\"},\\n{\"score\": 0.17750297486782074, \"start\": 139, \"end\": 180, \"answer\": \"learn more about the world of athletics.\\\\\"\"}]\\n\\nCopy\\n\\n\\nYou must specify either the sample_input_data or signatures parameter when logging a Hugging Face model\\nso that the registry knows the signatures of the target methods.\\n\\n\\nExample¶\\n# Prepare model\\nimport transformers\\nimport pandas as pd\\n\\nfinbert_model = transformers.pipeline(\\n    task=\"text-classification\",\\n    model=\"ProsusAI/finbert\",\\n    top_k=2,\\n)\\n\\n# Log the model\\nmv = registry.log_model(\\n    finbert_model,\\n    model_name=\"finbert\",\\n    version_name=\"v1\",\\n)\\n\\n# Use the model\\nmv.run(pd.DataFrame(\\n        [\\n            [\"I have a problem with my Snowflake that needs to be resolved asap!!\", \"\"],\\n            [\"I would like to have udon for today\\'s dinner.\", \"\"],\\n        ]\\n    )\\n)\\n\\nCopy\\nResult:\\n0  [{\"label\": \"negative\", \"score\": 0.8106237053871155}, {\"label\": \"neutral\", \"score\": 0.16587384045124054}]\\n1  [{\"label\": \"neutral\", \"score\": 0.9263970851898193}, {\"label\": \"positive\", \"score\": 0.05286872014403343}]\\n\\nCopy\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageDifferences from Private PreviewRequired PrivilegesCurrent Limitations and IssuesOpening the Snowflake Model RegistryRegistering Models and VersionsWorking with Model ArtifactsDeleting ModelsGetting Models from the RegistryViewing and Updating a Model’s MetadataWorking with Model VersionsCalling Model MethodsCost ConsiderationsNotes on Specific Model TypesRelated contentModel Registry SQL ReferenceLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/sql-reference-snowflake-scripting', 'title': 'Snowflake Scripting reference | Snowflake Documentation', 'language': 'en'}, page_content='Snowflake Scripting reference | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSQL command referenceFunction and stored procedure referenceClass referenceScripting referenceBEGIN ... ENDBREAKCASECLOSECONTINUEDECLAREEXCEPTIONFETCHFORIFLETLOOPNULLOPENRAISEREPEATRETURNWHILEGeneral referenceAPI referenceReferenceScripting reference\\n\\nSnowflake Scripting reference¶\\nThese topics provide reference information for the language elements supported in\\nSnowflake Scripting.\\n-- Variable declaration\\n[ DECLARE ... ]\\n  ...\\nBEGIN\\n  ...\\n  -- Branching\\n  [ IF ... ]\\n  [ CASE ... ]\\n\\n  -- Looping\\n  [ FOR ... ]\\n  [ WHILE ... ]\\n  [ REPEAT ... ]\\n  [ LOOP ... ]\\n\\n  -- Loop termination (within a looping construct)\\n  [ BREAK ]\\n  [ CONTINUE ]\\n\\n  -- Variable assignment\\n  [ LET ... ]\\n\\n  -- Cursor management\\n  [ OPEN ... ]\\n  [ FETCH ... ]\\n  [ CLOSE ... ]\\n\\n  -- \"No-op\" (no-operation) statement (usually within a branch or exception)\\n  [ NULL ]\\n\\n  -- Raising exceptions\\n  [ RAISE ... ]\\n\\n  -- Returning a value\\n  [ RETURN ... ]\\n\\n-- Exception handling\\n[ EXCEPTION ... ]\\n\\nEND;\\n\\nCopy\\n\\n\\nNext Topics:\\n\\nBEGIN … END\\nBREAK\\nCASE\\nCLOSE\\nCONTINUE\\nDECLARE\\nEXCEPTION\\nFETCH\\nFOR\\nIF\\nLET\\nLOOP\\nNULL\\nOPEN\\nRAISE\\nREPEAT\\nRETURN\\nWHILE\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentSnowflake Scripting Developer GuideWriting stored procedures in Snowflake ScriptingLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/getting-started-tutorial', 'title': 'Snowflake in 20 minutes | Snowflake Documentation', 'language': 'en'}, page_content='Snowflake in 20 minutes | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusSnowflake in 20 minutes Getting StartedTutorialsSnowflake in 20 Minutes\\n\\nSnowflake in 20 minutes¶\\n\\nIntroduction¶\\nThis tutorial uses the Snowflake command line client, SnowSQL, to introduce key concepts and tasks, including:\\n\\nCreating Snowflake objects—You create a database and a table for storing data.\\nLoading data—We provide small sample CSV data files for you to load into the table.\\nQuerying—You explore sample queries.\\n\\n\\nNote\\nSnowflake bills a minimal amount for the on-disk storage used for the sample data in\\nthis tutorial. The tutorial provides steps to drop the database and minimize storage\\ncost.\\nSnowflake requires a virtual warehouse to load the\\ndata and execute queries. A running virtual warehouse consumes Snowflake credits.\\nIn this tutorial, you will be using a 30-day trial account,\\nwhich provides free credits, so you won’t incur any costs.\\n\\n\\nWhat you’ll learn¶\\nIn this tutorial you’ll learn how to:\\n\\nCreate Snowflake objects—You create a database and a table for storing data.\\nInstall SnowSQL—You install and use SnowSQL, the Snowflake command line query tool.\\nUsers of Visual Studio Code might consider using the Snowflake Extension for Visual Studio Code instead of SnowSQL.\\n\\nLoad CSV data files—You use various mechanisms to load data into tables from CSV files.\\nWrite and execute sample queries—You write and execute a variety of queries against newly loaded data.\\n\\n\\n\\n\\nPrerequisites¶\\nThis tutorial requires a database, table, and virtual warehouse to load and query data.\\nCreating these Snowflake objects requires a Snowflake user with a role with the\\nnecessary access control privileges. In addition, SnowSQL\\nis required to execute the SQL statements in the tutorial. Lastly, the tutorial requires CSV files that contain sample data to load.\\nYou can complete this tutorial using an existing Snowflake warehouse, database, and table, and your own local data files, but we recommend using the Snowflake objects and the set of\\nprovided data.\\nTo set up Snowflake for this tutorial, complete the following before continuing:\\n\\nCreate a user\\nTo create the database, table, and virtual warehouse, you must be logged in as a\\nSnowflake user with a role that grants you the privileges to create these objects.\\n\\nIf you’re using a 30-day trial account, you can log in as the user that was created for the account.\\nThis user has the role with the privileges needed to create the objects.\\nIf you don’t have a Snowflake user, you can’t perform this tutorial.\\nIf you don’t have a role that lets you create a user, ask someone who does to perform this step for you.\\nUsers with the ACCOUNTADMIN or SECURITYADMIN role can create users.\\n\\n\\nInstall SnowSQL\\nTo install SnowSQL, see Installing SnowSQL.\\n\\nDownload sample data files\\nFor this tutorial you download sample employee data files in CSV format that Snowflake provides.\\nTo download and unzip the sample data files:\\n\\nDownload the set of sample data files. Right-click the name of the archive\\nfile, getting-started.zip, and save the link/file to your local file system.\\nUnzip the sample files. The tutorial assumes you unpacked files into one of the following directories:\\n\\n\\n\\nLinux/macOS: /tmp\\nWindows: C:\\\\\\\\temp\\n\\n\\nEach file has five data records. The data uses a comma (,) character as field\\ndelimiter. The following is an example record:\\nAlthea,Featherstone,afeatherstona@sf_tuts.com,\"8172 Browning Street, Apt B\",Calatrava,7/12/2017\\n\\nCopy\\n\\n\\nThere are no blank spaces before or after the commas separating the\\nfields in each record. This is the default that Snowflake expects when loading CSV data.\\n\\n\\nLog in to SnowSQL¶\\nAfter you have SnowSQL, start SnowSQL to connect to Snowflake:\\n\\nOpen a command line window.\\nStart SnowSQL:\\n$ snowsql -a <account_identifier> -u <user_name>\\n\\nCopy\\nWhere:\\n\\n\\n\\n<account_identifier> is the unique identifier for your Snowflake account.The preferred format of the account identifier is as follows:\\n\\norganization_name-account_nameNames of your Snowflake organization and account. For more information, see Format 1 (preferred): Account name in your organization.\\n\\n\\n\\n\\n\\n<user_name> is the login name for your Snowflake user.\\n\\n\\n\\nNote\\nIf your account has an identity provider (IdP) that has been defined for your account, you can use a web browser to authenticate instead of a password, as the following example demonstrates:\\n$ snowsql -a <account_identifier> -u <user_name> --authenticator externalbrowser\\n\\nCopy\\n\\nFor more information, see Using a web browser for federated authentication/SSO.\\n\\nWhen SnowSQL prompts you, enter the password for your Snowflake user.\\n\\nIf you log in successfully, SnowSQL displays a command prompt that includes\\nyour current warehouse, database, and schema.\\n\\nNote\\nIf you get locked out of the account and can’t obtain the account identifier, you can find it in the Welcome email that Snowflake sent to\\nyou when you signed up for the trial account, or you can work with your\\nORGADMIN to get the account details.\\nYou can also find the values for locator, cloud, and region\\nin the Welcome email.\\n\\nIf your Snowflake user doesn’t have a default warehouse, database, and schema, or if\\nyou didn’t configure SnowSQL to specify a default warehouse, database, and schema,\\nthe prompt displays no warehouse, no database, and no schema. For example:\\nuser-name#(no warehouse)@(no database).(no schema)>\\n\\nCopy\\nThis prompt indicates that there is no warehouse, database, and schema\\nselected for the current session. You create these objects\\nin the next step. As you follow the next steps in this tutorial to create\\nthese objects, the prompt automatically updates to include the names of these objects.\\nFor more information, see Connecting through SnowSQL.\\n\\n\\nCreate Snowflake objects¶\\nDuring this step you create the following Snowflake objects:\\n\\nA database (sf_tuts) and a table (emp_basic). You load sample data into this table.\\nA virtual warehouse (sf_tuts_wh).\\nThis warehouse provides the compute resources needed to load data into\\nthe table and query the table. For this tutorial, you create an X-Small warehouse.\\n\\nAt the completion of this tutorial, you will remove these objects.\\n\\nCreate a database¶\\nCreate the sf_tuts database using the CREATE DATABASE command:\\nCREATE OR REPLACE DATABASE sf_tuts;\\n\\nCopy\\nIn this tutorial, you use the default schema (public) available for each database, rather than creating a new schema.\\nNote that the database and schema you just created are now in use for your current\\nsession, as reflected in the SnowSQL command prompt. You can also use the context\\nfunctions to get this information.\\nSELECT CURRENT_DATABASE(), CURRENT_SCHEMA();\\n\\nCopy\\nThe following is an example result:\\n+--------------------+------------------+\\n| CURRENT_DATABASE() | CURRENT_SCHEMA() |\\n|--------------------+------------------|\\n| SF_TUTS            | PUBLIC           |\\n+--------------------+------------------+\\n\\n\\n\\n\\nCreate a table¶\\nCreate a table named emp_basic in sf_tuts.public using the CREATE TABLE command:\\nCREATE OR REPLACE TABLE emp_basic (\\n   first_name STRING ,\\n   last_name STRING ,\\n   email STRING ,\\n   streetaddress STRING ,\\n   city STRING ,\\n   start_date DATE\\n   );\\n\\nCopy\\nNote that the number of columns in the table, their positions, and their data types correspond to the fields in the sample CSV data files that you stage in the next step in this tutorial.\\n\\n\\nCreate a virtual warehouse¶\\nCreate an X-Small warehouse named sf_tuts_wh using the CREATE WAREHOUSE command:\\nCREATE OR REPLACE WAREHOUSE sf_tuts_wh WITH\\n   WAREHOUSE_SIZE=\\'X-SMALL\\'\\n   AUTO_SUSPEND = 180\\n   AUTO_RESUME = TRUE\\n   INITIALLY_SUSPENDED=TRUE;\\n\\nCopy\\nThe sf_tuts_wh warehouse is initially suspended, but the DML statement also sets\\nAUTO_RESUME = true. The AUTO_RESUME setting causes a warehouse to automatically start\\nwhen SQL statements that require compute resources are executed.\\nAfter you create the warehouse, it’s now in use for your current session.\\nThis information is displayed in your SnowSQL command prompt. You can also retrieve\\nthe name of the warehouse by using the following context function:\\nSELECT CURRENT_WAREHOUSE();\\n\\nCopy\\nThe following is an example result:\\n+---------------------+\\n| CURRENT_WAREHOUSE() |\\n|---------------------|\\n| SF_TUTS_WH          |\\n+---------------------+\\n\\n\\n\\n\\n\\nStage data files¶\\nA Snowflake stage is a location in cloud storage that you use to load and\\nunload data from a table. Snowflake supports the following types of stages:\\n\\nInternal stages—Used to store data files internally within Snowflake. Each user and table in Snowflake gets an internal stage by default for staging data files.\\nExternal stages—Used to store data files externally in Amazon S3, Google Cloud Storage, or Microsoft Azure.\\nIf your data is already stored in these cloud storage services, you can use an external stage to load data in Snowflake tables.\\n\\nIn this tutorial, we upload the sample data files\\n(downloaded in Prerequisites)\\nto the internal stage for the emp_basic table that you created earlier. You use the PUT command\\nto upload the sample data files to that stage.\\n\\nStaging sample data files¶\\nExecute the PUT command in SnowSQL to upload local data files to the table stage\\nprovided for the emp_basic table you created.\\nPUT file://<file-path>[/\\\\]employees0*.csv @sf_tuts.public.%emp_basic;\\n\\nCopy\\nFor example:\\n\\nLinux or macOS\\nPUT file:///tmp/employees0*.csv @sf_tuts.public.%emp_basic;\\n\\nCopy\\n\\nWindows\\nPUT file://C:\\\\temp\\\\employees0*.csv @sf_tuts.public.%emp_basic;\\n\\nCopy\\n\\n\\nLet’s take a closer look at the command:\\n\\nfile://<file-path>[/]employees0*.csv specifies the full directory path and\\nnames of the files on your local machine to stage. Note that file system wildcards are allowed, and if multiple files fit the pattern they are all displayed.\\n@<namespace>.%<table_name> indicates to use the stage for the specified table, in this case the emp_basic table.\\n\\nThe command returns the following result, showing the staged files:\\n+-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------+\\n| source          | target             | source_size | target_size | source_compression | target_compression | status   | message |\\n|-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------|\\n| employees01.csv | employees01.csv.gz |         360 |         287 | NONE               | GZIP               | UPLOADED |         |\\n| employees02.csv | employees02.csv.gz |         355 |         274 | NONE               | GZIP               | UPLOADED |         |\\n| employees03.csv | employees03.csv.gz |         397 |         295 | NONE               | GZIP               | UPLOADED |         |\\n| employees04.csv | employees04.csv.gz |         366 |         288 | NONE               | GZIP               | UPLOADED |         |\\n| employees05.csv | employees05.csv.gz |         394 |         299 | NONE               | GZIP               | UPLOADED |         |\\n+-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------+\\n\\n\\nThe PUT command compresses files by default using gzip, as indicated in the TARGET_COMPRESSION column.\\n\\n\\nListing the staged files (Optional)¶\\nYou can list the staged files using the LIST command.\\nLIST @sf_tuts.public.%emp_basic;\\n\\nCopy\\nThe following is an example result:\\n+--------------------+------+----------------------------------+------------------------------+\\n| name               | size | md5                              | last_modified                |\\n|--------------------+------+----------------------------------+------------------------------|\\n| employees01.csv.gz |  288 | a851f2cc56138b0cd16cb603a97e74b1 | Tue, 9 Jan 2018 15:31:44 GMT |\\n| employees02.csv.gz |  288 | 125f5645ea500b0fde0cdd5f54029db9 | Tue, 9 Jan 2018 15:31:44 GMT |\\n| employees03.csv.gz |  304 | eafee33d3e62f079a054260503ddb921 | Tue, 9 Jan 2018 15:31:45 GMT |\\n| employees04.csv.gz |  304 | 9984ab077684fbcec93ae37479fa2f4d | Tue, 9 Jan 2018 15:31:44 GMT |\\n| employees05.csv.gz |  304 | 8ad4dc63a095332e158786cb6e8532d0 | Tue, 9 Jan 2018 15:31:44 GMT |\\n+--------------------+------+----------------------------------+------------------------------+\\n\\n\\n\\n\\n\\nCopy data into target tables¶\\nTo load your staged data into the target table, execute COPY INTO <table>.\\nThe COPY INTO <table> command uses the virtual warehouse you created\\nin Create Snowflake objects to copy files.\\nCOPY INTO emp_basic\\n  FROM @%emp_basic\\n  FILE_FORMAT = (type = csv field_optionally_enclosed_by=\\'\"\\')\\n  PATTERN = \\'.*employees0[1-5].csv.gz\\'\\n  ON_ERROR = \\'skip_file\\';\\n\\nCopy\\nWhere:\\n\\nThe FROM clause specifies the location containing the data files (the internal stage for the table).\\nThe FILE_FORMAT clause specifies the file type as CSV, and specifies the double-quote\\ncharacter (\") as the character used to enclose strings. Snowflake supports\\ndiverse file types and options. These are described\\nin CREATE FILE FORMAT.\\nThe PATTERN clause specifies that the command should load data from the filenames matching\\nthis regular expression (.*employees0[1-5].csv.gz).\\nThe ON_ERROR clause specifies what to do when the COPY command encounters errors in the files. By default, the command stops loading data\\nwhen the first error is encountered. This example skips any file containing an error and moves on to loading\\nthe next file. (None of the files in this tutorial contain errors; this is included for illustration purposes.)\\n\\nThe COPY command also provides an option for validating files before they are loaded. For more information about additional error checking and validation instructions, see the COPY INTO <table> topic and the other data loading tutorials.\\nThe COPY command returns a result showing the list of files copied and related information:\\n+--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+\\n| file               | status | rows_parsed | rows_loaded | error_limit | errors_seen | first_error | first_error_line | first_error_character | first_error_column_name |\\n|--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------|\\n| employees02.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees04.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees05.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees03.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees01.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n+--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+\\n\\n\\n\\n\\nQuery loaded data¶\\nYou can query the data loaded in the emp_basic table using standard SQL and any supported\\nfunctions and\\noperators.\\nYou can also manipulate the data, such as updating the loaded data or inserting more data, using standard DML commands.\\n\\nRetrieve all data¶\\nReturn all rows and columns from the table:\\nSELECT * FROM emp_basic;\\n\\nCopy\\nThe following is a partial result:\\n+------------+--------------+---------------------------+-----------------------------+--------------------+------------+\\n| FIRST_NAME | LAST_NAME    | EMAIL                     | STREETADDRESS               | CITY               | START_DATE |\\n|------------+--------------+---------------------------+-----------------------------+--------------------+------------|\\n| Arlene     | Davidovits   | adavidovitsk@sf_tuts.com  | 7571 New Castle Circle      | Meniko             | 2017-05-03 |\\n| Violette   | Shermore     | vshermorel@sf_tuts.com    | 899 Merchant Center         | Troitsk            | 2017-01-19 |\\n| Ron        | Mattys       | rmattysm@sf_tuts.com      | 423 Lien Pass               | Bayaguana          | 2017-11-15 |\\n ...\\n ...\\n ...\\n| Carson     | Bedder       | cbedderh@sf_tuts.co.au    | 71 Clyde Gallagher Place    | Leninskoye         | 2017-03-29 |\\n| Dana       | Avory        | davoryi@sf_tuts.com       | 2 Holy Cross Pass           | Wenlin             | 2017-05-11 |\\n| Ronny      | Talmadge     | rtalmadgej@sf_tuts.co.uk  | 588 Chinook Street          | Yawata             | 2017-06-02 |\\n+------------+--------------+---------------------------+-----------------------------+--------------------+------------+\\n\\nCopy\\n\\n\\nInsert additional data rows¶\\nIn addition to loading data from staged files into a table, you can insert rows directly into a table using the INSERT DML command.\\nFor example, to insert two additional rows into the table:\\nINSERT INTO emp_basic VALUES\\n   (\\'Clementine\\',\\'Adamou\\',\\'cadamou@sf_tuts.com\\',\\'10510 Sachs Road\\',\\'Klenak\\',\\'2017-9-22\\') ,\\n   (\\'Marlowe\\',\\'De Anesy\\',\\'madamouc@sf_tuts.co.uk\\',\\'36768 Northfield Plaza\\',\\'Fangshan\\',\\'2017-1-26\\');\\n\\nCopy\\n\\n\\nQuery rows based on email address¶\\nReturn a list of email addresses with United Kingdom top-level domains using the [ NOT ] LIKE function:\\nSELECT email FROM emp_basic WHERE email LIKE \\'%.uk\\';\\n\\nCopy\\nThe following is an example result:\\n+--------------------------+\\n| EMAIL                    |\\n|--------------------------|\\n| gbassfordo@sf_tuts.co.uk |\\n| rtalmadgej@sf_tuts.co.uk |\\n| madamouc@sf_tuts.co.uk   |\\n+--------------------------+\\n\\n\\n\\n\\nQuery rows based on start date¶\\nFor example, to calculate when certain employee benefits might start, add 90 days to employee start\\ndates using the DATEADD function. Filter the list by employees whose start date occurred earlier than January 1, 2017:\\nSELECT first_name, last_name, DATEADD(\\'day\\',90,start_date) FROM emp_basic WHERE start_date <= \\'2017-01-01\\';\\n\\nCopy\\nThe following is an example result:\\n+------------+-----------+------------------------------+\\n| FIRST_NAME | LAST_NAME | DATEADD(\\'DAY\\',90,START_DATE) |\\n|------------+-----------+------------------------------|\\n| Granger    | Bassford  | 2017-03-30                   |\\n| Catherin   | Devereu   | 2017-03-17                   |\\n| Cesar      | Hovie     | 2017-03-21                   |\\n| Wallis     | Sizey     | 2017-03-30                   |\\n+------------+-----------+------------------------------+\\n\\n\\n\\n\\n\\nSummary, clean up, and additional resources¶\\nCongratulations! You’ve successfully completed this introductory tutorial.\\nTake a few minutes to review a short summary and the key points covered in the tutorial.\\nYou might also want to consider cleaning up by dropping any objects you created in the tutorial.\\nLearn more by reviewing other topics in the Snowflake Documentation.\\n\\nSummary and key points¶\\nIn summary, data loading is performed in two steps:\\n\\nStage the data files to load. The files can be staged internally (in Snowflake) or in an external location. In this tutorial, you stage files internally.\\nCopy data from the staged files into an existing target table. A running\\nwarehouse is required for this step.\\n\\nRemember the following key points about loading CSV files:\\n\\nA CSV file consists of 1 or more records, with 1 or more fields in each record, and sometimes a header record.\\nRecords and fields in each file are separated by delimiters. The default delimiters are:\\n\\n\\nRecords:\\nnewline characters\\n\\nFields:\\ncommas\\n\\n\\n\\nIn other words, Snowflake expects each record in a CSV file to be separated by new lines and the fields (i.e. individual values) in each record to be separated by commas. If different\\ncharacters are used as record and field delimiters, you must explicitly specify this as part of the file format when loading.\\n\\nThere is a direct correlation between the fields in the files and the columns in the table you will be loading, in terms of:\\n\\n\\nNumber of fields (in the file) and columns (in the target table).\\nPositions of the fields and columns within their respective file/table.\\nData types, such as string, number, or date, for fields and columns.\\n\\n\\nThe records will not be loaded if the numbers, positions, and data types don’t align with the data.\\n\\nNote\\nSnowflake supports loading files in which the fields don’t exactly align with the columns in the target table;\\nhowever, this is a more advanced data loading topic (covered in\\nTransforming data during a load).\\n\\n\\n\\n\\n\\nTutorial cleanup (Optional)¶\\nIf the objects you created in this tutorial are no longer needed,\\nyou can remove them from the system with DROP <object> statements.\\nDROP DATABASE IF EXISTS sf_tuts;\\n\\nDROP WAREHOUSE IF EXISTS sf_tuts_wh;\\n\\nCopy\\n\\n\\nExit the connection¶\\nTo exit a connection, use the !exit command for SnowSQL (or its alias, !disconnect).\\nExit drops the current connection and quits SnowSQL if it is the last connection.\\n\\n\\nWhat’s next?¶\\nContinue learning about Snowflake using the following resources:\\n\\nComplete the other tutorials provided by Snowflake:\\n\\nSnowflake Tutorials\\n\\n\\nFamiliarize yourself with key Snowflake concepts and features, as well as the SQL commands to perform queries and insert/update data:\\n\\nIntroduction to Snowflake\\nQuery syntax\\nData Manipulation Language (DML) commands\\n\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/release-notes/performance-improvements', 'title': 'Performance Improvements | Snowflake Documentation', 'language': 'en'}, page_content=\"Performance Improvements | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusWhat's NewBehavior ChangesDeprecated FeaturesPreview FeaturesPerformance Improvements2024 Improvements2023 Improvements2022 ImprovementsSQL ImprovementsClients, Drivers, and LibrariesMonthly ReleasesClient versions & support policySnowflake ConnectorsSnowflake Connector for Google Analytics Raw DataSnowflake Connector for Google Analytics Aggregate DataSnowflake Connector for ServiceNowSnowflake Connector for ServiceNow V2Snowflake Connector for MySQLSnowflake Connector for PostgreSQLNative SDK for ConnectorsNative SDK for Connectors Java libraryNative SDK for Connectors Java Test libraryNative SDK for Connectors Java TemplateNative SDK Example Java GitHub ConnectorReleasesPerformance Improvements\\n\\nPerformance Improvements¶\\nSnowflake is constantly introducing features that improve performance. Many of these features require no user interaction and no additional\\ncost. Improvements that make queries run faster can reduce costs because a warehouse only consumes credits while it is running.\\n\\nImportant\\nPerformance improvements often target specific query patterns or workloads. These improvements might or might not have a material impact\\non a specific workload.\\n\\nIf you have questions about any of these features, please feel free to contact\\nSnowflake Support.\\n\\n\\n2024 Performance Improvements\\n2023 Performance Improvements\\n2022 Performance Improvements\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/warehouses', 'title': 'Virtual warehouses | Snowflake Documentation', 'language': 'en'}, page_content='Virtual warehouses | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesOverviewMulticlusterConsiderationsWorking with warehousesQuery Acceleration ServiceMonitoring loadSnowpark-optimized warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesVirtual warehouses\\n\\nVirtual warehouses¶\\nA virtual warehouse, often referred to simply as a “warehouse”, is a cluster of compute resources in Snowflake. A virtual warehouse is\\navailable in two types:\\n\\nStandard\\nSnowpark-optimized\\n\\nA warehouse provides the required resources, such as CPU, memory, and temporary storage, to\\nperform the following operations in a Snowflake session:\\n\\nExecuting SQL SELECT statements that require compute resources (e.g. retrieving rows from tables and views).\\nPerforming DML operations, such as:\\n\\nUpdating rows in tables (DELETE , INSERT , UPDATE).\\nLoading data into tables (COPY INTO <table>).\\nUnloading data from tables  (COPY INTO <location>).\\n\\n\\n\\n\\nNote\\nTo perform these operations, a warehouse must be running and in use for the session. While a warehouse is running, it consumes Snowflake\\ncredits.\\n\\n\\nOverview of warehousesWarehouses are required for queries, as well as all DML operations, including loading data into tables.\\nIn addition to being defined by its type as either Standard or Snowpark-optimized, a warehouse is defined by its size,\\nas well as the other properties that can be set to help control and automate warehouse activity.\\n\\nSnowpark-optimized warehousesSnowpark workloads can be run on both Standard and Snowpark-optimized warehouses. Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases\\n\\nWarehouse considerationsBest practices and general guidelines for using virtual warehouses in Snowflake to process queries\\n\\nMulti-cluster warehousesMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.\\n\\nWorking with warehousesLearn how to create, stop, start and otherwise manage Snowflake warehouses.\\n\\nUsing the Query Acceleration ServiceThe query acceleration service can accelerate parts of the query workload in a warehouse.\\nWhen enabled for a warehouse, query acceleration can improve overall warehouse performance by reducing the impact of outlier queries\\n(i.e. queries which use more resources then typical queries).\\n\\nMonitoring warehouse loadWarehouse query load measures the average number of queries that were running or queued within a specific interval.\\n\\n\\n\\n\\nOverview of warehouses\\nSnowpark-optimized warehouses\\nWarehouse considerations\\nMulti-cluster warehouses\\nWorking with warehouses\\nUsing the Query Acceleration Service\\nMonitoring warehouse load\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnderstanding compute costWorking with resource monitorsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/data-load-s3', 'title': 'Bulk loading from Amazon S3 | Snowflake Documentation', 'language': 'en'}, page_content='Bulk loading from Amazon S3 | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemAmazon S3Allowing the Virtual Private Cloud IDsConfiguring Secure AccessAWS Data File EncryptionCreating an S3 StageCopying Data from an S3 StageGoogle Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingLoading Semi-Structured DataIntroductionSupported FormatsConsiderationsAccessing Unstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData LoadingAmazon S3\\n\\nBulk loading from Amazon S3¶\\nIf you already have a Amazon Web Services (AWS) account and use S3 buckets for storing and managing your data files, you can make use of your existing buckets and folder paths for bulk loading into\\nSnowflake.This set of topics describes how to use the COPY command to bulk load from an S3 bucket into tables.\\nAs illustrated in the diagram below, loading data from an S3 bucket is performed in two steps:\\n\\nStep 1:\\nSnowflake assumes the data files have already been staged in an S3 bucket. If they haven’t been staged yet, use the upload interfaces/utilities provided by AWS to stage the files.\\n\\nStep 2:\\nUse the COPY INTO <table> command to load the contents of the staged file(s) into a Snowflake database table. You can load directly from the bucket, but\\nSnowflake recommends creating an external stage that references the bucket and using the external stage instead.\\nRegardless of the method you use, this step requires a running, current virtual warehouse for the session if you execute the command\\nmanually or within a script. The warehouse provides the compute resources to perform the actual insertion of rows into the table.\\n\\n\\n\\n\\n\\n\\nNote\\nSnowflake uses Amazon S3 Gateway Endpoints in each of its Amazon Virtual Private Clouds.\\nIf the S3 bucket referenced by your external stage is in the same region as your Snowflake account, your network traffic does not traverse the public Internet. The Amazon S3 Gateway Endpoints ensure that regional traffic stays within the AWS network.\\n\\n\\nTip\\nThe instructions in this set of topics assume you have read Preparing to load data and have created a named file format, if desired.\\nBefore you begin, you may also want to read Data loading considerations for best practices, tips, and other guidance.\\n\\nNext Topics:\\n\\nConfiguration tasks (complete as needed):\\n\\n\\nAllowing the Virtual Private Cloud IDs\\nConfiguring secure access to Amazon S3\\nAWS data file encryption\\nCreating an S3 stage\\n\\n\\n\\nData loading tasks (complete for each set of files you load):\\n\\n\\nCopying data from an S3 stage\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnloading into Amazon S3SnowpipeLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/sql-reference-functions', 'title': 'Function and stored procedure reference | Snowflake Documentation', 'language': 'en'}, page_content='Function and stored procedure reference | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSQL command referenceFunction and stored procedure referenceSummary of functionsAll functions (alphabetical)AggregateBitwise expressionConditional expressionContextConversionData generationData metricDate & timeEncryptionFileGeospatialHashMetadataNotificationNumericRegular expressionsSemi-structured and structured dataString & binarySystemTableVectorWindowStored proceduresClass referenceScripting referenceGeneral referenceAPI referenceReferenceFunction and stored procedure reference\\n\\nFunction and stored procedure reference¶\\nThese topics provide reference information for the system-defined functions and system-defined stored procedures.\\n\\nSummary of functions — combined summary of all system-defined functions. Can be used as a\\nquick-reference.\\nAll functions (alphabetical) — alphabetical list of all system-defined functions (scalar, aggregate, table, etc.).\\nAggregate functions — functions that take multiple rows/values as input and return a single value.\\nScalar functions — functions that take a single row/value as input and return a single value:\\n\\nBitwise expression functions\\nConditional expression functions\\nContext functions\\nConversion functions\\nData generation functions\\nDate & time functions\\nEncryption functions\\nGeospatial functions\\nHash functions\\nMetadata functions\\nNotification functions\\nNumeric functions\\nSemi-structured and structured data functions\\nString functions (regular expressions)  — regular expression (search) functions\\nString & binary functions\\nVector similarity functions\\n\\n\\nSystem functions — functions that perform control operations or return system-level information.\\nTable functions — functions that return results in tabular format.\\nWindow functions — subset of aggregate functions that can operate on a subset of rows.\\nData metric functions — functions that enable data quality measurements for tables and views.\\nStored procedures — stored procedures to facilitate using certain Snowflake features.\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentWriting external functionsUser-defined functions overviewLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/guides-overview-db', 'title': 'Databases, Tables and Views - Overview | Snowflake Documentation', 'language': 'en'}, page_content='Databases, Tables and Views - Overview | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsTable StructuresTemporary and Transient TablesExternal TablesIceberg TablesHybrid TablesWorking with tables in SnowsightSearch optimization serviceViewsViewsSecure ViewsMaterialized ViewsWorking with ViewsConsiderationsViews, Materialized Views, and Dynamic TablesTable DesignCloningData StorageData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesDatabases, Tables, & Views\\n\\nDatabases, Tables and Views - Overview¶\\nAll data in Snowflake is maintained in databases. Each database consists of one or more schemas, which are logical groupings of database objects,\\nsuch as tables and views. Snowflake does not place any hard limits on the number of databases, schemas (within a database), or objects (within\\na schema) you can create.\\nUse the following pages to learn about tables and table types, views, design considerations and other related content.\\n\\nUnderstanding Snowflake Table StructuresIntroduction to micro-partitions and data clustering, two of the principal concepts utilized in Snowflake physical table structures.\\n\\nTemporary and Transient TablesSnowflake supports creating temporary tables for storing non-permanent, transitory data such as ETL data, session-specific\\nor other short lived data.\\n\\nExternal TablesSnowflake supports the concept of an external table. External tables are read-only, and their files are stored in an external stage.\\n\\nHybrid TablesSnowflake supports the concept of a hybrid table. Hybrid tables provide\\noptimized performance for read and write operations in transactional and\\nhybrid workloads.\\n\\nIceberg TablesSnowflake supports the Apache Iceberg open table format. Snowflake Iceberg tables use data in external cloud\\nstorage and give you the option to use Snowflake as the Iceberg catalog, an external Iceberg catalog, or to create a table\\nfrom files in object storage.\\n\\nViewsA view allows the result of a query to be accessed as if it were a table.\\nViews serve a variety of purposes, including combining, segregating, and protecting data.\\n\\nSecure ViewsSnowflake supports the concept of a secure view. Secure views are specifically designed for data privacy.\\nFor example to limit access to sensitive data that should not be exposed to all users of the underlying table(s).\\n\\nMaterialized ViewsMaterialized views are views precomputed from data derived from a query specification and stored for later use.\\nQuerying a materialized view is faster than executing a query against the base table of the view because the data is pre-computed.\\n\\nTable Design Best PracticesBest practices, general guidelines, and important considerations when designing and managing tables.\\n\\nCloning Best PracticesBest practices, general guidelines, and important considerations when cloning objects in Snowflake, particularly databases, schemas,\\nand permanent tables.\\n\\nData storage considerationsBest practices and guidelines for controlling data storage costs associated with Continuous Data Protection (CDP), particularly for tables.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentData Definition Language (DDL) commandsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/views-materialized', 'title': 'Working with Materialized Views | Snowflake Documentation', 'language': 'en'}, page_content=\"Working with Materialized Views | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsTable StructuresTemporary and Transient TablesExternal TablesIceberg TablesHybrid TablesWorking with tables in SnowsightSearch optimization serviceViewsViewsSecure ViewsMaterialized ViewsWorking with ViewsConsiderationsViews, Materialized Views, and Dynamic TablesTable DesignCloningData StorageData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesDatabases, Tables, & ViewsMaterialized Views\\n\\nWorking with Materialized Views¶\\n\\n Enterprise Edition Feature\\nMaterialized views require Enterprise Edition. To inquire about upgrading, please contact Snowflake Support.\\n\\nA materialized view is a pre-computed data set derived from a query specification (the SELECT in the view definition) and stored for later use.\\nBecause the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view.\\nThis performance difference can be significant when a query is run frequently or is sufficiently complex. As a result,\\nmaterialized views can speed up expensive aggregation, projection, and selection operations, especially those that run frequently\\nand that run on large data sets.\\n\\nNote\\nMaterialized views are designed to improve query performance for workloads composed of common, repeated query patterns. However, materializing\\nintermediate results incurs\\nadditional costs. As such, before creating any materialized views, you\\nshould consider whether the costs are\\noffset by the savings from re-using these results frequently enough.\\n\\n\\nDeciding When to Create a Materialized View¶\\nMaterialized views are particularly useful when:\\n\\nQuery results contain a small number of rows and/or columns relative to the base table (the table on\\nwhich the view is defined).\\nQuery results contain results that require significant processing, including:\\n\\nAnalysis of semi-structured data.\\nAggregates that take a long time to calculate.\\n\\n\\nThe query is on an external table, which might have slower\\nperformance compared to querying native database tables or Iceberg tables.\\nThe view’s base table does not change frequently.\\n\\n\\nAdvantages of Materialized Views¶\\nSnowflake’s implementation of materialized views provides a number of unique characteristics:\\n\\nMaterialized views can improve the performance of queries that use the same subquery results repeatedly.\\nMaterialized views are automatically and transparently maintained by Snowflake. A background service updates the materialized view\\nafter changes are made to the base table. This is more efficient and less error-prone than manually maintaining the equivalent of a\\nmaterialized view at the application level.\\nData accessed through materialized views is always current, regardless of the amount of DML that has been performed on the base table.\\nIf a query is run before the materialized view is up-to-date, Snowflake either updates the materialized view or uses the up-to-date\\nportions of the materialized view and retrieves any required newer data from the base table.\\n\\n\\nImportant\\nThe automatic maintenance of materialized views consumes credits. For more details, see\\nMaterialized Views Cost (in this topic).\\n\\n\\n\\nDeciding When to Create a Materialized View or a Regular View¶\\nIn general, when deciding whether to create a materialized view or a regular view, use the following criteria:\\n\\nCreate a materialized view when all of the following are true:\\n\\nThe query results from the view don’t change often. This almost always means that the underlying/base table\\nfor the view doesn’t change often, or at least that the subset of base\\ntable rows used in the materialized view don’t change often.\\nThe results of the view are used often (typically significantly more often than the query results change).\\nThe query consumes a lot of resources. Typically, this means that the query consumes a lot of processing\\ntime or credits, but it could also mean that the query consumes a lot of storage space for intermediate results.\\n\\n\\nCreate a regular view when any of the following are true:\\n\\nThe results of the view change often.\\nThe results are not used often (relative to the rate at which the results change).\\nThe query is not resource intensive so it is not costly to re-run it.\\n\\n\\n\\nThese criteria are just guidelines. A materialized view might provide benefits even if it is not used often — especially if the results change less frequently than the usage of the view.\\nAlso, there are other factors to consider when deciding whether to use a regular view or a materialized view.\\nFor example, the cost of storing the materialized view is a factor; if the results are not used very often (even\\nif they are used more often than they change), then the additional storage costs might not be worth the performance gain.\\n\\n\\nComparison with Tables, Regular Views, and Cached Results¶\\nMaterialized views are similar to tables in some ways and similar to regular (i.e. non-materialized) views in other ways.\\nIn addition, materialized views have some similarities with cached results, particularly because both enable storing\\nquery results for future re-use.\\nThis section describes some of the similarities and differences between these objects in specific areas, including:\\n\\nQuery performance.\\nQuery security.\\nReduced query logic complexity.\\nData clustering (related to query performance).\\nStorage and maintenance costs.\\n\\nSnowflake caches query results for a short period of time after a query has been run. In some situations,\\nif the same query is re-run and if nothing has changed in the table(s) that the query accesses, then\\nSnowflake can simply return the same results without re-running the query. This is the fastest and most\\nefficient form of re-use, but also the least flexible. For more details, see\\nUsing Persisted Query Results.\\nBoth materialized views and cached query results provide query performance benefits:\\n\\nMaterialized views are more flexible than, but typically slower than, cached results.\\nMaterialized views are faster than tables because of their “cache” (i.e. the query results for the view); in addition,\\nif data has changed, they can use their “cache” for data that hasn’t changed and use the base table for any data that has changed.\\n\\nRegular views do not cache data, and therefore cannot improve performance by caching. However, in some\\ncases, views help Snowflake generate a more efficient query plan. Also, both materialized views and\\nregular views enhance data security by allowing data to be exposed or hidden at the row level or column level.\\nThe following table shows key similarities and differences between tables, regular views, cached query results, and materialized views:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPerformance Benefits\\nSecurity Benefits\\nSimplifies Query Logic\\nSupports Clustering\\nUses Storage\\nUses Credits for Maintenance\\nNotes\\n\\n\\n\\nRegular table\\n\\n\\n\\n✔\\n✔\\n\\n\\n\\nRegular view\\n\\n✔\\n✔\\n\\n\\n\\n\\n\\nCached query result\\n✔\\n\\n\\n\\n\\n\\nUsed only if data has not changed and if query only uses deterministic functions (e.g. not CURRENT_DATE).\\n\\nMaterialized view\\n✔\\n✔\\n✔\\n✔\\n✔\\n✔\\nStorage and maintenance requirements typically result in increased costs.\\n\\nExternal table\\n\\n\\n\\n\\n\\n\\nData is maintained outside Snowflake and, therefore, does not incur any storage charges within Snowflake.\\n\\n\\n\\n\\n\\nExamples of Use Cases For Materialized Views¶\\nThis section describes some general usage scenarios that also provide a conceptual overview of materialized views:\\n\\nSuppose that, every day, you run a query Q that includes a subquery S. If S is resource-intensive and queries data that\\nchanges only once a week, then you could improve performance of the outer query Q by running S and caching the results in a table named CT:\\n\\nYou would update the table only once a week.\\nThe rest of the time, when you run Q, it would reference the subquery results of S that were stored in the table.\\n\\nThis would work well as long as the results of subquery S change predictably (e.g. at the same time every week).\\nHowever, if the results of S change unpredictably then caching the results in a table is risky; sometimes your\\nmain query Q will return out-of-date results if the results of subquery S are out of date (and thus the results of cached\\ntable CT are out of date).\\nIdeally, you’d like a special type of cache for results that change rarely, but for which the timing of the change is unpredictable. Looking\\nat it another way, you’d like to force your subquery S to be re-run (and your cache table CT to be updated) when necessary.\\nA materialized view implements an approximation of the best of both worlds. You define a query for your materialized view, and the results\\nof the query are cached (as though they were stored in an internal table), but Snowflake updates the cache when the table that the materialized\\nview is defined on is updated. Thus, your subquery results are readily available for fast performance.\\n\\nAs a less abstract example, suppose that you run a small branch of a large pharmacy, and your branch stocks hundreds of medications out of a\\ntotal of tens of thousands of FDA-approved medications.\\nSuppose also that you have a complete list of all medications that each of your customers takes, and that almost all of those customers order\\nonly medicines that are in stock (i.e. special orders are rare).\\nIn this scenario, you could create a materialized view that lists only the interactions among medicines that you keep in stock. When a customer\\norders a medicine that she has never used before, if both that medicine and all of the other medicines that she takes are covered by your\\nmaterialized view, then you don’t need to check the entire FDA database for drug interactions; you can just check the materialized view, so\\nthe search is faster.\\n\\nYou can use a materialized view by itself, or you can use it in a join.\\nContinuing with the pharmacy example, suppose that you have one table that lists all of the medicines that each of your customers takes; you can\\njoin that table to the materialized view of drug interactions to find out which of the customer’s current medications might interact with the\\nnew medication.\\nYou might use an outer join to make sure that you list all of the customer’s medicines, whether or not they are in your materialized view;\\nif the outer join shows that any of the current medicines are not in the materialized view, you can re-run the query on the\\nfull drug interactions table.\\n\\n\\n\\n\\nHow the Query Optimizer Uses Materialized Views¶\\nYou don’t need to specify a materialized view in a SQL statement in order for the view to be used. The query optimizer can\\nautomatically rewrite queries against the base table or regular views to use the materialized view instead.\\nFor example, suppose that a materialized view contains all of the rows and columns that are needed by a query against a base\\ntable. The optimizer can decide to rewrite the query to use the materialized view, rather than the base table. This can\\ndramatically speed up a query, especially if the base table contains a large amount of historical data.\\nAs another example, in a multi-table join, the optimizer might decide to use a materialized view instead of a table for one of the\\ntables in the join.\\n\\nNote\\nEven if a materialized view can replace the base table in a particular query, the optimizer might not use the materialized view.\\nFor example, if the base table is clustered by a field, the optimizer might choose to scan the base table (rather than the\\nmaterialized view) because the optimizer can effectively prune out partitions and provide equivalent performance using the\\nbase table.\\n\\nA materialized view can also be used as the data source for a subquery.\\nWhen the optimizer chooses to use a materialized view implicitly, the materialized view is listed in the EXPLAIN plan or the\\nQuery Profile instead of the base table. You can use this information to experiment and understand which queries can benefit from\\nexisting materialized views.\\n\\n\\n\\nAbout Materialized Views in Snowflake¶\\nThe next sections explain how materialized views are represented in Snowflake.\\n\\nDDL Commands For Materialized Views¶\\nMaterialized views are first-class database objects. Snowflake provides the following DDL commands for creating and maintaining materialized\\nviews:\\n\\nCREATE MATERIALIZED VIEW\\nALTER MATERIALIZED VIEW\\nDROP MATERIALIZED VIEW\\nDESCRIBE MATERIALIZED VIEW\\nSHOW MATERIALIZED VIEWS\\n\\n\\n\\nDML Operations on Materialized Views¶\\nSnowflake does not allow standard DML (e.g. INSERT, UPDATE, DELETE) on materialized views.\\nSnowflake does not allow users to truncate materialized views.\\nSee Limitations on Working With Materialized Views (in this topic) for details.\\n\\n\\nAccess Control Privileges¶\\nThere are three types of privileges that are related to materialized views:\\n\\nPrivileges on the schema that contains the materialized view.\\nPrivileges directly on the materialized view itself.\\nPrivileges on the database objects (e.g. tables) that the materialized view accesses.\\n\\nYou can use the standard commands for granting and revoking privileges on materialized views:\\n\\nGRANT <privileges>\\nREVOKE <privileges>\\n\\n\\nPrivileges on a Materialized View’s Schema¶\\nMaterialized views consume storage space. To create a materialized view, you need the CREATE MATERIALIZED VIEW\\nprivilege on the schema that will contain the materialized view. You need to execute a statement similar to:\\nGRANT CREATE MATERIALIZED VIEW ON SCHEMA <schema_name> TO ROLE <role_name>;\\n\\nCopy\\nFor more details about the GRANT statement, see GRANT <privileges>.\\n\\n\\nPrivileges on a Materialized View¶\\nMaterialized Views, like other database objects (tables, views, UDFs, etc.), are owned by a role and have privileges that can be granted\\nto other roles.\\nYou can grant the following privileges on a materialized view:\\n\\nSELECT\\n\\nAs with non-materialized views, a materialized view does not automatically inherit the privileges of its base table.\\nYou should explicitly grant privileges on the materialized view to the roles that should use that view.\\n\\nNote\\nThe exception to this rule is when the query optimizer rewrites a query against the base table to use the materialized view\\n(as explained in How the Query Optimizer Uses Materialized Views). In this case, the user does not need privileges to use the\\nmaterialized view in order to access the results of the query.\\n\\n\\n\\nPrivileges on the Database Objects Accessed by the Materialized View¶\\nAs with non-materialized views, a user who wishes to access a materialized view needs privileges only on the view, not on the underlying object(s)\\nthat the view references.\\n\\n\\n\\nSecure Materialized Views¶\\nMaterialized views can be secure views.\\nMost information about secure views applies to secure materialized views. There are a few cases where secure\\nmaterialized views are different from secure non-materialized views. The differences include:\\n\\nThe command to find out whether a view is secure.\\n\\nFor non-materialized views, check the IS_SECURE column in the output of the SHOW VIEWS command.\\nFor materialized views, check the IS_SECURE column in the output of the SHOW MATERIALIZED VIEWS command.\\n\\n\\n\\nFor more information about secure views, see Working with Secure Views.\\nThe syntax to create secure materialized views is documented at\\nCREATE MATERIALIZED VIEW.\\n\\n\\n\\nCreating and Working With Materialized Views¶\\nThis section provides information about creating and working with materialized views.\\n\\nPlanning to Create a Materialized View¶\\nWhen deciding to create a materialized view, consider doing some analysis to determine the need for the view:\\n\\nExamine the filters, projections, and aggregations of queries that are frequent or expensive.\\nUse the Query Profile and the EXPLAIN command to see whether existing materialized views are already\\nbeing used by the automatic query rewrite feature. You might find that you do not need to create any new\\nmaterialized views if there are existing views that fit the queries well.\\nBefore adding any materialized views, record current query costs and performance so that you can\\nevaluate the difference after creating the new materialized view.\\nIf you find queries with very selective filters that do not benefit from clustering the table, then a materialized\\nview containing the same filters can help the queries avoid scanning a lot of data.\\nSimilarly, if you find queries that use aggregation, or that contain expressions that are very expensive to\\nevaluate (for example, expensive function calls, or expensive operations on semi-structured data), then\\na materialized views that uses the same expression(s) or aggregation(s) can provide a benefit.\\n\\nRun the EXPLAIN command against the original queries, or run the queries and check the Query Profile, to see\\nwhether the new materialized view is being used.\\nMonitor the combined query and materialized view costs, and\\nevaluate whether the performance or cost benefits justify the cost of the materialized view’s maintenance.\\nExamine the query costs of the base table as well. In cases where the optimizer can rewrite the query to use a materialized\\nview, query compilation can consume more time and resources. (The optimizer has a larger number of possibilities to consider.)\\n\\nRemember that you can always reference materialized views directly if it simplifies your queries or you know that a\\nmaterialized view will give you better performance. However, in most cases, you can simply query the base table and\\nthe automatic query rewrite feature will do that for you.\\n\\n\\n\\nCreating a Materialized View¶\\nUse the CREATE MATERIALIZED VIEW command to create a materialized view. For an example, see\\nBasic Example: Creating a Materialized View (in this topic).\\n\\nNote\\nThe CREATE MATERIALIZED VIEW statement might take a substantial amount of time to complete.\\nWhen a materialized view is first created, Snowflake performs the equivalent of a CTAS (CREATE TABLE … AS ….) operation.\\n\\nWhen you create a materialized view, note the following:\\n\\nHandling Column Names That Are Not Allowed in Materialized Views\\nReferring to the Base Table\\nSpecifying Filters for Query Optimization\\n\\n\\nHandling Column Names That Are Not Allowed in Materialized Views¶\\nThe following column names are not allowed in a materialized view:\\n\\nNames that start with SYSTEM$ or METADATA$\\nNames that contain $SYS_FACADE$\\nThe column name SYS_MV_SOURCE_PARTITION\\n\\nIf you are defining a materialized view that selects a column with one of these names, you can define an alias for that column.\\nFor example:\\nCREATE OR REPLACE MATERIALIZED VIEW mv AS\\n  SELECT SYSTEM$ALPHA AS col1, ...\\n\\nCopy\\n\\n\\nReferring to the Base Table¶\\nWhenever possible, use the fully-qualified name for the base table referenced in a materialized view. This insulates the view\\nfrom changes that can invalidate the view, such as moving the base table to a different schema from the view (or vice versa).\\nIf the name of the base table is not qualified, and the table or view is moved to a different schema, the reference becomes\\ninvalid.\\nIn addition, if you are referring to the base table more than once in the view definition, use the same qualifier in all\\nreferences to the base table. For example, if you choose to use the fully-qualified name, make sure that all references to the\\nbase table use the fully-qualified name.\\n\\n\\nSpecifying Filters for Query Optimization¶\\nIf you specify a filter when creating a materialized view (e.g. WHERE column_1 BETWEEN Y and Z), the optimizer can use\\nthe materialized view for queries against the base table that have the same filter or a more restrictive filter. Here are some\\nexamples:\\n\\nHere’s a simple example of range subsumption.\\nIn this example, the filter in the query does not match the filter in the materialized view. However, the filter in the query\\nselects only rows that are in the materialized view, so the optimizer can choose to scan the materialized view rather than the\\nentire table.\\n-- Example of a materialized view with a range filter\\ncreate materialized view v1 as\\n  select * from table1 where column_1 between 100 and 400;\\n\\nCopy\\n-- Example of a query that might be rewritten to use the materialized view\\nselect * from table1 where column_1 between 200 and 300;\\n\\nCopy\\n\\nThis example shows OR subsumption. The materialized view contains all the rows that the subsequent query needs.\\nDefine a materialized view that contains all rows that have either value X or value Y:\\ncreate materialized view mv1 as\\n  select * from tab1 where column_1 = X or column_1 = Y;\\n\\nCopy\\nDefine a query that looks only for value Y (which is included in the materialized view):\\n\\nselect * from tab1 where column_1 = Y;\\n\\nCopy\\n\\nThe query above can be rewritten internally as:\\n\\nselect * from mv1 where column_1 = Y;\\n\\nCopy\\n\\n\\nThis example is another example of OR subsumption. There’s no explicit OR in the materialized view definition. However, an IN\\nclause is equivalent to a series of OR expressions, so the optimizer can re-write this query the same way as it re-wrote the\\nOR subsumption example above:\\ncreate materialized view mv1 as\\n  select * from tab1 where column_1 in (X, Y);\\n\\nCopy\\n\\n\\n\\nDefine a query that looks only for value Y (which is included in the materialized view):\\nselect * from tab1 where column_1 = Y;\\n\\nCopy\\nThe query above can be rewritten internally as:\\n\\nselect * from mv1 where column_1 = Y;\\n\\nCopy\\n\\n\\n\\nThis example uses AND subsumption:\\nCreate a materialized view that contains all rows where column_1 = X.\\ncreate materialized view mv2 as\\n  select * from table1 where column_1 = X;\\n\\nCopy\\nCreate a query:\\nselect column_1, column_2 from table1 where column_1 = X AND column_2 = Y;\\n\\nCopy\\nThe query can be rewritten as:\\nselect * from mv2 where column_2 = Y;\\n\\nCopy\\nThe rewritten query does not even need to include the expression column_1 = X because the materialized view’s\\ndefinition already requires that all rows match column_1 = X.\\n\\nThe following example shows aggregate subsumption:\\nThe materialized view is defined below:\\ncreate materialized view mv4 as\\n  select column_1, column_2, sum(column_3) from table1 group by column_1, column_2;\\n\\nCopy\\nThe following query can use the materialized view defined above:\\nselect column_1, sum(column_3) from table1 group by column_1;\\n\\nCopy\\nThe query can be rewritten as:\\nselect column_1, sum(column_3) from mv4 group by column_1;\\n\\nCopy\\nThe rewritten query does not take advantage of the additional grouping by column_2, but the rewritten query is not blocked by\\nthat additional grouping, either.\\n\\n\\n\\n\\n\\nLimitations on Creating Materialized Views¶\\n\\nNote\\nThese are current limitations; some of them might be removed or changed in future versions.\\n\\nThe following limitations apply to creating materialized views:\\n\\nA materialized view can query only a single table.\\nJoins, including self-joins, are not supported.\\nA materialized view cannot query:\\n\\nA materialized view.\\nA non-materialized view.\\nA UDTF (user-defined table function).\\n\\n\\nA materialized view cannot include:\\n\\nUDFs (this limitation applies to all types of user-defined functions, including external functions).\\nWindow functions.\\nHAVING clauses.\\nORDER BY clause.\\nLIMIT clause.\\nGROUP BY keys that are not within the SELECT list. All GROUP BY keys in a materialized view must be part of the SELECT list.\\nGROUP BY GROUPING SETS.\\nGROUP BY ROLLUP.\\nGROUP BY CUBE.\\nNesting of subqueries within a materialized view.\\nThe MINUS, EXCEPT, or INTERSECT set operators.\\n\\n\\nMany aggregate functions are not allowed in a materialized view definition.\\n\\nThe aggregate functions that are supported in materialized views are:\\n\\nAPPROX_COUNT_DISTINCT (HLL).\\nAVG (except when used in PIVOT).\\nBITAND_AGG.\\nBITOR_AGG.\\nBITXOR_AGG.\\nCOUNT.\\nCOUNT_IF.\\nMAX.\\nMIN.\\nSTDDEV, STDDEV_SAMP.\\nSTDDEV_POP.\\nSUM.\\nVARIANCE (VARIANCE_SAMP, VAR_SAMP).\\nVARIANCE_POP (VAR_POP).\\n\\nThe other aggregate functions are not supported in materialized views.\\n\\n\\n\\nNote\\nAggregate functions that are allowed in materialized views still have some restrictions:\\n\\nAggregate functions cannot be nested.\\nAggregate functions used in complex expressions (e.g. (sum(salary)/10)) can only be used\\nin the outer-most level of a query, not in a subquery or an in-line view.\\nFor example, the following is allowed:\\n\\ncreate materialized view mv1 as\\n    select\\n        sum(x) + 100\\n      from t;\\n\\nCopy\\n\\nThe following is not allowed:\\n\\ncreate materialized view mv2 as\\n    select\\n        y + 10\\n      from (\\n        select\\n          sum(x) as y\\n        from t\\n      );\\n\\nCopy\\n\\n\\nDISTINCT cannot be combined with aggregate functions.\\nIn a materialized view, the aggregate functions AVG, COUNT, COUNT_IF, MIN, MAX, and SUM can be used as aggregate\\nfunctions but not as window functions. In a materialized view, these functions cannot be used with the OVER\\nclause:\\n\\nOVER ( [ PARTITION BY <expr1> ] [ ORDER BY <expr2> ] )\\n\\nCopy\\n\\n\\nIf an aggregate function is in a subquery, the materialized view cannot create an expression on top of the\\naggregated column(s) from that subquery.\\nFor example, consider the following materialized view definition:\\n\\ncreate or replace materialized view mv1 as\\n    select c1 + 10 as c1new, c2\\n        from (select sum(c1) as c1, c2 from t group by c2);\\n\\nCopy\\n\\nThe expression “c1 + 10” is an expression on top of an aggregate function in a subquery, and therefore causes\\nan error message.\\nNote that even an equality operator counts as an expression, which means that CASE expressions using\\ncolumns that represent aggregate functions in a subquery are also prohibited.\\nTo work around this limitation, create a materialized view without the expression, and then create a\\nnon-materialized view that includes the expression, for example:\\n\\ncreate or replace materialized view mv1 as\\n    select c1, c2\\n        from (select sum(c1) as c1, c2 from t group by c2);\\n\\ncreate or replace view expr_v1 as\\n    select c1 + 10 as c1new, c2\\n        from (select c1, c2 from mv1);\\n\\nCopy\\n\\n\\n\\n\\n\\nFunctions used in a materialized view must be deterministic. For example, using CURRENT_TIME or\\nCURRENT_TIMESTAMP is not permitted.\\nA materialized view should not be defined using a function that produces different results for different settings\\nof parameters, such as the session-level parameter TIMESTAMP_TYPE_MAPPING.\\nFor example, suppose that a view is defined as follows:\\ncreate materialized view bad_example (ts1) as\\n    select to_timestamp(n) from t1;\\n\\nCopy\\nThe data type of the return value from TO_TIMESTAMP(n) depends upon the parameter TIMESTAMP_TYPE_MAPPING, so\\nthe contents of the materialized view depend upon the value of TIMESTAMP_TYPE_MAPPING at the time that the view was\\ncreated.\\nWhen a materialized view is created, the expression defining each of its columns is evaluated and stored. If a\\ncolumn definition depends upon a particular session variable, and the session variable changes, the expression is\\nnot re-evaluated, and the materialized view is not updated. If the materialized view depends upon a particular value\\nof a session variable, and if the session variable’s value has changed, then queries on the materialized view fail.\\nTo avoid this problem, force the expression to a value that does not depend upon any session variables. The\\nexample below casts the output to a particular data type, independent of the TIMESTAMP_TYPE_MAPPING parameter:\\ncreate materialized view good_example (ts1) as\\n    select to_timestamp(n)::TIMESTAMP_NTZ from t1;\\n\\nCopy\\nThis issue is specific to materialized views. Non-materialized views generate their output\\ndynamically based on current parameter settings, so the results can’t be stale.\\n\\nIn the definition of a materialized view, selecting the SEQ column from the output of the\\nFLATTEN function is not supported.\\nThe values in the SEQ column are not guaranteed to be ordered in any way when selected from a materialized view. If you\\nselect this column in the materialized view definition, the output may be indeterministic.\\n\\nMaterialized views cannot be created using the Time Travel feature.\\n\\n\\n\\nBasic Example: Creating a Materialized View¶\\nThis section contains a basic example of creating and using a materialized view:\\n\\nCREATE OR REPLACE MATERIALIZED VIEW mv1 AS\\n  SELECT My_ResourceIntensive_Function(binary_col) FROM table1;\\n\\nSELECT * FROM mv1;\\n\\nCopy\\n\\nMore detailed examples are provided in Examples (in this topic).\\n\\n\\nUnderstanding How Materialized Views Are Maintained¶\\nAfter you create a materialized view, a background process automatically maintains the data in the materialized view. Note the\\nfollowing:\\n\\nMaintenance of materialized views is performed by a background process, and the timing is optimally based on the workload on the\\nbase table and the materialized view.\\n\\nThis process updates the materialized view with changes made by DML operations to the base table (insertions, updates, and\\ndeletions).\\nIn addition, clustering on the base table can also result in refreshes of a materialized view. Refer to\\nBest Practices for Clustering Materialized Views and their Base Tables.\\n\\nWhen rows are inserted in the base table, the process performs a “refresh” operation to insert the new rows into the\\nmaterialized view.\\nWhen rows are deleted in the base table, the process performs a “compaction” operation on the materialized view, deleting\\nthese rows from the materialized view.\\n\\n\\nTo see the last time that a materialized view was refreshed, execute the\\nSHOW MATERIALIZED VIEWS command.\\nCheck the REFRESHED_ON and BEHIND_BY columns in the output:\\n\\nThe REFRESHED_ON and COMPACTED_ON columns show the timestamp of the last DML operation on the base table that was processed\\nby the refresh and compaction operations, respectively.\\nThe BEHIND_BY column indicates the amount of time that the updates to the materialized view are behind the updates to the base\\ntable.\\n\\n\\nIf maintenance falls behind, queries might run more slowly than when the views are up-to-date, but the results will always be\\nup-to-date.\\nIf some micro-partitions of the materialized view are out of date, Snowflake skips those partitions and looks up the data from\\nthe base table.\\n\\nIf the background process encounters certain user errors (for example, the query for the view results in a “division by zero”\\nerror), the process invalidates the materialized view.\\nQuerying an invalid materialized view resuts in an error. The error message includes the reason why the materialized view was\\ninvalidated. For example:\\nFailure during expansion of view 'MY_MV':\\n  SQL compilation error: Materialized View MY_MV is invalid.\\n  Invalidation reason: Division by zero\\n\\n\\nIf this occurs, address the problem described in the error message (for example, delete the rows that introduce the “divide by\\nzero” error), and resume the materialized view by using the\\nALTER MATERIALIZED VIEW … RESUME command.\\n\\n\\n\\n\\nSuspending and Resuming Maintenance on a Materialized View¶\\nIf you need to suspend the maintenance and use of a materialized view, execute the\\nALTER MATERIALIZED VIEW command with the SUSPEND parameter:\\nALTER MATERIALIZED VIEW <name> SUSPEND\\n\\nCopy\\nIf you suspend maintenance of a view, you cannot query the view until you resume maintenance.\\nTo resume the maintenance and use of a materialized view, execute the ALTER MATERIALIZED VIEW command with the RESUME parameter:\\nALTER MATERIALIZED VIEW <name> RESUME\\n\\nCopy\\nFor an example, see Suspending Updates to a Materialized View.\\n\\n\\nDisplaying Information About Materialized Views¶\\nThe following command and view provide information about materialized views:\\n\\nThe SHOW VIEWS command returns information about both materialized and regular views.\\nThe INFORMATION_SCHEMA.TABLES view shows materialized views. The TABLE_TYPE\\ncolumn shows “MATERIALIZED VIEW”. The IS_INSERTABLE column is always “NO”, because you cannot insert directly into a\\nmaterialized view.\\n\\nNote\\nThe INFORMATION_SCHEMA.VIEWS view does not show materialized views. Materialized\\nviews are shown by INFORMATION_SCHEMA.TABLES.\\n\\n\\n\\n\\n\\nLimitations on Working With Materialized Views¶\\n\\nNote\\nThese are current limitations; some of them might be removed or changed in future versions.\\n\\nThe following limitations apply to using materialized views:\\n\\nTo ensure that materialized views stay consistent with the base table on\\nwhich they are defined, you cannot perform most DML operations on a\\nmaterialized view itself. For example, you cannot insert rows directly\\ninto a materialized view (although of course you can insert rows into\\nthe base table). The prohibited DML operations include:\\n\\nCOPY\\nDELETE\\nINSERT\\nMERGE\\nUPDATE\\n\\nTruncating a materialized view is not supported.\\n\\nYou cannot directly clone a materialized view by using the CREATE MATERIALIZED VIEW ... CLONE... command.\\nHowever, if you clone a schema or a database that contains a materialized view, the materialized view will be cloned\\nand included in the new schema or database.\\nSnowflake does not support using the Time Travel feature to query materialized views at\\na point in the past (e.g. using the AT clause\\nwhen querying a materialized view).\\nHowever, you can use Time Travel to\\nclone a database or schema containing a materialized view at a point in the past. For\\ndetails, see Materialized Views and Time Travel.\\n\\nMaterialized Views are not monitored by Snowflake Working with resource monitors.\\n\\n\\n\\nEffects of Changes to Base Tables on Materialized Views¶\\nThe following sections explain how materialized views are affected by changes to the base tables.\\n\\nAdding Columns to the Base Table\\nChanging or Dropping Columns in the Base Table\\nRenaming or Swapping the Base Table\\nDropping the Base Table\\n\\n\\nAdding Columns to the Base Table¶\\nIf columns are added to the base table, those new columns are not propagated to the materialized view automatically.\\nThis is true even if the materialized view was defined with SELECT * (e.g.\\nCREATE MATERIALIZED VIEW AS SELECT * FROM table2 ...). The columns of the materialized view are defined at the time that\\nthe materialized view is defined. The SELECT * is not interpreted dynamically each time that the materialized view is\\nqueried.\\nTo avoid confusion, Snowflake recommends not using SELECT * in the definition of a materialized view.\\n\\nNote\\nAdding a column to the base table does not suspend a materialized view created on that base table.\\n\\n\\n\\nChanging or Dropping Columns in the Base Table¶\\nIf a base table is altered so that existing columns are changed or dropped, then all materialized views on that base table are\\nsuspended; the materialized views cannot be used or maintained. (This is true even if the modified or dropped column was not part\\nof the materialized view.)\\nYou cannot RESUME that materialized view. If you want to use it again, you must recreate it.\\nThe simplest way to recreate a materialized view with the same privileges on the view is by running the command:\\n\\nCREATE OR REPLACE MATERIALIZED VIEW <view_name> ... COPY GRANTS ...\\n\\nCopy\\n\\nThis is more efficient than running separate commands to:\\n\\nDrop the materialized view (DROP MATERIALIZED VIEW).\\nCreate the materialized view again (CREATE MATERIALIZED VIEW).\\nCreate the same privileges on the view (GRANT and\\nREVOKE).\\n\\n\\n\\nRenaming or Swapping the Base Table¶\\nRenaming or swapping the base table (or the schema or database containing the base table) can result in the materialized view\\npointing to a different base table than the base table used to create the materialized view. The following are examples of\\nsituations in which this can occur:\\n\\nThe base table is renamed (through ALTER TABLE … RENAME), and another table is created with the\\noriginal name of the base table.\\nThe base table of a materialized view is swapped with another table (through\\nALTER TABLE … SWAP WITH).\\nThe schema or database containing the base table of the materialized view is moved through DROP, SWAP or RENAME.\\n\\nIn these cases, the materialized view is suspended. In most cases, you must recreate the materialized view in order to use the\\nview.\\n\\n\\nDropping the Base Table¶\\nIf a base table is dropped, the materialized view is suspended (but not automatically dropped).\\nIn most cases, the materialized view must be dropped.\\nIf for some reason you are recreating the base table and would also like to recreate the materialized view with the same\\ndefinition it had previously, then first recreate the base table and then replace the view by using\\nCREATE OR REPLACE MATERIALIZED VIEW <view_name> ... COPY GRANTS ....\\n\\n\\n\\nMaterialized Views in Cloned Schemas and Databases¶\\nIf you clone a schema or a database that contains a materialized view, then the materialized view is cloned.\\nIf you clone the materialized view and the corresponding base table at the same time (as part of the same\\nCREATE SCHEMA ... CLONE or CREATE DATABASE ... CLONE operation), then the cloned materialized view\\nrefers to the cloned base table.\\nIf you clone the materialized view without cloning the base table (e.g. if the table is in Database1.Schema1\\nand the view is in Database1.Schema2, and you clone only Schema2 rather than all of Database1), then the cloned view\\nwill refer to the original base table.\\n\\n\\n\\nMaterialized Views Cost¶\\nMaterialized views impact your costs for both storage and compute resources:\\n\\nStorage: Each materialized view stores query results, which adds to the monthly storage usage for your account.\\nCompute resources: In order to prevent materialized views from becoming out-of-date, Snowflake performs automatic background maintenance of\\nmaterialized views. When a base table changes, all materialized views defined on the table are updated by a background service that uses compute\\nresources provided by Snowflake.\\nThese updates can consume significant resources, resulting in increased credit usage. However, Snowflake ensures efficient credit\\nusage by billing your account only for the actual resources used. Billing is calculated in 1-second increments.\\n\\n\\nTo learn how many credits per compute-hour are consumed by materialized views, refer to the “Serverless\\nFeature Credit Table” in the Snowflake service consumption table.\\n\\nEstimating and Controlling Costs¶\\nThere are no tools to estimate the costs of maintaining materialized views.\\nIn general, the costs are proportional to:\\n\\nThe number of materialized views created on each base table, and the amount of data that changes in each of those\\nmaterialized views when the base table changes. Any changes to micro-partitions in the base table require eventual\\nmaterialized view maintenance, whether those changes are due to reclustering or DML statements run on the base table.\\nThe number of those materialized views that are clustered. Maintaining clustering (of either a table or a\\nmaterialized view) adds costs.\\nIf a materialized view is clustered differently from the base table, the number of micro-partitions changed in the\\nmaterialized view might be substantially larger than the number of micro-partitions changed in the base table.\\nFor example, consider the case where the base table is changed largely by inserting (appending) data, and is not\\nclustered, so the base table is largely in the order that the rows were inserted into the table. Imagine that the\\nmaterialized view is clustered by an independent column, for example, postal code. If 100 new rows are added to\\nthe base table, those might go into one or two new micro-partitions, leaving the other micro-partitions in the\\nbase table untouched. But those 100 rows might require re-writing 100 micro-partitions in the clustered\\nmaterialized view.\\nAs another example, consider deletes. Deleting the oldest rows in an unclustered base table might delete only\\nthe oldest micro-partitions, but might require changes to a far larger number of micro-partitions in a\\nmaterialized view that is not clustered by age.\\n(For more details about clustering materialized views,\\nsee Materialized Views and Clustering.)\\n\\n\\nYou can control the cost of maintaining materialized views by carefully choosing how many views to create, which tables to create them on, and each view’s definition (including the number of rows and columns in that view).\\nYou can also control costs by suspending or resuming the materialized view; however, suspending maintenance typically only defers costs, rather\\nthan reducing them. The longer that maintenance has been deferred, the more maintenance there is to do.\\nSee also Best Practices for Maintaining Materialized Views.\\n\\nTip\\nIf you are concerned about the cost of maintaining materialized views, Snowflake recommends starting slowly with this\\nfeature (i.e. create only a few materialized views on selected tables) and monitor the costs over time.\\n\\n\\n\\nViewing Costs¶\\nYou can view the billing costs for maintaining materialized views using Snowsight, the Classic Console, or SQL:\\n\\nSnowsight:\\nAs a user with the proper privileges, select Admin » Cost Management » Consumption.\\nUse the All Service Types filter and select Materialized View.\\n\\nClassic Console:\\nAs an account administrator, select Account  » Billing & Usage.\\nThe credit costs are tracked in a Snowflake-provided virtual warehouse named  MATERIALIZED_VIEW_MAINTENANCE.\\n\\nSQL:\\nQuery either of the following:\\n\\nMATERIALIZED_VIEW_REFRESH_HISTORY table function (in the Snowflake Information Schema).\\nFor example:\\nSELECT * FROM TABLE(INFORMATION_SCHEMA.MATERIALIZED_VIEW_REFRESH_HISTORY());\\n\\nCopy\\n\\nMATERIALIZED_VIEW_REFRESH_HISTORY view view (in Account Usage).\\nThe following queries can be executed against the MATERIALIZED_VIEW_REFRESH_HISTORY view:\\nQuery: Materialized Views cost history (by day, by object)\\nThis query provides a full list of materialized views and the volume of credits consumed via the service over the last 30 days, broken\\nout by day. Any irregularities in the credit consumption or consistently high consumption are flags for additional investigation.\\nSELECT TO_DATE(start_time) AS date,\\n  database_name,\\n  schema_name,\\n  table_name,\\n  SUM(credits_used) AS credits_used\\nFROM snowflake.account_usage.materialized_view_refresh_history\\nWHERE start_time >= DATEADD(month,-1,CURRENT_TIMESTAMP())\\nGROUP BY 1,2,3,4\\nORDER BY 5 DESC;\\n\\nCopy\\nQuery: Materialized Views History & m-day average\\nThis query shows the average daily credits consumed by materialized views grouped by week over the last year. It can help identify\\nanomalies in daily averages over the year so you can investigate spikes or unexpected changes in\\nconsumption.\\nWITH credits_by_day AS (\\n  SELECT TO_DATE(start_time) AS date,\\n    SUM(credits_used) AS credits_used\\n  FROM snowflake.account_usage.materialized_view_refresh_history\\n  WHERE start_time >= DATEADD(year,-1,CURRENT_TIMESTAMP())\\n  GROUP BY 1\\n  ORDER BY 2 DESC\\n)\\n\\nSELECT DATE_TRUNC('week',date),\\n  AVG(credits_used) AS avg_daily_credits\\nFROM credits_by_day\\nGROUP BY 1\\nORDER BY 1;\\n\\nCopy\\n\\n\\n\\n\\n\\nNote\\nResource monitors provide control over virtual warehouse credit usage; however, you cannot use them to control\\ncredit usage for the Snowflake-provided warehouses, including the  MATERIALIZED_VIEW_MAINTENANCE warehouse.\\n\\n\\n\\n\\nMaterialized Views and Clustering¶\\nDefining a clustering key on a materialized view is supported and can increase performance in many situations.\\nHowever, it also adds costs.\\nIf you cluster both the materialized view(s) and the base table on which the\\nmaterialized view(s) are defined, you can cluster the materialized view(s) on\\ndifferent columns from the columns used to cluster the base table.\\nIn most cases, clustering a subset of the materialized views on a\\ntable tends to be more cost-effective than clustering the table itself.\\nIf the data in the base table is accessed (almost) exclusively through the\\nmaterialized views, and (almost) never directly through the base table,\\nthen clustering the base table adds costs without adding benefit.\\nIf you are considering clustering both the base table and the materialized\\nviews, Snowflake recommends that you start by clustering only the materialized\\nviews, and that you monitor performance and cost before and after adding\\nclustering to the base table.\\nIf you plan to create a table, load it, and create a clustered materialized\\nview(s) on the table, then Snowflake recommends that you create the\\nmaterialized views last (after loading as much data as possible). This\\ncan save money on the initial data load, because it avoids some extra effort\\nto maintain the clustering of the materialized view the first time that\\nthe materialized view is loaded.\\nFor more details about clustering, refer to:\\n\\nUnderstanding Snowflake Table Structures\\nAutomatic Clustering\\n\\nFor more information about the costs of clustering materialized views, refer to:\\n\\nMaterialized Views Cost\\nBest Practices for Materialized Views\\n\\n\\n\\nMaterialized Views and Time Travel¶\\nCurrently, you cannot use Time Travel to\\nquery historical data for materialized views.\\nHowever, note the following:\\n\\nYou can use Time Travel to\\nclone a database or schema containing a materialized view at a specific point in the past.\\nSnowflake clones the materialized view at the specified point in time.\\nTo support cloning with Time Travel, Snowflake does maintain historical data for materialized views. You will be billed for\\nthe storage costs for historical data  for materialized views.\\nThe storage costs depend on the data retention period for the materialized\\nviews, which is determined by the DATA_RETENTION_TIME_IN_DAYS parameter. Materialized\\nviews inherit this parameter from its parent schema or database.\\n\\n\\n\\nBest Practices for Materialized Views¶\\nThe following sections summarize the best practices for working with materialized views:\\n\\nBest Practices for Creating Materialized Views\\nBest Practices for Maintaining Materialized Views\\nBest Practices for Clustering Materialized Views and their Base Tables\\n\\n\\nBest Practices for Creating Materialized Views¶\\n\\nMost materialized views should do one or both of the following:\\n\\nFilter data. You can do this by:\\n\\nFiltering rows (e.g. defining the materialized view so that only very recent data is included).\\nIn some applications, the best data to store is the abnormal data. For example, if you are monitoring\\npressure in a gas pipeline to estimate when pipes might fail, you might store all pressure data in the base\\ntable, and store only unusually high pressure measurements in the materialized view. Similarly, if you are\\nmonitoring network traffic, your base table might store all monitoring information, while your materialized\\nview might store only unusual and suspicious information (e.g. from IP addresses known to launch\\nDOS (Denial Of Service) attacks).\\nFiltering columns (e.g. selecting specific columns rather than “SELECT * …”).\\nUsing SELECT * ... to define a materialized view typically is expensive. It can also lead to future\\nerrors; if columns are added to the base table later (e.g. ALTER TABLE ... ADD COLUMN ...), the\\nmaterialized view does not automatically incorporate the new columns.\\n\\n\\nPerform resource-intensive operations and store the results so that the resource intensive operations\\ndon’t need to be performed as often.\\n\\n\\nYou can create more than one materialized view for the same base table. For example, you can create one materialized\\nview that contains just the most recent data, and another materialized view that stores unusual data. You can then\\ncreate a non-materialized view that joins the two tables and shows recent data that matches unusual historical\\ndata so that you can quickly detect unusual situations, such as a DOS (denial of service) attack that is ramping up.\\nSnowflake recommends materialized views for unusual data only when:\\n\\n\\nThe base table is not clustered, or the columns that contain the unusual data are not already part of the base\\ntable’s clustering key.\\nThe data is unusual enough that it is easy to isolate, but not so unusual that it is rarely used. (If the data\\nis rarely used, the cost of maintaining the materialized view is likely to outweigh the performance benefit and\\ncost savings from being able to access it quickly when it is used.)\\n\\n\\n\\n\\n\\n\\nBest Practices for Maintaining Materialized Views¶\\n\\nSnowflake recommends batching DML operations on the base table:\\n\\nDELETE: If tables store data for the most recent time period (e.g. the most recent day or week or month),\\nthen when you trim your base table by deleting old data, the changes to the base table are propagated to the\\nmaterialized view. Depending upon how the data is distributed across the micro-partitions, this could cause you\\nto pay more for background updates of the materialized views. In\\nsome cases, you might be able to reduce costs by deleting less frequently (e.g. daily rather than hourly, or\\nhourly rather than every 10 minutes).\\nIf you do not need to keep a specific amount of old data, you should experiment to find the best balance between cost and\\nfunctionality.\\n\\nINSERT, UPDATE, and MERGE: Batching these types of DML statements on the\\nbase table can reduce the cost of maintaining the materialized views.\\n\\n\\n\\n\\n\\nBest Practices for Clustering Materialized Views and their Base Tables¶\\n\\nIf you create a materialized view on a base table, and if the materialized views are accessed frequently and the\\nbase table is not accessed frequently, it is usually more efficient to avoid clustering the base table.\\nIf you create a materialized view on a clustered table, consider removing any clustering on the base table, because\\nany change to the clustering of the base table will eventually require a refresh of the materialized view,\\nwhich adds to the materialized view’s maintenance costs.\\n\\nClustering materialized views, especially materialized views on base tables that change frequently, increases\\ncosts. Do not cluster more materialized views than you need to.\\nAlmost all information about clustering tables also applies to clustering materialized views.\\nFor more information about clustering tables, see Strategies for Selecting Clustering Keys.\\n\\n\\n\\n\\nExamples¶\\nThis section contains additional examples of creating and using materialized views. For a simple, introductory example, see\\nBasic Example: Creating a Materialized View (in this topic).\\n\\nSimple Materialized View¶\\nThis first example illustrates a simple materialized view and a simple query on the view.\\n\\nCreate the table and load the data, and create the view:\\nCREATE TABLE inventory (product_ID INTEGER, wholesale_price FLOAT,\\n  description VARCHAR);\\n    \\nCREATE OR REPLACE MATERIALIZED VIEW mv1 AS\\n  SELECT product_ID, wholesale_price FROM inventory;\\n\\nINSERT INTO inventory (product_ID, wholesale_price, description) VALUES \\n    (1, 1.00, 'cog');\\n\\nCopy\\nSelect data from the view:\\nSELECT product_ID, wholesale_price FROM mv1;\\n+------------+-----------------+\\n| PRODUCT_ID | WHOLESALE_PRICE |\\n|------------+-----------------|\\n|          1 |               1 |\\n+------------+-----------------+\\n\\nCopy\\n\\n\\n\\nJoining a Materialized View¶\\nYou can join a materialized view with a table or another view. This example builds on the previous example by creating an additional\\ntable, and then a non-materialized view that shows profits by joining the materialized view to a table:\\n\\nCREATE TABLE sales (product_ID INTEGER, quantity INTEGER, price FLOAT);\\n\\nINSERT INTO sales (product_ID, quantity, price) VALUES \\n   (1,  1, 1.99);\\n\\nCREATE or replace VIEW profits AS\\n  SELECT m.product_ID, SUM(IFNULL(s.quantity, 0)) AS quantity,\\n      SUM(IFNULL(quantity * (s.price - m.wholesale_price), 0)) AS profit\\n    FROM mv1 AS m LEFT OUTER JOIN sales AS s ON s.product_ID = m.product_ID\\n    GROUP BY m.product_ID;\\n\\nCopy\\nSelect data from the view:\\nSELECT * FROM profits;\\n+------------+----------+--------+\\n| PRODUCT_ID | QUANTITY | PROFIT |\\n|------------+----------+--------|\\n|          1 |        1 |   0.99 |\\n+------------+----------+--------+\\n\\nCopy\\n\\n\\n\\nSuspending Updates to a Materialized View¶\\nThe following example temporarily suspends the use (and maintenance)\\nof the mv1 materialized view, and shows that queries on that view\\ngenerate an error message while the materialized view is suspended:\\n\\nALTER MATERIALIZED VIEW mv1 SUSPEND;\\n    \\nINSERT INTO inventory (product_ID, wholesale_price, description) VALUES \\n    (2, 2.00, 'sprocket');\\n\\nINSERT INTO sales (product_ID, quantity, price) VALUES \\n   (2, 10, 2.99),\\n   (2,  1, 2.99);\\n\\nCopy\\nSelect data from the materialized view:\\nSELECT * FROM profits ORDER BY product_ID;\\n\\nCopy\\nOutput:\\n002037 (42601): SQL compilation error:\\nFailure during expansion of view 'PROFITS': SQL compilation error:\\nFailure during expansion of view 'MV1': SQL compilation error: Materialized View MV1 is invalid.\\n\\nCopy\\nResume:\\nALTER MATERIALIZED VIEW mv1 RESUME;\\n\\nCopy\\nSelect data from the materialized view:\\nSELECT * FROM profits ORDER BY product_ID;\\n+------------+----------+--------+\\n| PRODUCT_ID | QUANTITY | PROFIT |\\n|------------+----------+--------|\\n|          1 |        1 |   0.99 |\\n|          2 |       11 |  10.89 |\\n+------------+----------+--------+\\n\\nCopy\\n\\n\\n\\nClustering a Materialized View¶\\nThis example creates a materialized view and then later clusters it:\\n\\nThese statements create two tables that track information about segments of a\\npipeline (e.g. for natural gas).\\nThe segments that are most likely to fail in the near future are often the segments that are oldest, or that are\\nmade of materials that corrode easily, or that had experienced periods of unusually high pressure, so\\nthis example tracks each pipe’s age, pressure, and material (iron, copper, PVC plastic, etc.).\\n\\nCREATE TABLE pipeline_segments (\\n    segment_ID BIGINT,\\n    material VARCHAR, -- e.g. copper, cast iron, PVC.\\n    installation_year DATE,  -- older pipes are more likely to be corroded.\\n    rated_pressure FLOAT  -- maximum recommended pressure at installation time.\\n    );\\n    \\nINSERT INTO pipeline_segments \\n    (segment_ID, material, installation_year, rated_pressure)\\n  VALUES\\n    (1, 'PVC', '1994-01-01'::DATE, 60),\\n    (2, 'cast iron', '1950-01-01'::DATE, 120)\\n    ;\\n\\nCREATE TABLE pipeline_pressures (\\n    segment_ID BIGINT,\\n    pressure_psi FLOAT,  -- pressure in Pounds per Square Inch\\n    measurement_timestamp TIMESTAMP\\n    );\\nINSERT INTO pipeline_pressures \\n   (segment_ID, pressure_psi, measurement_timestamp) \\n  VALUES\\n    (2, 10, '2018-09-01 00:01:00'),\\n    (2, 95, '2018-09-01 00:02:00')\\n    ;\\n\\nCopy\\n\\nThe pipeline segments don’t change very frequently, and the oldest pipeline segments are the segments most\\nlikely to fail, so create a materialized view of the oldest segments.\\n\\nCREATE MATERIALIZED VIEW vulnerable_pipes \\n  (segment_ID, installation_year, rated_pressure) \\n  AS\\n    SELECT segment_ID, installation_year, rated_pressure\\n        FROM pipeline_segments \\n        WHERE material = 'cast iron' AND installation_year < '1980'::DATE;\\n\\nCopy\\n\\nYou can add clustering or change the clustering key. For example, to cluster on installation_year:\\n\\nALTER MATERIALIZED VIEW vulnerable_pipes CLUSTER BY (installation_year);\\n\\nCopy\\n\\nNew pressure measurements arrive frequently (perhaps every 10\\nseconds), so maintaining a materialized view on the pressure\\nmeasurements would be expensive. Therefore, even though high\\nperformance (fast retrieval) of recent pressure data is important,\\nthe pipeline_pressures table starts without a materialized view.\\nIf performance is too slow, you can create a materialized view that contains only recent pressure\\ndata, or that contains data only about abnormal high-pressure events.\\nCreate a (non-materialized) view that combines information from the\\nmaterialized view and the pipeline_pressures table:\\n\\nCREATE VIEW high_risk AS\\n    SELECT seg.segment_ID, installation_year, measurement_timestamp::DATE AS measurement_date, \\n         DATEDIFF('YEAR', installation_year::DATE, measurement_timestamp::DATE) AS age, \\n         rated_pressure - age AS safe_pressure, pressure_psi AS actual_pressure\\n       FROM vulnerable_pipes AS seg INNER JOIN pipeline_pressures AS psi \\n           ON psi.segment_ID = seg.segment_ID\\n       WHERE pressure_psi > safe_pressure\\n       ;\\n\\nCopy\\n\\nNow list the high-risk pipeline segments:\\n\\n\\nSELECT * FROM high_risk;\\n+------------+-------------------+------------------+-----+---------------+-----------------+\\n| SEGMENT_ID | INSTALLATION_YEAR | MEASUREMENT_DATE | AGE | SAFE_PRESSURE | ACTUAL_PRESSURE |\\n|------------+-------------------+------------------+-----+---------------+-----------------|\\n|          2 | 1950-01-01        | 2018-09-01       |  68 |            52 |              95 |\\n+------------+-------------------+------------------+-----+---------------+-----------------+\\n\\nCopy\\n\\nThis shows that the pipeline segment with segment_id = 2, which is made of a\\nmaterial that corrodes, is old. This segment has never experienced pressure higher than\\nthe maximum pressure rating at the time it was installed, but because of the potential\\nfor corrosion, its “safe limit” has declined over time, and the highest pressure it has\\nexperienced is higher than the pressure that was recommended for a pipe as old\\nas the pipe was at the time of the pressure measurement.\\n\\n\\n\\n\\nCreating a Materialized View on Shared Data¶\\nYou can create a materialized view on shared data.\\nAccount1:\\ncreate or replace table db1.schema1.table1(c1 int);\\ncreate or replace share sh1;\\ngrant usage on database db1 to share sh1;\\nalter share sh1 add accounts = account2;\\ngrant usage on schema db1.schema1 to share sh1;\\ngrant select on table db1.schema1.table1 to share sh1;\\n\\nCopy\\nAccount2:\\ncreate or replace database dbshared from share account1.sh1;\\ncreate or replace materialized view mv1 as select * from dbshared.schema1.table1 where c1 >= 50;\\n\\nCopy\\n\\nNote\\nRemember that maintaining materialized views will consume credits. When you create a materialized view on\\nsomeone else’s shared table, the changes to that shared table will result in charges to you as your\\nmaterialized view is maintained.\\n\\n\\n\\nSharing a Materialized View¶\\nYou can use Snowflake’s data sharing feature to share a materialized view.\\nFor more information about data sharing, see Data sharing and collaboration in Snowflake.\\n\\nNote\\nRemember that maintaining materialized views will consume credits. When someone else creates a materialized\\nview on your shared data, any changes to your shared data can cause charges to the people who have materialized\\nviews on your shared data. The larger the number of materialized views on a shared base table, the more important\\nit is to update that base table efficiently to minimize the costs of maintaining materialized views.\\n\\n\\n\\n\\nTroubleshooting¶\\n\\nCompilation Error: Failure during expansion of view '<name>': SQL compilation error: Materialized View <name> is invalid.¶\\n\\nPossible Causes:\\n\\nThe materialized view has been suspended. For more information about suspending and resuming views, see\\nALTER MATERIALIZED VIEW.\\nA change to the base table of the materialized view has invalidated the materialized view. For example, this error is\\nreturned if:\\n\\nThe base table is dropped.\\nA column in the base table column has been dropped.\\n\\n\\nThe background process has encountered an error of a specific type (for example, a “division by zero” error) and has failed\\nto refresh the materialized view.\\n\\n\\nPossible Solutions:\\n\\nIf the view has been suspended:\\n\\nConsider resuming the view by executing\\nALTER MATERIALIZED VIEW … RESUME.\\nConsider running the query against the base table. However, this is likely\\nto consume more credits and take longer than running the query against the materialized view.\\n\\n\\nIf the base table has been modified or dropped:\\n\\nIf the base table has been dropped, then drop the materialized view.\\nIf the base table has been modified (e.g. has dropped a column referenced by the view), and if the materialized view would still\\nbe useful with the new version of the table, then consider dropping and re-creating the materialized view, using the\\ncolumns that remain in the base table.\\nIf no other cause of the error message is apparent, consider dropping and re-creating the materialized view.\\nConsider running the query against the base table. However, this is likely\\nto consume more credits and take longer than running the query against the materialized view.\\n\\n\\nIf the background process has failed to refresh the materialized view due to an error, the error message should include\\ndetails about why the materialized view has been invalidated. For example:\\nFailure during expansion of view 'MY_MV':\\n  SQL compilation error: Materialized View MY_MV is invalid.\\n  Invalidation reason: Division by zero\\n\\n\\n\\n\\nIf this occurs, address the problem described in the error message, and resume the materialized view by using the\\nALTER MATERIALIZED VIEW … RESUME command.\\n\\n\\n\\n\\nSHOW MATERIALIZED VIEWS Command Shows Materialized Views That Are Not Updated¶\\n\\nPossible Cause:\\nOne possible cause is that the refresh failed because the SELECT statement in the view definition\\nfailed.\\nBecause the refresh is done performed by the background process, you will not see an error message at the time the refresh is\\nattempted. Instead, you will see the error message when you query the materialized view or when you execute\\nSHOW MATERIALIZED VIEWS.\\n\\nPossible Solution:\\nIf the invalid column is true, check the invalid_reason column for the reason why the view was invalidated.\\nIn some cases, you might be able to debug the problem by manually running the SELECT statement in the materialized view’s\\ndefinition, or by running a simpler (less expensive) SELECT statement on the table referenced in the materialized view’s\\ndefinition.\\nIf you do not know the exact definition of the materialized view, you can find it in the output of\\nSHOW MATERIALIZED VIEWS or by using the GET_DDL function.\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageDeciding When to Create a Materialized ViewAbout Materialized Views in SnowflakeCreating and Working With Materialized ViewsMaterialized Views CostMaterialized Views and ClusteringMaterialized Views and Time TravelBest Practices for Materialized ViewsExamplesTroubleshootingRelated contentOverview of ViewsTable, view, & sequence DDLWorking with views in SnowsightDynamic tablesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer-guide/extensibility', 'title': 'Extending Snowflake with Functions and Procedures | Snowflake Documentation', 'language': 'en'}, page_content='Extending Snowflake with Functions and Procedures | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIMachine LearningSnowflake MLSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresFunction or Procedure?GuidelinesStored ProceduresUser-Defined FunctionsLogging and TracingExternal Network AccessPackaging Handler CodeSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewReferenceAPI ReferenceDeveloperFunctions and Procedures\\n\\nExtending Snowflake with Functions and Procedures¶\\nYou can extend the SQL you use in Snowflake by writing user-defined functions (UDFs) and stored procedures that you can call from SQL. When\\nyou write a UDF or procedure, you write its logic in one of the supported handler languages, then create it using SQL.\\nWith a UDF, you typically calculate and return a value. With a stored procedure, you generally perform administrative operations by\\nexecuting SQL statements.\\nYou can also write an external function whose logic executes on a system external to Snowflake, such as a cloud provider.\\n\\nChoosing whether to write a stored procedure or a user-defined functionChoose between writing a stored procedure and writing a user-defined function.\\n\\nDesign Guidelines and Constraints for Functions and ProceduresRead more about the guidelines that functions and procedures share, including guidelines related to deployment options, security practices,\\nplatform constraints, and conventions.\\n\\nPackaging Handler CodeUse tools to package handler code and ensure that dependencies are available on Snowflake.\\n\\nStored procedures overviewLearn the benefits and supported languages.\\n\\nUser-defined functions overviewLearn the types of UDFs and supported languages.\\n\\nLogging and tracing overviewRecord handler code activity by capturing log messages and trace events, storing the data in a database you can query later.\\n\\nExternal network access overviewCreate secure access to specific network locations external to Snowflake, then use that access from within the handler code.\\n\\nIntroduction to external functionsAccess custom code that runs outside of Snowflake, such as API services that provide geocoding and machine learning models.\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentDeveloper ResourcesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/budgets', 'title': 'Monitoring credit usage with Budgets | Snowflake Documentation', 'language': 'en'}, page_content='Monitoring credit usage with Budgets | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingCost ManagementOverviewUnderstanding costExploring costOptimizing costMonitoring costBudgetsUnderstanding costUsing budgetsTroubleshootingTutorialResource monitorsAttributing costControlling costBillingAccessing Usage StatementsReconciling Usage StatementsGuidesCost & BillingMonitoring costBudgets\\n\\nMonitoring credit usage with Budgets¶\\n\\n Feature — Generally Available\\nBudgets is not supported in Gov regions\\nor trial accounts.\\n\\nBudgets enables account-level monitoring and notification of Snowflake credit usage for a group of specific Snowflake objects.\\n\\nBudgets overview¶\\nA budget defines a monthly spending limit on the compute costs for\\na Snowflake account or a custom group of Snowflake objects. When the spending limit is projected to be exceeded, a daily\\nnotification email is sent to the email addresses designated to receive budget notifications.\\n\\nAccount budget and custom budgets¶\\nThe account budget monitors spending for all supported objects in the account.\\nTo get notifications for the account budget, set up the spending limit and specify email addresses to receive notifications.\\nYou can also create a custom budget to monitor the spending limits for a specific\\ngroup of supported objects. Like the account budget, you must set the spending\\nlimit and the notification email addresses in order to receive notification emails.\\nTo start using budgets in Snowflake, activate the account budget.\\n\\n\\nSpending limit and time interval¶\\nThe spending limit is expressed in Snowflake credits and is used for alerting and notifications purposes only. The spending limit\\nis set for the time interval of one calendar month.\\nThe time interval starts at 12:00AM UTC on the first day of the month and ends at 11:59PM UTC on the last day of the month. If a budget\\nis created after the first day of the month, the first monitoring interval is the period until the last day of the current month, then\\nis reset on the first of the following month.\\nIf you activate your account budget after the first day of the month, data starting from the beginning of the month is backfilled\\nand used to determine if you are going to exceed your spending limit for the month.\\nIf you create a custom budget after the first day of the month, historical data for the month is not backfilled. Forecasting\\nfor future spend is based on the credit usage in the days only after the budget is enabled.\\n\\nNote\\nIf you create a custom budget after the first day of the month, you might exceed your budget for the first month. In a partial\\nmonth, credit usage before the budget is created is not backfilled for custom budgets and might result in incorrect\\nforecasting. Subsequent months will correctly forecast credit usage based on spending history.\\n\\n\\n\\nSupported objects¶\\nEach object in the list of supported objects contributes to the spending limit for the account budget.\\nFor a custom budget, you can select a group of specific objects to monitor. When you add an object to a custom budget,\\nthe budget monitors all compute\\ncosts for the object, including background maintenance operations and serverless features. For example, if you add a table\\nto a custom budget, and the table has automatic clustering enabled, the budget monitors credit usage for the background maintenance\\nfor automatic clustering.\\nA budget can monitor the following Snowflake objects:\\n\\n\\n\\n\\n\\n\\nObject\\nMonitored costs\\n\\n\\n\\nCompute pool\\nCompute pool usage for Snowpark Container Services. For more information, see Compute pool cost.\\n\\nDatabases\\nWhen you add a database to a budget, all supported objects the database contains are also automatically added. The budget\\nmonitors credit usage for the following objects and serverless features:\\n\\nSupported schema objects as described above.\\nReplication for secondary (replica) databases.\\n\\nNote\\nReplication costs for secondary databases that are replicated in a replication or failover group can only be\\nmonitored by the account budget.\\n\\n\\n\\n\\n\\nMaterialized views\\nBackground maintenance for the materialized view. For more information, see Materialized Views Cost.\\n\\nPipes\\nResource consumption for loading data using Snowpipe. For more information, see Snowpipe costs.\\n\\nTables\\nBackground maintenance operations for automatic clustering and\\nsearch optimization if they are enabled on the table.\\n\\nTasks\\nServerless tasks are monitored by a custom budget. To monitor the credit usage for a task that executes using a user-managed warehouse,\\nyou must add the warehouse to the budget. For more information, see Task costs.\\n\\nSchemas\\nWhen you add a schema to a budget, all supported objects the schema contains are also automatically added. The budget\\nmonitors the credit usage for schema objects as described above.\\n\\nWarehouses\\nCompute resources for query execution, web interface, and other features (see Virtual warehouse credit usage),\\nserverless tasks, and cloud services compute.\\n\\n\\n\\n\\nSupported serverless features¶\\nBudgets monitors the serverless feature credit usage for the supported objects contained in a budget. The following table lists\\nserverless features and account and custom budget support for monitoring credit usage.\\n\\nNote\\nAccount budget support for serverless features depends on the availability of\\nservice types in the Account Usage METERING_HISTORY view view.\\n\\n\\n\\nServerless feature\\nAccount budget\\nCustom budget\\nObject\\n\\n\\n\\nAI_SERVICES\\n✔\\n\\n\\n\\nAUTO_CLUSTERING\\n✔\\n✔\\nTable\\n\\nCOPY_FILES\\n✔\\n\\n\\n\\nHYBRID_TABLE_REQUESTS\\n✔\\n\\n\\n\\nMATERIALIZED_VIEW\\n✔\\n✔\\nMaterialized view\\n\\nPIPE\\n✔\\n✔\\nPipe\\n\\nQUERY_ACCELERATION\\n✔\\n\\n\\n\\nQUERY_ACCELERATION_READER\\n✔\\n\\n\\n\\nREPLICATION\\n✔\\n✔\\nDatabase\\n\\nSEARCH_OPTIMIZATION\\n✔\\n✔\\nTable\\n\\nSERVERLESS_TASK\\n✔\\n✔\\nTask\\n\\nSNOWPARK_CONTAINER_SERVICES\\n✔\\n✔\\n\\n\\nSNOWPIPE_STREAMING\\n✔\\n✔\\nTable\\n\\nWAREHOUSE_METERING\\n✔\\n✔\\nWarehouse\\n\\nWAREHOUSE_METERING_READER\\n✔\\n✔\\nWarehouse\\n\\n\\n\\n\\n\\n\\nBudgets notifications¶\\nA budget sends a daily alert notification when current spending is on track to exceed the spending limit based on time-series forecasting.\\nNotifications begin when the projected spending is more than 10% above the spending limit.\\nTo receive automatic alert notifications, you must do the following:\\n\\nSet a spending limit for the budget.\\nSet up email notifications for the budget.\\n\\nThe following is an example of a notification email:\\n\\n\\n\\nYou can disable email notifications for a budget by calling the\\nSET_NOTIFICATION_MUTE_FLAG method.\\n\\n\\n\\nBudgets roles and privileges¶\\nTo enable a custom role in your account to work with budgets, you can grant the following roles and privileges.\\n\\nApplication roles to manage the account budget¶\\nYou can grant the following application roles to a custom role in your account to monitor or manage the account budget:\\n\\n\\nApplication Role\\nDescription\\n\\n\\n\\nBUDGET_VIEWER\\nEnables a role to view budget usage data and list the resources in the budget.\\n\\nBUDGET_ADMIN\\nEnables a role to activate or deactivate the account budget and modify the properties of a budget including spending limit, the list\\nof email addresses to receive notifications, and the mute notifications setting.\\n\\n\\n\\nIn addition to the application role, there are other required privileges that must be granted to enable a custom role to monitor or manage\\nthe account budget. For more information, see Required roles and privileges.\\n\\n\\nInstance roles to manage custom budgets¶\\nEach custom budget has instance roles that you can grant to other roles to view or modify the budget:\\n\\n\\n\\n\\n\\n\\nInstance Role\\nDescription\\n\\n\\n\\nVIEWER\\nEnables a role to view budget usage data.\\n\\nADMIN\\nEnables a role to modify the properties of a budget including spending limit, list of email addresses to receive\\nnotifications, mute notifications setting, and the objects in the group.\\n\\n\\n\\nIn addition to the instance role, there are other required privileges that must be granted to enable a custom role to create, monitor, or\\nmodify a custom budget. For more information, see Required roles and privileges.\\n\\n\\nRequired roles and privileges¶\\nIn addition to the application or instance roles, the following privileges must be granted to a custom role to monitor or manage budgets:\\n\\n\\nPrivilege or role\\nBudget type\\nCreate\\nManage\\nMonitor\\nNotes\\n\\n\\n\\nUSAGE_VIEWER\\nBoth\\n✔\\n✔\\n✔\\nGrant the Snowflake database role USAGE_VIEWER to the custom role.\\n\\nUSAGE\\nCustom\\n✔\\n✔\\n✔\\nGrant the USAGE privilege on the database that contains the custom budget.\\n\\nUSAGE\\nCustom\\n✔\\n✔\\n✔\\nGrant the USAGE privilege on the schema that contains the custom budget.\\n\\nSNOWFLAKE.BUDGET_CREATOR\\nCustom\\n✔\\n\\n\\nGrant this database role to the custom role to enable it to create custom budgets.\\n\\nCREATE SNOWFLAKE.CORE.BUDGET\\nCustom\\n✔\\n\\n\\nGrant this privilege on the schema that will contain the custom budget.\\n\\nAPPLYBUDGET\\nCustom\\n\\n✔\\n\\nMust be granted on each object to be added or removed from a custom budget.\\n\\nUSAGE\\nCustom\\n\\n✔\\n\\nTo add or remove schema objects from a custom budget, this privilege must be granted on the database and schema that contains\\nthe object.\\n\\n\\n\\nFor more information and examples, see the following sections:\\n\\nCreate a custom role to manage the account budget\\nCreate a custom role to monitor the account budget\\nCreate a custom role to monitor a custom budget\\nCreate a custom role to manage a custom budget\\nCreate a custom role to create budgets\\n\\n\\n\\n\\nLimitations¶\\nThe following are limitations for Budgets:\\n\\nInstances of the BUDGET class can’t be replicated to target accounts.\\n\\nAn account can contain a maximum of 100 custom budgets.\\nCurrently, Budgets does not support monitoring costs for Hybrid tables.\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageBudgets overviewBudgets roles and privilegesLimitationsRelated contentActivating Budgets and creating custom budgetsUnderstanding cost for BudgetsTroubleshooting budgetsMonitoring costLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/intro-key-concepts', 'title': 'Key Concepts & Architecture | Snowflake Documentation', 'language': 'en'}, page_content='Key Concepts & Architecture | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewConceptsArchitectureCloud PlatformsCloud RegionsEditions, releases and featuresEditionsReleasesFeaturesData ManagementData LifecycleContinuous Data ProtectionComplianceRegulatory ComplianceTutorialsSample DataContacting SupportGetting StartedConceptsArchitecture\\n\\nKey Concepts & Architecture¶\\nSnowflake’s Data Cloud is powered by an advanced data platform provided as a self-managed service. Snowflake enables data storage,\\nprocessing, and analytic solutions that are faster, easier to use, and far more flexible than traditional offerings.\\nThe Snowflake data platform is not built on any existing database technology or “big data” software platforms such as Hadoop. Instead, Snowflake combines a completely new SQL query engine with an innovative architecture natively designed for the cloud. To the user,\\nSnowflake provides all of the functionality of an enterprise analytic database, along with many additional special features and unique capabilities.\\n\\nData Platform as a Self-managed Service¶\\nSnowflake is a true self-managed service, meaning:\\n\\nThere is no hardware (virtual or physical) to select, install, configure, or manage.\\nThere is virtually no software to install, configure, or manage.\\nOngoing maintenance, management, upgrades, and tuning are handled by Snowflake.\\n\\nSnowflake runs completely on cloud infrastructure. All components of Snowflake’s service (other than optional command line clients, drivers, and connectors), run in public cloud infrastructures.\\nSnowflake uses virtual compute instances for its compute needs and a storage service for persistent storage of data. Snowflake cannot be run on private cloud infrastructures (on-premises or\\nhosted).\\nSnowflake is not a packaged software offering that can be installed by a user. Snowflake manages all aspects of software installation and updates.\\n\\n\\nSnowflake Architecture¶\\nSnowflake’s architecture is a hybrid of traditional shared-disk and shared-nothing database architectures. Similar to shared-disk architectures, Snowflake uses a central data repository for persisted data that is\\naccessible from all compute nodes in the platform. But similar to shared-nothing architectures, Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of\\nthe entire data set locally. This approach offers the data management simplicity of a shared-disk architecture, but with the performance and scale-out benefits of a shared-nothing architecture.\\n\\n\\n\\n\\n\\nSnowflake’s unique architecture consists of three key layers:\\n\\nDatabase Storage\\nQuery Processing\\nCloud Services\\n\\n\\nDatabase Storage¶\\nWhen data is loaded into Snowflake, Snowflake reorganizes that data into its internal optimized, compressed, columnar format. Snowflake stores this optimized data in cloud storage.\\nSnowflake manages all aspects of how this data is stored — the organization, file size, structure, compression, metadata, statistics, and other aspects of data storage are handled by Snowflake. The data objects stored by Snowflake are not directly visible nor accessible by customers; they are only accessible through SQL query operations run using Snowflake.\\n\\n\\nQuery Processing¶\\nQuery execution is performed in the processing layer. Snowflake processes queries using “virtual warehouses”. Each virtual warehouse is an MPP compute cluster composed of multiple compute nodes allocated by Snowflake from a cloud provider.\\nEach virtual warehouse is an independent compute cluster that does not share compute resources with other virtual warehouses. As a result, each virtual warehouse has no impact on the performance of other virtual warehouses.\\nFor more information, see Virtual warehouses.\\n\\n\\nCloud Services¶\\nThe cloud services layer is a collection of services that coordinate activities across Snowflake. These services tie together all of the different components of Snowflake in order to process user requests, from login to query dispatch.\\nThe cloud services layer also runs on compute instances provisioned by Snowflake from the cloud provider.\\nServices managed in this layer include:\\n\\nAuthentication\\nInfrastructure management\\nMetadata management\\nQuery parsing and optimization\\nAccess control\\n\\n\\n\\n\\nConnecting to Snowflake¶\\nSnowflake supports multiple ways of connecting to the service:\\n\\nA web-based user interface from which all aspects of managing and using Snowflake can be accessed.\\nCommand line clients (e.g. SnowSQL) which can also access all aspects of managing and using Snowflake.\\nODBC and JDBC drivers that can be used by other applications (e.g. Tableau) to connect to Snowflake.\\nNative connectors (e.g. Python, Spark) that can be used to develop applications for connecting to Snowflake.\\nThird-party connectors that can be used to connect applications such as ETL tools (e.g. Informatica) and BI tools (e.g. ThoughtSpot) to Snowflake.\\n\\nFor more information, see Logging in to Snowflake.\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageData Platform as a Self-managed ServiceSnowflake ArchitectureConnecting to SnowflakeLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/data-load-local-file-system', 'title': 'Bulk loading from a local file system | Snowflake Documentation', 'language': 'en'}, page_content='Bulk loading from a local file system | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemChoosing an Internal StageStaging FilesCopying Data from a Local File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingLoading Semi-Structured DataIntroductionSupported FormatsConsiderationsAccessing Unstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData LoadingLocal File System\\n\\nBulk loading from a local file system¶\\nThis set of topics describes how to use the COPY command to bulk load data from a local file system into tables using an internal (i.e.\\nSnowflake-managed) stage. For instructions on loading data from a cloud storage location that you manage, refer to Bulk loading from Amazon S3, Bulk loading from Google Cloud Storage, or Bulk loading from Microsoft Azure.\\nAs illustrated in the diagram below, loading data from a local file system is performed in two, separate steps:\\n\\nStep 1:\\nUpload (i.e. stage) one or more data files to a Snowflake stage (named internal stage or table/user stage) using the PUT command.\\n\\nStep 2:\\nUse the COPY INTO <table> command to load the contents of the staged file(s) into a Snowflake database table.\\nRegardless of the stage you use, this step requires a running virtual warehouse that is also the current (i.e. in use) warehouse for the session. The warehouse provides the compute resources to\\nperform the actual insertion of rows into the table.\\n\\n\\n\\n\\n\\n\\nTip\\nThe instructions in this set of topics assume you have read Preparing to load data and have created a named file format, if desired.\\nBefore you begin, you may also want to read Data loading considerations for best practices, tips, and other guidance.\\n\\nNext Topics:\\n\\nConfiguration tasks (complete as needed):\\n\\n\\nChoosing an internal stage for local files\\n\\n\\n\\nData loading tasks (complete for each set of files you load):\\n\\n\\nStaging data files from a local file system\\nCopying data from an internal stage\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnloading into a Snowflake stageLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/release-notes/behavior-changes', 'title': 'Behavior Change Log | Snowflake Documentation', 'language': 'en'}, page_content=\"Behavior Change Log | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusWhat's NewBehavior ChangesAbout Behavior ChangesBehavior Change PolicyBehavior Change ManagementCurrent Bundles2024_06 Bundle (Disabled by Default)2024_05 Bundle (Enabled by Default)2024_04 Bundle (Generally Enabled)Previous Bundles2024_03 Bundle (Generally Enabled)2024_02 Bundle (Generally Enabled)2024_01 Bundle (Generally Enabled)2023_08 Bundle (Generally Enabled)2023_07 Bundle2023_06 Bundle2023_05 Bundle2023_04 Bundle2023_03 Bundle2023_02 Bundle2023_01 Bundle2022_08 Bundle2022_07 Bundle2022_06 Bundle2022_05 Bundle2022_04 Bundle2022_03 Bundle2022_02 Bundle2022_01 BundleEarlier BundlesUnbundled ChangesDeprecated FeaturesPreview FeaturesPerformance ImprovementsSQL ImprovementsClients, Drivers, and LibrariesMonthly ReleasesClient versions & support policySnowflake ConnectorsSnowflake Connector for Google Analytics Raw DataSnowflake Connector for Google Analytics Aggregate DataSnowflake Connector for ServiceNowSnowflake Connector for ServiceNow V2Snowflake Connector for MySQLSnowflake Connector for PostgreSQLNative SDK for ConnectorsNative SDK for Connectors Java libraryNative SDK for Connectors Java Test libraryNative SDK for Connectors Java TemplateNative SDK Example Java GitHub ConnectorReleasesBehavior Changes\\n\\nBehavior Change Log¶\\nTo help you manage your operations and minimize disruption to your Snowflake service, we document behavior changes that may impact your usage,\\nincluding:\\n\\nUpcoming pending changes that can be enabled (for testing), unless otherwise noted.\\nRecently implemented changes that were previously pending/disabled.\\n\\nFor behavior changes that are not associated with a specific behavior change bundle, see Unbundled Behavior Changes.\\nIf you have questions about any of these behavior changes, please feel free to contact Snowflake Support.\\n\\nUpcoming Pending Changes¶\\nThe following table lists behavior changes that are pending. If the change is in a monthly behavior change bundle, the bundle is currently\\ndisabled, but can be enabled (for testing purposes).\\nTo enable a bundle that is currently disabled by default, use the\\nSYSTEM$ENABLE_BEHAVIOR_CHANGE_BUNDLE system function.\\nThis is typically done in your non-production accounts (for development and testing purposes).\\n\\nImportant\\nAll information in this table, including planned versions and dates, is subject to change; the information is provided only as a guideline\\nfor any updates you need to make to accommodate the changes.\\nIf a link is not provided to the individual pending behavior changes, the release in which the bundle was introduced has not started or is\\nis still in progress.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBundle\\nStatus / History\\nPending Changes\\nAdditional Notes\\n\\n\\n\\n2024_07\\nPlanned to be introduced, Disabled by Default, in the 8.32 release (scheduled for August 26-27), subject to change;\\nonce introduced, account admins can enable for testing.\\nStatus planned to change in September 2024 to Enabled by Default; however, this schedule is subject to change.\\nStatus planned to change in October 2024 to Generally Enabled; however, this schedule is subject to change.\\n\\n\\nThis is a preview of the upcoming release of the 2024_07 bundle.\\n\\n2024_06\\nIntroduced in the 8.27 release as Disabled by Default, (scheduled for July 22-25);\\naccount admins can enable for testing.\\nStatus planned to change in August 2024 to Enabled by Default; however, this schedule is subject to change.\\nStatus planned to change in September 2024 to Generally Enabled; however, this schedule is subject to change.\\n\\nFor detailed descriptions of each change, grouped by functional area, see:\\n2024_06 Bundle (Disabled by Default)\\n\\n\\n\\n\\n\\n\\n\\nRecently Implemented Changes¶\\nThe following table lists behavior changes that were previously pending, but have been implemented in a recent release. If the change is in a monthly behavior change bundle that is currently enabled by default, the bundle can be disabled.\\nTo disable a bundle that is currently enabled by default, use the SYSTEM$DISABLE_BEHAVIOR_CHANGE_BUNDLE\\nsystem function. This is typically done in your production accounts to opt-out of the changes in the bundle while you continue testing the changes in your non-production accounts.\\n\\n\\n\\n\\n\\n\\n\\n\\nBundle\\nStatus / History\\nImplemented Changes\\nAdditional Notes\\n\\n\\n\\n2024_05\\nIntroduced in the 8.22 release (June 11-15) as Disabled by Default; account admins can enable for testing.\\nStatus changed in the 8.27 release (July 22-25) to Enabled by Default; account admins can disable for opt-out.\\nStatus planned to change in August 2024 to Generally Enabled; however, this schedule is subject to change.\\n\\nFor detailed descriptions of each change, grouped by functional area, see:\\n2024_05 Bundle (Enabled by Default)\\n\\n\\n\\n2024_04\\nIntroduced in the 8.17 release (April 30 - May 7) as Disabled by Default; account admins can enable for testing.\\nStatus changed in the 8.22 release (June 11-15) to Enabled by Default; account admins can disable for opt-out.\\nStatus changed in the 8.27 release (July 22-25) to Generally Enabled; account admins can no longer enable or disable\\nthis bundle.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2024_04 Bundle (Generally Enabled)\\n\\n\\n\\n2024_03\\nIntroduced in the 8.12 release (March 26-27) as Disabled by Default; account admins can enable for testing.\\nStatus changed in the 8.17 release (April 30 - May 7) to Enabled by Default; account admins can disable for opt-out.\\nStatus changed in the 8.22 release (June 11-15) to Generally Enabled; account admins can no longer enable or disable this bundle.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2024_03 Bundle (Generally Enabled)\\n\\n\\n\\n2024_02\\nIntroduced in the 8.7 release (February 19-21) as Disabled by Default; account admins can enable for testing.\\nStatus changed in the 8.12 release (March 26-27) to Enabled by Default; account admins can disable for opt-out.\\nStatus changed in the 8.17 release (April 30 - May 7) to Generally Enabled; account admins can no longer enable or disable this bundle.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2024_02 Bundle (Generally Enabled)\\n\\n\\n\\n2024_01\\nIntroduced,  Disabled by Default, in the 8.2 release (scheduled for January 15-17); account admins can enable for testing.\\nStatus changed in the 8.7 release (February 19-21) to Enabled by Default; account admins can disable for opt-out.\\nStatus changed in then 8.12 release (March 26-27) to Generally Enabled; account admins can no longer enable or disable this bundle.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2024_01 Bundle (Generally Enabled)\\n\\n\\n\\n2023_08\\nIntroduced, Disabled by default, in the 7.41 release (November 11-14, 2023); account admins can enable for testing.\\nStatus changed in the 8.2 release (January 15-17, 2024) to Enabled by Default; account admins can disable for opt-out.\\nStatus changed in the 8.7 release (February 19-21, 2024) to Generally Enabled; account admins can no longer enable or disable this bundle.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_08 Bundle (Generally Enabled)\\n\\n\\n\\n2023_07\\nIntroduced, Disabled by default, in the 7.34 release (September 27-28); account admins can enable for testing.\\nStatus changed to Enabled by Default in the 7.41 (November 11-14, 2023); account admins can disable for opt-out.\\nStatus changed in the 8.2 release (January 15-17, 2024) to Generally Enabled; account admins can no longer enable or disable this bundle.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_07 Bundle (Generally Enabled)\\n\\n\\n\\n2023_06\\nIntroduced, disabled by default, in the 7.29 release (August 22-23); account admins can enable for testing.\\nStatus changed to Enabled by Default in the 7.34 release (September 25-26); account admins can disable for out-out.\\nStatus changed to Generally Enabled in the 7.41 release (November 11-14, 2023). Account admins can no longer enable or disable.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_06 Bundle (Generally Enabled)\\n\\n\\n\\n2023_05\\nIntroduced, disabled by default, in the 7.23 release (July 10-11).\\nStatus changed in the 7.29 release (August 22-23) to Enabled by Default; account admins can disable for opt-out.\\nStatus changed to Generally Enabled in the 7.34 release (September 25-26). Account admins can no longer enable or disable.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_05 Bundle (Generally Enabled)\\n\\n\\n\\n2023_04\\nIntroduced, disabled by default, in the 7.19 release (Jun 7-8); account admins can enable for testing.\\nEnabled by default in the 7.23 release (July 10-11); account admins can disable to opt-out.\\nGenerally enabled in the 7.29 release (August 22-23). Account admins can no longer enable or disable.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_04 Bundle (Generally Enabled)\\n\\n\\n\\n2023_03\\nIntroduced, disabled by default, in the 7.13 release (Apr 20-24); account admins can enable for testing.\\nEnabled by default in the 7.19 release (Jun 7-8); account admins can disable to opt-out.\\nGenerally enabled in the 7.23 release (July 10-11). Account admins can no longer enable or disable.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_03 Bundle (Generally Enabled)\\n\\n\\n\\n2023_02\\nIntroduced, disabled by default, in the 7.7 release (Mar 6-7).\\nEnabled by default in the 7.13 release (Apr 20-23).\\nGenerally enabled in the 7.19 release (Jun 7-8); Account admins can no longer enable or disable.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_02 Bundle (Generally Enabled)\\n\\n\\n\\n2023_01\\nIntroduced, disabled by default, in the 7.2 release (Jan 19-20).\\nEnabled by default in the 7.7 release (Mar 6-7).\\nGenerally enabled in the 7.13 release (Apr 20-24); Account admins can no longer enable or disable.\\n\\nFor detailed descriptions of each change, grouped by functional area, refer to:\\n2023_01 Bundle (Generally Enabled)\\n\\n\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageUpcoming Pending ChangesRecently Implemented ChangesRelated contentWhat’s NewUnbundled Behavior ChangesBehavior Change PolicyDeprecated FeaturesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer-guide/drivers', 'title': 'Drivers | Snowflake Documentation', 'language': 'en'}, page_content='Drivers | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIMachine LearningSnowflake MLSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewGoJDBC.NETNode.jsODBCPHPPythonReferenceAPI ReferenceDeveloperOverview\\n\\nDrivers¶\\n\\n\\nUsing languages such as Go, C#, and Python, you can write applications that perform operations on Snowflake. Use the drivers described in\\nthis section to access Snowflake from applications written in the driver’s supported language.\\n\\nGo Snowflake DriverConnect to Snowflake and perform all standard operations with an interface for developing applications using the Go programming language.\\n\\nJDBC DriverConnect to Snowflake from most client tools/applications that support JDBC.\\n\\n.NET DriverConnect to Snowflake with an interface to the Microsoft .NET open source software framework for developing applications.\\n\\nNode.js DriverConnect to Snowflake with a native asynchronous Node.js interface.\\n\\nODBC DriverConnect to Snowflake using ODBC-based client applications.\\n\\nPHP PDO Driver for SnowflakeConnect to Snowflake and perform all standard operations with an interface for developing PHP applications.\\n\\nSnowflake Connector for PythonDevelop Python applications that can connect to Snowflake and perform all standard operations.\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related info\\n\\nDeveloper Resources\\n\\nLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://other-docs.snowflake.com/en/polaris/overview', 'title': 'Polaris Catalog overview | Snowflake Documentation', 'language': 'en'}, page_content='Polaris Catalog overview | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusGetting startedOverviewAccess controlTutorialAccessing PolarisSign up: Snowflake customerSign up: Not a Snowflake customerSign in: Snowflake customerSign in: Polaris Catalog customerSecuring PolarisEnroll in MFASetting up catalogsCreate external cloud storage for a catalogCreate a catalogConfigure and remove a service connectionRegister a service connectionManaging catalogsOrganize catalog contentSecure catalogsQuerying data in catalogsQuery a table in Polaris Using SnowflakeCreate a Snowflake-managed Iceberg table that you sync to Polaris CatalogQuery a table in Polaris Catalog using a third-party engineCode examplesApache SparkReleasesRelease notesPolaris CatalogOverview\\n\\nPolaris Catalog overview¶\\n\\nPREVIEW\\nAvailable to all accounts in all Snowflake regions except government regions.\\n\\nPolaris Catalog is a catalog implementation for Apache Iceberg™ tables and is built on the open source Apache Iceberg™ REST protocol.\\nWith Polaris Catalog, you can provide centralized, secure read and write access to your Iceberg tables across different REST-compatible query engines.\\nPolaris Catalog is currently offered as a service hosted in Snowflake-managed infrastructure.\\n\\n\\nSigning up¶\\nPolaris Catalog offers the following signup options:\\n\\nFor existing Snowflake customers: Sign in to an existing Snowflake account as an organization administrator and create a new Polaris\\nCatalog account in your Snowflake organization. Users with the ORGADMIN role in Snowflake can manage the Polaris Catalog account from\\nSnowflake. For instructions, see Sign up: Snowflake customer.\\nIf you are not an existing Snowflake customer: Create a Polaris Catalog account if you do not have an existing Snowflake account. You\\nmight choose this option if you currently use a Snowflake Free Trial and want to explore Polaris Catalog, or if you’re interested in\\nexploring Polaris without using Snowflake. For instructions, see Sign up: Not a Snowflake customer.\\n\\n\\n\\nKey concepts¶\\nThis section introduces key concepts associated with using Polaris Catalog hosted in Snowflake.\\nIn the following diagram, a sample Polaris Catalog structure with nested namespaces is shown for Catalog1. No tables\\nor namespaces have been created yet for Catalog2 or Catalog3.\\n\\n\\nCatalog¶\\nIn Polaris Catalog, you can create one or more catalog resources to organize Iceberg tables.\\nConfigure your catalog by setting values in the storage configuration for S3, Azure, or Google Cloud Storage. An Iceberg catalog enables a\\nquery engine to manage and organize tables. The catalog forms the first architectural layer in the Apache Iceberg™ table specification and must support the following tasks:\\n\\nStoring the current metadata pointer for one or more Iceberg tables. A metadata pointer maps a table name to the location of that table’s\\ncurrent metadata file.\\nPerforming atomic operations so that you can update the current metadata pointer for a table to the metadata pointer of a new version of\\nthe table.\\n\\nTo learn more about Iceberg catalogs, see the Apache Iceberg™ documentation.\\n\\nCatalog types¶\\nA catalog can be one of the following two types:\\n\\nInternal: The catalog is managed by Polaris. Tables from this catalog can be read and written in Polaris.\\nExternal: The catalog is externally managed by another Iceberg catalog provider (for example, Snowflake, Glue, Dremio Arctic). Tables from\\nthis catalog are synced to Polaris. These tables are read-only in Polaris. In the current release, only a Snowflake external catalog is provided.\\n\\nA catalog is configured with a storage configuration that can point to S3, Azure storage, or GCS.\\nTo create a new catalog, see Create a catalog.\\n\\n\\n\\nNamespace¶\\nYou create namespaces to logically group Iceberg tables within a catalog. A catalog can have multiple namespaces. You can also create\\nnested namespaces. Iceberg tables belong to namespaces.\\n\\n\\nApache Iceberg™ tables and catalogs¶\\nIn an internal catalog, an Iceberg table is registered in Polaris Catalog, but read and written via query engines. The table data and\\nmetadata is stored in your external cloud storage. The table uses Polaris Catalog as the Iceberg catalog.\\nIf you have tables that use Snowflake as the Iceberg catalog (Snowflake-managed tables), you can sync these tables to an external\\ncatalog in Polaris Catalog. If you sync this catalog to Polaris Catalog, it appears as an external catalog in Polaris Catalog. The table data and\\nmetadata is stored in your external cloud storage. The Snowflake query engine can read from or write to these tables. However, the other query\\nengines can only read from these tables.\\nImportant\\n\\nFor the access privileges defined for a catalog to be enforced correctly, the following conditions must be met:\\n\\n\\n\\nThe directory only contains the data files that belong to a\\nsingle table.\\n\\n\\n\\n\\nThe directory hierarchy matches the namespace hierarchy for the catalog.\\n\\n\\n\\nFor example, if a catalog includes the following items:\\n\\n\\n\\nTop-level namespace namespace1\\n\\n\\n\\n\\nNested namespace namespace1a\\n\\n\\n\\n\\nA customers table, which is grouped under nested namespace namespace1a\\n\\n\\n\\n\\nAn orders table, which is grouped under nested namespace namespace1a\\n\\n\\n\\nThe directory hierarchy for the catalog must follow this structure:\\n\\n\\n\\n/namespace1/namespace1a/customers/<files for the customers table only>\\n\\n\\n\\n\\n/namespace1/namespace1a/orders/<files for the orders table only>\\n\\n\\n\\n\\nService principal¶\\nA service principal is an entity that you create in Polaris Catalog. Each service principal encapsulates credentials that you use to connect\\nto Polaris Catalog.\\nQuery engines use service principals to connect to catalogs.\\nPolaris Catalog generates a Client ID and Client Secret pair for each service principal.\\nThe following table displays example service principals that you might create in Polaris Catalog:\\n\\n\\nService connection name\\nPurpose\\n\\n\\n\\nFlink ingestion\\nFor Apache Flink® to ingest streaming data into Apache Iceberg™ tables.\\n\\nSpark ETL pipeline\\nFor Apache Spark™ to run ETL pipeline jobs on Iceberg tables.\\n\\nSnowflake data pipelines\\nFor Snowflake to run data pipelines for transforming data in Apache Iceberg™ tables.\\n\\nTrino BI dashboard\\nFor Trino to run BI queries for powering a dashboard.\\n\\nSnowflake AI team\\nFor Snowflake to run AI jobs on data in Apache Iceberg™ tables.\\n\\n\\n\\n\\n\\nService connection¶\\nA service connection represents a REST-compatible engine (such as Apache Spark™, Apache Flink®, or Trino) that can read from and write to Polaris\\nCatalog. When creating a new service connection, the Polaris administrator grants the service principal that is created with the new service\\nconnection either a new or existing principal role. A principal role is a resource in Polaris that you can use to logically group Polaris\\nservice principals together and grant privileges on securable objects. For more information, see Principal role. Polaris Catalog uses a role-based access control (RBAC) model to grant service principals access to resources. For more information,\\nsee Access control. For a diagram of this model, see RBAC model.\\nIf the Polaris administrator grants the service principal for the new service connection a new principal role, the service principal\\ndoesn’t have any privileges granted to it yet. When securing the catalog that the new service connection will connect to, the Polaris\\nadministrator grants privileges to catalog roles and then grants these catalog roles to the new principal role. As a result, the service\\nprincipal for the new service connection has these privileges. For more information about catalog roles, see Catalog role.\\nIf the Polaris administrator grants an existing principal role to the service principal for the new service connection, the service principal\\nis bestowed with the privileges granted to the catalog roles that are granted to the existing principal role. If needed, the Polaris\\nadministrator can grant additional catalog roles to the existing principal role or remove catalog roles from it to adjust the privileges\\nbestowed to the service principal. For an example of how RBAC works in Polaris, see RBAC example.\\n\\n\\nStorage configuration¶\\nA storage configuration stores a generated identity and access management (IAM) entity for your external cloud storage and is created\\nwhen you create a catalog. The storage configuration is used to set the values to connect Polaris Catalog to your cloud storage. During the\\ncatalog creation process, an IAM entity is generated and used to create a trust relationship between the cloud storage provider and Polaris\\nCatalog.\\nWhen you create a catalog, you supply the following information about your external cloud storage:\\n\\n\\nCloud storage provider\\nInformation\\n\\n\\n\\nAmazon S3\\nDefault base location for your Amazon S3 bucketLocations for your Amazon S3 bucketS3 role ARNExternal ID (optional)\\n\\nGoogle Cloud Storage (GCS)\\nDefault base location for your GCS bucketLocations for your GCS bucket\\n\\nAzure\\nDefault base location for your Microsoft Azure containerLocations for your Microsoft Azure containerAzure tenant ID\\n\\n\\n\\n\\n\\n\\nExample workflow¶\\nIn the following example workflow, Bob creates an Apache Iceberg™ table named Table1 and Alice reads data from Table1.\\n\\nBob uses Apache Spark™ to create the Table1 table under the Namespace1 namespace in the Catalog1 catalog and insert values into Table1.\\nBob can create Table1 and insert data into it because he is using a service connection with a service principal that has\\nthe privileges to perform these actions.\\n\\nAlice uses Snowflake to read data from Table1.\\nAlice can read data from Table1 because she is using a service connection with a service principal with a catalog integration that\\nhas the privileges to perform this action. Alice creates an unmanaged table in Snowflake to read data from Table1.\\n\\n\\n\\n\\n\\nSecurity and access control¶\\nThis section describes security and access control.\\n\\nCredential vending¶\\nTo secure interactions with service connections, Polaris Catalog vends temporary storage credentials to the query engine during query\\nexecution. These credentials allow the query engine to run the query without requiring access to your external cloud storage for\\nIceberg tables. This process is called credential vending.\\n\\n\\nIdentity and access management (IAM)¶\\nPolaris Catalog uses the identity and access management (IAM) entity to securely connect to your storage for accessing table data, Iceberg\\nmetadata, and manifest files that store the table schema, partitions, and other metadata. Polaris Catalog retains the IAM entity for your\\nstorage location.\\n\\n\\nAccess control¶\\nPolaris Catalog enforces the access control that you configure across all tables registered with the service and governs security for all\\nqueries from query engines in a consistent manner.\\nPolaris uses a role-based access control (RBAC) model that lets you centrally configure access for Polaris service principals to catalogs,\\nnamespaces, and tables.\\nPolaris RBAC uses two different role types to delegate privileges:\\n\\nPrincipal roles: Granted to Polaris service principals and\\nanalogous to roles in other access control systems that you grant to service principals.\\nCatalog roles: Configured with certain privileges on Polaris catalog resources and granted to principal roles.\\n\\nFor more information, see Access control.\\n\\n\\n\\nBilling¶\\nThere is no cost for using Polaris Catalog during the preview period.\\n\\n\\nLegal Notices¶\\nApache®, Apache Iceberg™, Apache Spark™, Apache Flink®, and Flink® are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Supplemental DocumentationAdditional terms of use may apply to features listed on this page.On this pageSigning upKey conceptsExample workflowSecurity and access controlBillingLegal NoticesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/tutorials/snowflake-in-20minutes', 'title': 'Snowflake in 20 minutes | Snowflake Documentation', 'language': 'en'}, page_content='Snowflake in 20 minutes | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusSnowflake in 20 minutes Getting StartedTutorialsSnowflake in 20 Minutes\\n\\nSnowflake in 20 minutes¶\\n\\nIntroduction¶\\nThis tutorial uses the Snowflake command line client, SnowSQL, to introduce key concepts and tasks, including:\\n\\nCreating Snowflake objects—You create a database and a table for storing data.\\nLoading data—We provide small sample CSV data files for you to load into the table.\\nQuerying—You explore sample queries.\\n\\n\\nNote\\nSnowflake bills a minimal amount for the on-disk storage used for the sample data in\\nthis tutorial. The tutorial provides steps to drop the database and minimize storage\\ncost.\\nSnowflake requires a virtual warehouse to load the\\ndata and execute queries. A running virtual warehouse consumes Snowflake credits.\\nIn this tutorial, you will be using a 30-day trial account,\\nwhich provides free credits, so you won’t incur any costs.\\n\\n\\nWhat you’ll learn¶\\nIn this tutorial you’ll learn how to:\\n\\nCreate Snowflake objects—You create a database and a table for storing data.\\nInstall SnowSQL—You install and use SnowSQL, the Snowflake command line query tool.\\nUsers of Visual Studio Code might consider using the Snowflake Extension for Visual Studio Code instead of SnowSQL.\\n\\nLoad CSV data files—You use various mechanisms to load data into tables from CSV files.\\nWrite and execute sample queries—You write and execute a variety of queries against newly loaded data.\\n\\n\\n\\n\\nPrerequisites¶\\nThis tutorial requires a database, table, and virtual warehouse to load and query data.\\nCreating these Snowflake objects requires a Snowflake user with a role with the\\nnecessary access control privileges. In addition, SnowSQL\\nis required to execute the SQL statements in the tutorial. Lastly, the tutorial requires CSV files that contain sample data to load.\\nYou can complete this tutorial using an existing Snowflake warehouse, database, and table, and your own local data files, but we recommend using the Snowflake objects and the set of\\nprovided data.\\nTo set up Snowflake for this tutorial, complete the following before continuing:\\n\\nCreate a user\\nTo create the database, table, and virtual warehouse, you must be logged in as a\\nSnowflake user with a role that grants you the privileges to create these objects.\\n\\nIf you’re using a 30-day trial account, you can log in as the user that was created for the account.\\nThis user has the role with the privileges needed to create the objects.\\nIf you don’t have a Snowflake user, you can’t perform this tutorial.\\nIf you don’t have a role that lets you create a user, ask someone who does to perform this step for you.\\nUsers with the ACCOUNTADMIN or SECURITYADMIN role can create users.\\n\\n\\nInstall SnowSQL\\nTo install SnowSQL, see Installing SnowSQL.\\n\\nDownload sample data files\\nFor this tutorial you download sample employee data files in CSV format that Snowflake provides.\\nTo download and unzip the sample data files:\\n\\nDownload the set of sample data files. Right-click the name of the archive\\nfile, getting-started.zip, and save the link/file to your local file system.\\nUnzip the sample files. The tutorial assumes you unpacked files into one of the following directories:\\n\\n\\n\\nLinux/macOS: /tmp\\nWindows: C:\\\\\\\\temp\\n\\n\\nEach file has five data records. The data uses a comma (,) character as field\\ndelimiter. The following is an example record:\\nAlthea,Featherstone,afeatherstona@sf_tuts.com,\"8172 Browning Street, Apt B\",Calatrava,7/12/2017\\n\\nCopy\\n\\n\\nThere are no blank spaces before or after the commas separating the\\nfields in each record. This is the default that Snowflake expects when loading CSV data.\\n\\n\\nLog in to SnowSQL¶\\nAfter you have SnowSQL, start SnowSQL to connect to Snowflake:\\n\\nOpen a command line window.\\nStart SnowSQL:\\n$ snowsql -a <account_identifier> -u <user_name>\\n\\nCopy\\nWhere:\\n\\n\\n\\n<account_identifier> is the unique identifier for your Snowflake account.The preferred format of the account identifier is as follows:\\n\\norganization_name-account_nameNames of your Snowflake organization and account. For more information, see Format 1 (preferred): Account name in your organization.\\n\\n\\n\\n\\n\\n<user_name> is the login name for your Snowflake user.\\n\\n\\n\\nNote\\nIf your account has an identity provider (IdP) that has been defined for your account, you can use a web browser to authenticate instead of a password, as the following example demonstrates:\\n$ snowsql -a <account_identifier> -u <user_name> --authenticator externalbrowser\\n\\nCopy\\n\\nFor more information, see Using a web browser for federated authentication/SSO.\\n\\nWhen SnowSQL prompts you, enter the password for your Snowflake user.\\n\\nIf you log in successfully, SnowSQL displays a command prompt that includes\\nyour current warehouse, database, and schema.\\n\\nNote\\nIf you get locked out of the account and can’t obtain the account identifier, you can find it in the Welcome email that Snowflake sent to\\nyou when you signed up for the trial account, or you can work with your\\nORGADMIN to get the account details.\\nYou can also find the values for locator, cloud, and region\\nin the Welcome email.\\n\\nIf your Snowflake user doesn’t have a default warehouse, database, and schema, or if\\nyou didn’t configure SnowSQL to specify a default warehouse, database, and schema,\\nthe prompt displays no warehouse, no database, and no schema. For example:\\nuser-name#(no warehouse)@(no database).(no schema)>\\n\\nCopy\\nThis prompt indicates that there is no warehouse, database, and schema\\nselected for the current session. You create these objects\\nin the next step. As you follow the next steps in this tutorial to create\\nthese objects, the prompt automatically updates to include the names of these objects.\\nFor more information, see Connecting through SnowSQL.\\n\\n\\nCreate Snowflake objects¶\\nDuring this step you create the following Snowflake objects:\\n\\nA database (sf_tuts) and a table (emp_basic). You load sample data into this table.\\nA virtual warehouse (sf_tuts_wh).\\nThis warehouse provides the compute resources needed to load data into\\nthe table and query the table. For this tutorial, you create an X-Small warehouse.\\n\\nAt the completion of this tutorial, you will remove these objects.\\n\\nCreate a database¶\\nCreate the sf_tuts database using the CREATE DATABASE command:\\nCREATE OR REPLACE DATABASE sf_tuts;\\n\\nCopy\\nIn this tutorial, you use the default schema (public) available for each database, rather than creating a new schema.\\nNote that the database and schema you just created are now in use for your current\\nsession, as reflected in the SnowSQL command prompt. You can also use the context\\nfunctions to get this information.\\nSELECT CURRENT_DATABASE(), CURRENT_SCHEMA();\\n\\nCopy\\nThe following is an example result:\\n+--------------------+------------------+\\n| CURRENT_DATABASE() | CURRENT_SCHEMA() |\\n|--------------------+------------------|\\n| SF_TUTS            | PUBLIC           |\\n+--------------------+------------------+\\n\\n\\n\\n\\nCreate a table¶\\nCreate a table named emp_basic in sf_tuts.public using the CREATE TABLE command:\\nCREATE OR REPLACE TABLE emp_basic (\\n   first_name STRING ,\\n   last_name STRING ,\\n   email STRING ,\\n   streetaddress STRING ,\\n   city STRING ,\\n   start_date DATE\\n   );\\n\\nCopy\\nNote that the number of columns in the table, their positions, and their data types correspond to the fields in the sample CSV data files that you stage in the next step in this tutorial.\\n\\n\\nCreate a virtual warehouse¶\\nCreate an X-Small warehouse named sf_tuts_wh using the CREATE WAREHOUSE command:\\nCREATE OR REPLACE WAREHOUSE sf_tuts_wh WITH\\n   WAREHOUSE_SIZE=\\'X-SMALL\\'\\n   AUTO_SUSPEND = 180\\n   AUTO_RESUME = TRUE\\n   INITIALLY_SUSPENDED=TRUE;\\n\\nCopy\\nThe sf_tuts_wh warehouse is initially suspended, but the DML statement also sets\\nAUTO_RESUME = true. The AUTO_RESUME setting causes a warehouse to automatically start\\nwhen SQL statements that require compute resources are executed.\\nAfter you create the warehouse, it’s now in use for your current session.\\nThis information is displayed in your SnowSQL command prompt. You can also retrieve\\nthe name of the warehouse by using the following context function:\\nSELECT CURRENT_WAREHOUSE();\\n\\nCopy\\nThe following is an example result:\\n+---------------------+\\n| CURRENT_WAREHOUSE() |\\n|---------------------|\\n| SF_TUTS_WH          |\\n+---------------------+\\n\\n\\n\\n\\n\\nStage data files¶\\nA Snowflake stage is a location in cloud storage that you use to load and\\nunload data from a table. Snowflake supports the following types of stages:\\n\\nInternal stages—Used to store data files internally within Snowflake. Each user and table in Snowflake gets an internal stage by default for staging data files.\\nExternal stages—Used to store data files externally in Amazon S3, Google Cloud Storage, or Microsoft Azure.\\nIf your data is already stored in these cloud storage services, you can use an external stage to load data in Snowflake tables.\\n\\nIn this tutorial, we upload the sample data files\\n(downloaded in Prerequisites)\\nto the internal stage for the emp_basic table that you created earlier. You use the PUT command\\nto upload the sample data files to that stage.\\n\\nStaging sample data files¶\\nExecute the PUT command in SnowSQL to upload local data files to the table stage\\nprovided for the emp_basic table you created.\\nPUT file://<file-path>[/\\\\]employees0*.csv @sf_tuts.public.%emp_basic;\\n\\nCopy\\nFor example:\\n\\nLinux or macOS\\nPUT file:///tmp/employees0*.csv @sf_tuts.public.%emp_basic;\\n\\nCopy\\n\\nWindows\\nPUT file://C:\\\\temp\\\\employees0*.csv @sf_tuts.public.%emp_basic;\\n\\nCopy\\n\\n\\nLet’s take a closer look at the command:\\n\\nfile://<file-path>[/]employees0*.csv specifies the full directory path and\\nnames of the files on your local machine to stage. Note that file system wildcards are allowed, and if multiple files fit the pattern they are all displayed.\\n@<namespace>.%<table_name> indicates to use the stage for the specified table, in this case the emp_basic table.\\n\\nThe command returns the following result, showing the staged files:\\n+-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------+\\n| source          | target             | source_size | target_size | source_compression | target_compression | status   | message |\\n|-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------|\\n| employees01.csv | employees01.csv.gz |         360 |         287 | NONE               | GZIP               | UPLOADED |         |\\n| employees02.csv | employees02.csv.gz |         355 |         274 | NONE               | GZIP               | UPLOADED |         |\\n| employees03.csv | employees03.csv.gz |         397 |         295 | NONE               | GZIP               | UPLOADED |         |\\n| employees04.csv | employees04.csv.gz |         366 |         288 | NONE               | GZIP               | UPLOADED |         |\\n| employees05.csv | employees05.csv.gz |         394 |         299 | NONE               | GZIP               | UPLOADED |         |\\n+-----------------+--------------------+-------------+-------------+--------------------+--------------------+----------+---------+\\n\\n\\nThe PUT command compresses files by default using gzip, as indicated in the TARGET_COMPRESSION column.\\n\\n\\nListing the staged files (Optional)¶\\nYou can list the staged files using the LIST command.\\nLIST @sf_tuts.public.%emp_basic;\\n\\nCopy\\nThe following is an example result:\\n+--------------------+------+----------------------------------+------------------------------+\\n| name               | size | md5                              | last_modified                |\\n|--------------------+------+----------------------------------+------------------------------|\\n| employees01.csv.gz |  288 | a851f2cc56138b0cd16cb603a97e74b1 | Tue, 9 Jan 2018 15:31:44 GMT |\\n| employees02.csv.gz |  288 | 125f5645ea500b0fde0cdd5f54029db9 | Tue, 9 Jan 2018 15:31:44 GMT |\\n| employees03.csv.gz |  304 | eafee33d3e62f079a054260503ddb921 | Tue, 9 Jan 2018 15:31:45 GMT |\\n| employees04.csv.gz |  304 | 9984ab077684fbcec93ae37479fa2f4d | Tue, 9 Jan 2018 15:31:44 GMT |\\n| employees05.csv.gz |  304 | 8ad4dc63a095332e158786cb6e8532d0 | Tue, 9 Jan 2018 15:31:44 GMT |\\n+--------------------+------+----------------------------------+------------------------------+\\n\\n\\n\\n\\n\\nCopy data into target tables¶\\nTo load your staged data into the target table, execute COPY INTO <table>.\\nThe COPY INTO <table> command uses the virtual warehouse you created\\nin Create Snowflake objects to copy files.\\nCOPY INTO emp_basic\\n  FROM @%emp_basic\\n  FILE_FORMAT = (type = csv field_optionally_enclosed_by=\\'\"\\')\\n  PATTERN = \\'.*employees0[1-5].csv.gz\\'\\n  ON_ERROR = \\'skip_file\\';\\n\\nCopy\\nWhere:\\n\\nThe FROM clause specifies the location containing the data files (the internal stage for the table).\\nThe FILE_FORMAT clause specifies the file type as CSV, and specifies the double-quote\\ncharacter (\") as the character used to enclose strings. Snowflake supports\\ndiverse file types and options. These are described\\nin CREATE FILE FORMAT.\\nThe PATTERN clause specifies that the command should load data from the filenames matching\\nthis regular expression (.*employees0[1-5].csv.gz).\\nThe ON_ERROR clause specifies what to do when the COPY command encounters errors in the files. By default, the command stops loading data\\nwhen the first error is encountered. This example skips any file containing an error and moves on to loading\\nthe next file. (None of the files in this tutorial contain errors; this is included for illustration purposes.)\\n\\nThe COPY command also provides an option for validating files before they are loaded. For more information about additional error checking and validation instructions, see the COPY INTO <table> topic and the other data loading tutorials.\\nThe COPY command returns a result showing the list of files copied and related information:\\n+--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+\\n| file               | status | rows_parsed | rows_loaded | error_limit | errors_seen | first_error | first_error_line | first_error_character | first_error_column_name |\\n|--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------|\\n| employees02.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees04.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees05.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees03.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n| employees01.csv.gz | LOADED |           5 |           5 |           1 |           0 | NULL        |             NULL |                  NULL | NULL                    |\\n+--------------------+--------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------------+\\n\\n\\n\\n\\nQuery loaded data¶\\nYou can query the data loaded in the emp_basic table using standard SQL and any supported\\nfunctions and\\noperators.\\nYou can also manipulate the data, such as updating the loaded data or inserting more data, using standard DML commands.\\n\\nRetrieve all data¶\\nReturn all rows and columns from the table:\\nSELECT * FROM emp_basic;\\n\\nCopy\\nThe following is a partial result:\\n+------------+--------------+---------------------------+-----------------------------+--------------------+------------+\\n| FIRST_NAME | LAST_NAME    | EMAIL                     | STREETADDRESS               | CITY               | START_DATE |\\n|------------+--------------+---------------------------+-----------------------------+--------------------+------------|\\n| Arlene     | Davidovits   | adavidovitsk@sf_tuts.com  | 7571 New Castle Circle      | Meniko             | 2017-05-03 |\\n| Violette   | Shermore     | vshermorel@sf_tuts.com    | 899 Merchant Center         | Troitsk            | 2017-01-19 |\\n| Ron        | Mattys       | rmattysm@sf_tuts.com      | 423 Lien Pass               | Bayaguana          | 2017-11-15 |\\n ...\\n ...\\n ...\\n| Carson     | Bedder       | cbedderh@sf_tuts.co.au    | 71 Clyde Gallagher Place    | Leninskoye         | 2017-03-29 |\\n| Dana       | Avory        | davoryi@sf_tuts.com       | 2 Holy Cross Pass           | Wenlin             | 2017-05-11 |\\n| Ronny      | Talmadge     | rtalmadgej@sf_tuts.co.uk  | 588 Chinook Street          | Yawata             | 2017-06-02 |\\n+------------+--------------+---------------------------+-----------------------------+--------------------+------------+\\n\\nCopy\\n\\n\\nInsert additional data rows¶\\nIn addition to loading data from staged files into a table, you can insert rows directly into a table using the INSERT DML command.\\nFor example, to insert two additional rows into the table:\\nINSERT INTO emp_basic VALUES\\n   (\\'Clementine\\',\\'Adamou\\',\\'cadamou@sf_tuts.com\\',\\'10510 Sachs Road\\',\\'Klenak\\',\\'2017-9-22\\') ,\\n   (\\'Marlowe\\',\\'De Anesy\\',\\'madamouc@sf_tuts.co.uk\\',\\'36768 Northfield Plaza\\',\\'Fangshan\\',\\'2017-1-26\\');\\n\\nCopy\\n\\n\\nQuery rows based on email address¶\\nReturn a list of email addresses with United Kingdom top-level domains using the [ NOT ] LIKE function:\\nSELECT email FROM emp_basic WHERE email LIKE \\'%.uk\\';\\n\\nCopy\\nThe following is an example result:\\n+--------------------------+\\n| EMAIL                    |\\n|--------------------------|\\n| gbassfordo@sf_tuts.co.uk |\\n| rtalmadgej@sf_tuts.co.uk |\\n| madamouc@sf_tuts.co.uk   |\\n+--------------------------+\\n\\n\\n\\n\\nQuery rows based on start date¶\\nFor example, to calculate when certain employee benefits might start, add 90 days to employee start\\ndates using the DATEADD function. Filter the list by employees whose start date occurred earlier than January 1, 2017:\\nSELECT first_name, last_name, DATEADD(\\'day\\',90,start_date) FROM emp_basic WHERE start_date <= \\'2017-01-01\\';\\n\\nCopy\\nThe following is an example result:\\n+------------+-----------+------------------------------+\\n| FIRST_NAME | LAST_NAME | DATEADD(\\'DAY\\',90,START_DATE) |\\n|------------+-----------+------------------------------|\\n| Granger    | Bassford  | 2017-03-30                   |\\n| Catherin   | Devereu   | 2017-03-17                   |\\n| Cesar      | Hovie     | 2017-03-21                   |\\n| Wallis     | Sizey     | 2017-03-30                   |\\n+------------+-----------+------------------------------+\\n\\n\\n\\n\\n\\nSummary, clean up, and additional resources¶\\nCongratulations! You’ve successfully completed this introductory tutorial.\\nTake a few minutes to review a short summary and the key points covered in the tutorial.\\nYou might also want to consider cleaning up by dropping any objects you created in the tutorial.\\nLearn more by reviewing other topics in the Snowflake Documentation.\\n\\nSummary and key points¶\\nIn summary, data loading is performed in two steps:\\n\\nStage the data files to load. The files can be staged internally (in Snowflake) or in an external location. In this tutorial, you stage files internally.\\nCopy data from the staged files into an existing target table. A running\\nwarehouse is required for this step.\\n\\nRemember the following key points about loading CSV files:\\n\\nA CSV file consists of 1 or more records, with 1 or more fields in each record, and sometimes a header record.\\nRecords and fields in each file are separated by delimiters. The default delimiters are:\\n\\n\\nRecords:\\nnewline characters\\n\\nFields:\\ncommas\\n\\n\\n\\nIn other words, Snowflake expects each record in a CSV file to be separated by new lines and the fields (i.e. individual values) in each record to be separated by commas. If different\\ncharacters are used as record and field delimiters, you must explicitly specify this as part of the file format when loading.\\n\\nThere is a direct correlation between the fields in the files and the columns in the table you will be loading, in terms of:\\n\\n\\nNumber of fields (in the file) and columns (in the target table).\\nPositions of the fields and columns within their respective file/table.\\nData types, such as string, number, or date, for fields and columns.\\n\\n\\nThe records will not be loaded if the numbers, positions, and data types don’t align with the data.\\n\\nNote\\nSnowflake supports loading files in which the fields don’t exactly align with the columns in the target table;\\nhowever, this is a more advanced data loading topic (covered in\\nTransforming data during a load).\\n\\n\\n\\n\\n\\nTutorial cleanup (Optional)¶\\nIf the objects you created in this tutorial are no longer needed,\\nyou can remove them from the system with DROP <object> statements.\\nDROP DATABASE IF EXISTS sf_tuts;\\n\\nDROP WAREHOUSE IF EXISTS sf_tuts_wh;\\n\\nCopy\\n\\n\\nExit the connection¶\\nTo exit a connection, use the !exit command for SnowSQL (or its alias, !disconnect).\\nExit drops the current connection and quits SnowSQL if it is the last connection.\\n\\n\\nWhat’s next?¶\\nContinue learning about Snowflake using the following resources:\\n\\nComplete the other tutorials provided by Snowflake:\\n\\nSnowflake Tutorials\\n\\n\\nFamiliarize yourself with key Snowflake concepts and features, as well as the SQL commands to perform queries and insert/update data:\\n\\nIntroduction to Snowflake\\nQuery syntax\\nData Manipulation Language (DML) commands\\n\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/sql-reference-commands', 'title': 'SQL command reference | Snowflake Documentation', 'language': 'en'}, page_content='SQL command reference | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSQL data types referenceSQL command referenceQuery syntaxQuery operatorsGeneral DDLGeneral DMLAll commands (alphabetical)AccountsUsers, roles, & privilegesIntegrationsReplication & failoverSessionsTransactionsVirtual warehouses & resource monitorsDatabases, schemas, & sharesTables, views, & sequencesFunctions, procedures, & scriptingStreams & tasksClasses & instancesMachine learning modelsCortex Search ServiceListingsSecurityData GovernancePrivacyData loading & unloadingFile stagingAlertsNative Apps FrameworkStreamlitNotebookSnowpark Container ServicesFunction and stored procedure referenceClass referenceScripting referenceGeneral referenceAPI referenceReferenceSQL command reference\\n\\nSQL command reference¶\\n\\n\\nThese topics provide reference information for all the Snowflake SQL commands (DDL, DML, and query syntax).\\n\\nQuery syntax — structure of SQL queries in Snowflake.\\nQuery operators — arithmetic, logical, and other types of operators.\\nData Definition Language (DDL) commands — overview of DDL commands.\\nData Manipulation Language (DML) commands — commands for performing DML operations, including:\\n\\nInserting, deleting, updating, and merging data in Snowflake tables.\\nBulk copying data into and out of Snowflake tables.\\nStaging files for bulk copying.\\n\\n\\nAll commands (alphabetical) — alphabetical list of all the commands.\\nCommands categorized by the type of objects and operations they control, including:\\n\\nGeneral account-level objects (accounts, users, roles, securty policies, integrations, etc.) and operations (failover & recovery, etc.).\\nSession-based operations (session context, queries, variables, transactions, etc.).\\nVirtual warehouses (for loading data and performing queries) and resource monitors (for controlling credit usage).\\nDatabases, schemas, tables, and other schema-level objects (views, sequences, etc.).\\nSnowflake extensions and application development (user-defined functions, stored procedures, scripting, etc.).\\nObjects for sharing data (shares, listings, etc.).\\nObjects for classifying, protecting, and governing data (masking polcies, row-access polcies, tags, etc.).\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/release-notes/clients-drivers/monthly-releases', 'title': 'Snowflake connector, driver, and library monthly releases | Snowflake Documentation', 'language': 'en'}, page_content=\"Snowflake connector, driver, and library monthly releases | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusWhat's NewBehavior ChangesDeprecated FeaturesPreview FeaturesPerformance ImprovementsSQL ImprovementsClients, Drivers, and LibrariesMonthly Releases.NET DriverGo Snowflake DriverIngest Java SDKJDBC DriverNode.js DriverODBC DriverPHP PDO Driver for SnowflakeSnowflake CLISnowflake Connector for KafkaSnowflake Connector for PythonSnowflake Connector for SparkSnowflake API for PythonSnowpark Library for PythonSnowpark Library for Scala and JavaSnowpark MLSnowSQLSQLAlchemyClient versions & support policySnowflake ConnectorsSnowflake Connector for Google Analytics Raw DataSnowflake Connector for Google Analytics Aggregate DataSnowflake Connector for ServiceNowSnowflake Connector for ServiceNow V2Snowflake Connector for MySQLSnowflake Connector for PostgreSQLNative SDK for ConnectorsNative SDK for Connectors Java libraryNative SDK for Connectors Java Test libraryNative SDK for Connectors Java TemplateNative SDK Example Java GitHub ConnectorReleasesClients, Drivers, and LibrariesMonthly Releases\\n\\nSnowflake connector, driver, and library monthly releases¶\\nThis topic provides a monthly list of the connector, driver, and library releases and includes links to the release\\nnotes for each. For each client, the monthly table lists the version number and date of the latest release.\\nA TBD indicates that a new version has not yet been released for a client during the month, but does not preclude a\\nrelease later in the month. A TBD in a previous month indicates that a client did not release an update in that month.\\nSnowflake uses semantic versioning for client and driver updates.\\n\\nNote\\nStarting with January 2023, the BCR? column indicates whether a version contains a change that might break applications built on earlier versions.\\n\\nTo determine the latest minimum versions for the clients and drivers, refer to the Client versions & support policy topic.\\nFor release note information for versions released prior to January 2022, see the Client Release History.\\n\\nAugust 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n4.1.0\\n05-Aug-2024\\n\\n\\nGo Snowflake Driver\\nTBD\\nTBD\\n\\n\\nIngest Java SDK\\n2.2.0\\n09-Aug-2024\\n\\n\\nJDBC Driver\\nTBD\\nTBD\\n\\n\\nNode.js Driver\\n1.12.0\\n05-Aug-2024\\n\\n\\nODBC Driver\\nTBD\\nTBD\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake CLI\\n2.7.0\\n02-Aug-2024\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\nTBD\\nTBD\\n\\n\\nSnowSQL\\n1.3.2\\n12-Aug-2024\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nJuly 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n4.0.0\\n08-Jul-2024\\nY\\n\\nGo Snowflake Driver\\n1.11.0\\n31-Jul-2024\\n\\n\\nIngest Java SDK\\n2.1.2\\n29-Jul-2024\\n\\n\\nJDBC Driver\\n3.18.0\\n24-Jul-2024\\n\\n\\n\\n3.17.0\\n08-Jul-2024\\n\\n\\nNode.js Driver\\nTBD\\nTBD\\n\\n\\nODBC Driver\\n3.4.0\\n29-Jul-2024\\n\\n\\nPHP PDO Driver for Snowflake\\n3.0.1\\n24-Jul-2024\\n\\n\\nSnowflake CLI\\n2.6.1\\n15-Jul-2024\\n\\n\\n\\n2.6.0\\n11-Jul-2024\\n\\n\\nSnowflake Connector for Kafka\\n2.3.0\\n10-Jul-2024\\n\\n\\nSnowflake Connector for Python\\n3.12.0\\n26-Jul-2024\\n\\n\\nSnowflake Connector for Spark\\n3.0.0\\n31-Jul-2024\\n\\n\\nSnowflake API for Python\\n0.11.0\\n25-Jul-2024\\n\\n\\nSnowpark Library for Python\\n1.20.0\\n17-Jul-2024\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.5.4\\n29-Jul-2024\\n✔\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\n1.6.1\\n09-Jul-2024\\n\\n\\n\\n1.6.0\\n08-Jul-2024\\n\\n\\n\\n\\n\\n\\nJune 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\n\\nGo Snowflake Driver\\nTBD\\nTBD\\n\\n\\nIngest Java SDK\\nTBD\\nTBD\\n\\n\\nJDBC Driver\\nTBD\\nTBD\\n\\n\\nNode.js Driver\\nTBD\\nTBD\\n\\n\\nODBC Driver\\n3.3.2\\n24-Jun-2024\\n\\n\\nPHP PDO Driver for Snowflake\\n3.0.0\\n18-Jun-2024\\nY\\n\\nSnowflake CLI\\n2.5.0\\n20-Jun-2024\\n\\n\\n\\n2.4.1\\n12-Jun-2024\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.11.0\\n18-Jun-2024\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\n0.10.0\\n24-Jun-2024\\n\\n\\n\\n0.9.0\\n10-Jun-2024\\n\\n\\nSnowpark Library for Python\\n1.19.0\\n25-Jun-2024\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.5.2\\n10-Jun-2024\\n\\n\\n\\n1.5.3\\n17-Jun-2024\\n\\n\\nSnowSQL\\n1.3.1\\n28-Jun-2024\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nMay 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\n\\nGo Snowflake Driver\\n1.10.1\\n29-May-2024\\n\\n\\n\\n1.10.0\\n08-May-2024\\n\\n\\nIngest Java SDK\\n2.1.1\\n09-May-2024\\n\\n\\nJDBC Driver\\n3.16.1\\n27-May-2024\\n\\n\\nNode.js Driver\\n1.11.0\\n28-May-2024\\n\\n\\nODBC Driver\\n3.3.1\\n03-May-2024\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake CLI\\n2.4.0\\n31-May-2024\\n\\n\\n\\n2.3.1\\n20-May-2024\\n\\n\\n\\n2.3.0\\n15-May-2024\\n\\n\\nSnowflake Connector for Kafka\\n2.2.2\\n07-May-2024\\n\\n\\nSnowflake Connector for Python\\n3.10.1\\n21-May-2024\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\n0.8.1\\n31-May-2024\\n\\n\\nSnowpark Library for Python\\n1.18.0\\n28-May-2024\\n\\n\\n\\n1.17.0\\n21-May-2024\\n\\n\\n\\n1.16.0\\n08-May-2024\\n\\n\\nSnowpark Library for Scala and Java\\n1.12.1\\n13-May-2024\\n\\n\\nSnowpark ML\\n1.5.1\\n22-May-2024\\n\\n\\n\\n1.5.0\\n01-May-2024\\n\\n\\nSnowSQL\\n1.3.0\\n02-May-2024\\nY\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nApril 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\n\\nGo Snowflake Driver\\nTBD\\nTBD\\n\\n\\nIngest Java SDK\\nTBD\\nTBD\\n\\n\\nJDBC Driver\\n3.16.0\\n29-Apr-1024\\n\\n\\n\\n3.15.1\\n05-Apr-1024\\n\\n\\nNode.js Driver\\n1.10.1\\n08-Apr-2024\\n\\n\\nODBC Driver\\n3.3.0\\n08-Apr-2024\\n\\n\\nPHP PDO Driver for Snowflake\\n2.0.3\\n29-Apr-2024\\n\\n\\nSnowflake CLI\\n2.2.0\\n25-Apr-2024\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.10.0\\n29-Apr-2024\\n\\n\\n\\n3.9.1\\n22-Apr-2024\\n\\n\\n\\n3.9.0\\n18-Apr-2024\\n\\n\\n\\n3.8.1\\n09-Apr-2024\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\n0.8.0\\n30-Apr-2024\\n\\n\\nSnowpark Library for Python\\n1.15.0\\n24-Apr-2024\\n\\n\\nSnowpark Library for Scala and Java\\n1.12.0\\n16-Apr-2024\\n\\n\\n\\n1.11.0\\n01-Apr-2024\\n\\n\\nSnowpark ML\\n1.4.0\\n08-Apr-2024\\nY\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\n1.5.3\\n16-Apr-2024\\n\\n\\n\\n1.5.2\\n11-Apr-2024\\n\\n\\n\\n\\n\\n\\nMarch 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n3.1.0\\n27-Mar-2024\\n\\n\\nGo Snowflake Driver\\n1.9.0\\n28-Mar-2024\\n\\n\\nIngest Java SDK\\nTBD\\nTBD\\n\\n\\nJDBC Driver\\nTBD\\nTBD\\n\\n\\nNode.js Driver\\nTBD\\nTBD\\n\\n\\nODBC Driver\\nTBD\\nTBD\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake CLI\\n2.1.2\\n27-Mar-2024\\n\\n\\n\\n2.1.1\\n20-Mar-2024\\n\\n\\nSnowflake Connector for Kafka\\n2.2.1\\n15-Mar-2024\\n\\n\\nSnowflake Connector for Python\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\n0.7.0\\n20-Mar-2024\\n\\n\\nSnowpark Library for Python\\n1.14.0\\n20-Mar-2024\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.3.1\\n21-Mar-2024\\nY\\n\\nSnowSQL\\n1.2.32\\n05-Mar-2024\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nFebruary 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n3.0.0\\n29-Feb-2024\\nY\\n\\nGo Snowflake Driver\\n1.8.0\\n21-Feb-2024\\n\\n\\nIngest Java SDK\\n2.1.0\\n28-Feb-2024\\nY\\n\\nJDBC Driver\\n3.15.0\\n20-Feb-2024\\n\\n\\nNode.js Driver\\n1.10.0\\n27-Feb-2024\\n\\n\\nODBC Driver\\nTBD\\nTBD\\n\\n\\nPHP PDO Driver for Snowflake\\n2.0.2\\n22-Feb-2024\\n\\n\\nSnowflake Connector for Kafka\\n2.2.0\\n06-Feb-2024\\nY\\n\\nSnowflake Connector for Python\\n3.7.1\\n22-Feb-2024\\n\\n\\nSnowflake Connector for Spark\\n2.15.0\\n26-Feb-2024\\n\\n\\nSnowflake API for Python\\n0.6.0\\n06-Feb-2024\\n\\n\\nSnowpark Library for Python\\n1.13.0\\n26-Feb-2024\\n\\n\\n\\n1.12.1\\n8-Feb-2024\\n\\n\\nSnowpark Library for Scala and Java\\n1.10.0\\n9-Feb-2024\\n\\n\\nSnowpark ML\\n1.2.3\\n26-Feb-2024\\n\\n\\n\\n1.2.2\\n13-Feb-2024\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nJanuary 2024 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.2.0\\n17-Jan-2024\\nY\\n\\nGo Snowflake Driver\\n1.7.2\\n17-Jan-2024\\n\\n\\nIngest Java SDK\\n2.0.5\\n22-Jan-2024\\n\\n\\nJDBC Driver\\n3.14.5\\n24-Jan-2024\\n\\n\\nNode.js Driver\\n1.9.3\\n17-Jan-2024\\n\\n\\nODBC Driver\\n3.2.0\\n19-Jan-2024\\nY\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.7.0\\n26-Jan-2024\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\n1.12.0\\n29-Jan-2024\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.2.1\\n25-Jan-2024\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nDecember 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.1.5\\n18-Dec-2023\\n\\n\\n\\n2.1.4\\n05-Dec-2023\\n\\n\\nGo Snowflake Driver\\n1.7.1\\n07-Dec-2023\\n\\n\\nJDBC Driver\\n3.14.4\\n07-Dec-2023\\n\\n\\nNode.js Driver\\n1.9.2\\n07-Dec-2023\\n\\n\\nODBC Driver\\n3.1.4\\n07-Dec-2023\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\n2.1.2\\n04-Dec-2023\\n\\n\\nSnowflake Connector for Python\\n3.6.0\\n07-Dec-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowflake API for Python\\n0.4.0\\n04-Dec-2023\\n\\n\\n\\n0.5.0\\n06-Dec-2023\\n\\n\\nSnowpark Library for Python\\n1.11.1\\n7-Dec-2023\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.1.2\\n18-Dec-2023\\n\\n\\nSnowSQL\\n1.2.31\\n13-Dec-2023\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nNovember 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.1.3\\n15-Nov-2023\\n\\n\\nGo Snowflake Driver\\n1.7.0\\n15-Nov-2023\\nY\\n\\nIngest Java SDK\\nTBD\\nTBD\\n\\n\\nJDBC Driver\\n3.14.3\\n07-Nov-2023\\n\\n\\nNode.js Driver\\n1.9.1\\n14-Nov-2023\\n\\n\\nODBC Driver\\n3.1.3\\n13-Nov-2023\\n\\n\\nPHP PDO Driver for Snowflake\\n2.0.1\\n09-Nov-2023\\nY\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.5.0\\n13-Nov-2023\\n\\n\\n\\n3.4.1\\n09-Nov-2023\\n\\n\\n\\n3.4.0\\n03-Nov-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.0.12\\n14-Nov-2023\\n\\n\\nSnowpark Library for Python\\n1.10.0\\n03-Nov-2023\\nY\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\n1.2.29\\n13-Nov-2023\\n\\n\\nSQLAlchemy\\n1.5.1\\n02-Nov-2023\\n\\n\\n\\n\\n\\n\\nOctober 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\n\\nGo Snowflake Driver\\nTBD\\nTBD\\n\\n\\nIngest Java SDK\\n2.0.4\\n31-Oct-2023\\n\\n\\nJDBC Driver\\n3.13.34\\n26-Oct-2023\\n\\n\\n\\n3.14.2\\n02-Oct-2023\\n\\n\\nNode.js Driver\\nTBD\\nTBD\\n\\n\\nODBC Driver\\nTBD\\nTBD\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.3.1\\n18-Oct-2023\\n\\n\\n\\n3.3.0\\n12-Oct-2023\\n\\n\\n\\n3.2.1\\n03-Oct-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\n1.9.0\\n16-Oct-2023\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\n1.2.29\\n10-Oct-2023\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nSeptember 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.1.2\\n27-Sep-2023\\n\\n\\nGo Snowflake Driver\\n1.6.25\\n26-Sep-2023\\n\\n\\nIngest Java SDK\\nTBD\\nTBD\\n\\n\\nJDBC Driver\\nTBD\\nTBD\\n\\n\\nNode.js Driver\\n1.9.0\\n28-Sep-2023\\nY\\n\\nODBC Driver\\n3.1.1\\n29-Sep-2023\\n\\n\\nPHP PDO Driver for Snowflake\\n2.0.0\\n29-Sep-2023\\nY\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.2.0\\n07-Sep-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.0.6\\n01-Sep-2023\\n\\n\\nSnowpark Library for Python\\n1.8.0\\n14-Sep-2023\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nAugust 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.1.1\\n23-Aug-2023\\n\\n\\nGo Snowflake Driver\\n1.6.24\\n22-Aug-2023\\n\\n\\nIngest Java SDK\\n2.0.3\\n31-Aug-2023\\n\\n\\nJDBC Driver\\n3.14.1\\n24-Aug-2023\\n\\n\\nNode.js Driver\\n1.8.0\\n29-Aug-2023\\n\\n\\nODBC Driver\\n3.1.0\\n23-Aug-2023\\nY\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\n2.0.1\\n25-Aug-2023\\n\\n\\nSnowflake Connector for Python\\n3.1.1\\n28-Aug-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark ML\\n1.0.5\\n17-Aug-2023\\nY\\n\\nSnowpark Library for Python\\n1.7.0\\n28-Aug-2023\\nY\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\n1.2.28\\n07-Aug-2023\\n\\n\\nSQLAlchemy\\n1.5.0\\n28-Aug-2023\\n\\n\\n\\n\\n\\n\\nJuly 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.1.0\\n27-Jul-2023\\nY\\n\\nGo Snowflake Driver\\n1.6.23\\n22-Jul-2023\\n\\n\\nIngest Java SDK\\n2.0.2\\n25-Jul-2023\\n\\n\\nJDBC Driver\\n3.14.0\\n27-Jul-2023\\nY\\n\\nNode.js Driver\\n1.7.0\\n28-Jul-2023\\n\\n\\nODBC Driver\\n3.0.1\\n06-Jul-2023\\nY\\n\\n\\n3.0.2\\n27-Jul-2023\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\n1.9.4\\n13-Jul-2023\\n\\n\\n\\n2.0.0\\n31-Jul-2023\\n\\n\\nSnowflake Connector for Python\\n3.1.0\\n31-Jul-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\n1.2.28\\n07-Aug-2023\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nJune 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.0.25\\n16-Jun-2023\\n\\n\\nGo Snowflake Driver\\n1.6.22\\n16-Jun-2023\\n\\n\\nIngest Java SDK\\n2.0.1\\n14-Jun-2023\\n\\n\\nJDBC Driver\\n3.13.33\\n14-Jun-2023\\n\\n\\nNode.js Driver\\n1.6.23\\n14-Jun-2023\\n\\n\\nODBC Driver\\n2.25.10\\n06-Jun-2023\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\n1.5.0\\n14-Jun-2023\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\n1.2.27\\n14-Jun-2023\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nMay 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.0.24\\n23-May-2023\\n\\n\\nGo Snowflake Driver\\n1.6.21\\n23-May-2023\\n\\n\\nJDBC Driver\\n3.13.31\\n25-May-2023\\n\\n\\n\\n3.13.32\\n26-May-2023\\n\\n\\nNode.js Driver\\n1.6.22\\n24-May-2023\\n\\n\\nODBC Driver\\nTBD\\nTBD\\n\\n\\nPHP PDO Driver for Snowflake\\n1.2.7\\n23-May-2023\\n\\n\\nSnowflake Connector for Kafka\\n1.9.3\\n21-May-2023\\n\\n\\nSnowflake Connector for Python\\n3.0.5\\n25-May-2023\\n\\n\\nSnowflake Connector for Spark\\nTBD\\n23-May-2023\\n\\n\\nSnowpark Library for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nApril 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.0.23\\n19-Apr-2023\\n\\n\\nGo Snowflake Driver\\n1.6.20\\n18-Apr-2023\\n\\n\\nJDBC Driver\\n3.13.30\\n18-Apr-2023\\n\\n\\nNode.js Driver\\n1.6.21\\n18-Apr-2023\\n\\n\\nODBC Driver\\n2.25.11\\n20-Apr-2023\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.0.3\\n20-Apr-2023\\n\\n\\nSnowflake Connector for Spark\\n2.11.3\\n21-Apr-2023\\n\\n\\nSnowpark Library for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nMarch 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.0.22\\n22-Mar-2023\\n\\n\\nGo Snowflake Driver\\n1.6.19\\n21-Mar-2023\\n\\n\\nJDBC Driver\\n3.13.29\\n17-Mar-2023\\n\\n\\nNode.js Driver\\n1.6.20\\n23-Mar-2023\\n\\n\\nODBC Driver\\n2.25.10\\n22-Mar-2023\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.0.1\\n01-Mar-2023\\n\\n\\n\\n3.0.2\\n23-Mar-2023\\n\\n\\nSnowflake Connector for Spark\\n2.11.2\\n21-Mar-2023\\n\\n\\nSnowpark Library for Python\\n1.2.0\\n2-Mar-2023\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\n1.4.7\\n21-Mar-2023\\n\\n\\n\\n\\n\\n\\nFebruary 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.0.21\\n22-Feb-2023\\n\\n\\nGo Snowflake Driver\\n1.6.18\\n22-Feb-2023\\n\\n\\nJDBC Driver\\n3.13.28\\n22-Feb-2023\\n\\n\\nNode.js Driver\\n1.6.19\\n27-Feb-2023\\n\\n\\nODBC Driver\\n2.25.8\\n08-Feb-2023\\n\\n\\n\\n3.0.2\\n23-Mar-2023\\n\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Scala and Java\\n1.7.1\\n08-Feb-2023\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nJanuary 2023 release notes¶\\n\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\nBCR?\\n\\n\\n\\n.NET Driver\\n2.0.20\\n24-Jan-2023\\n\\n\\nGo Snowflake Driver\\n1.6.17\\n26-Jan-2023\\n\\n\\nJDBC Driver\\n3.13.27\\n30-Jan-2023\\n\\n\\nNode.js Driver\\n1.6.18\\n31-Jan-2023\\n\\n\\nODBC Driver\\nTBD\\nTBD\\n\\n\\nPHP PDO Driver for Snowflake\\n1.2.6\\n24-Jan-2023\\n\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\n\\nSnowflake Connector for Python\\n3.0.0\\n27-Jan-2023\\nY\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\n\\nSnowpark Library for Python\\n26-Jan-2023\\n\\n\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\n\\nDecember 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\nGo Snowflake Driver\\n1.6.16\\n14-Dec-2022\\n\\nJDBC Driver\\n3.13.26\\n14-Dec-2022\\n\\nNode.js Driver\\n1.6.17\\n14-Dec-2022\\n\\nODBC Driver\\n2.25.7 (Doc only)\\n13-Dec-2022\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\n2.9.0\\n14-Dec-2022\\n\\nSnowflake Connector for Spark\\n2.11.1\\n13-Dec-2022\\n\\nSnowpark Library for Python\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\n1.4.5\\n09-Dec-2022\\n\\n\\n\\n\\n\\nNovember 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.18\\n02-Nov-2022\\n\\n\\n2.0.19\\n16-Nov-2022\\n\\nGo Snowflake Driver\\n1.6.15\\n16-Nov-2022\\n\\nJDBC Driver\\n3.13.25\\n16-Nov-2022\\n\\nNode.js Driver\\n1.6.16\\n18-Nov-2022\\n\\nODBC Driver\\nTBD\\nTBD\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\n1.8.2\\n18-Nov-2022\\n\\nSnowflake Connector for Python\\n2.8.2\\n18-Nov-2022\\n\\n\\n2.8.3\\n28-Nov-2022\\n\\nSnowflake Connector for Spark\\n2.11.1\\n13-Dec-2022\\n\\nSnowpark Library for Python\\n1.0.0\\n01-Nov-2022\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\n1.4.4\\n16-Nov-2022\\n\\n\\n\\n\\n\\nOctober 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.17\\n03-Oct-2022\\n\\nGo Snowflake Driver\\nTBD\\nTBD\\n\\nIngest Java SDK\\n2.0.4\\n31-Oct-20\\n\\nJDBC Driver\\n3.13.24\\n28-Oct-2022\\n\\nNode.js Driver\\n1.6.15\\n28-Oct-2022\\n\\nODBC Driver\\n2.25.6\\n12-Oct-2022\\n\\n\\n2.25.7\\n31-Oct-2022\\n\\nPHP PDO Driver for Snowflake\\n1.2.5\\n26-Oct-2022\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\nTBD\\nTBD\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\n1.6.1\\n03-Oct-2022\\n\\nSnowSQL\\n1.2.24\\n21-Oct-2022\\n\\nSQLAlchemy\\n1.4.3\\n21-Oct-2022\\n\\n\\n\\n\\n\\nSeptember 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\nGo Snowflake Driver\\nTBD\\nTBD\\n\\nJDBC Driver\\n3.13.23\\n30-Sep-2022\\n\\nNode.js Driver\\nTBD\\nTBD\\n\\nODBC Driver\\nTBD\\nTBD\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\n2.8.0\\n27-Sep-2022\\n\\nSnowflake Connector for Spark\\n2.11.0\\n02-Sep-2022\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\n1.4.2\\n28-Sep-2022\\n\\n\\n\\n\\n\\nAugust 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.16\\n23-Aug-2022\\n\\nGo Snowflake Driver\\n1.6.13\\n22-Aug-2022\\n\\nJDBC Driver\\n3.13.22\\n23-Aug-2022\\n\\nNode.js Driver\\n1.6.13\\n24-Aug-2022\\n\\nODBC Driver\\n2.25.4\\n01-Aug-2022\\n\\nPHP PDO Driver for Snowflake\\n1.2.4\\n24-Aug-2022\\n\\nSnowflake Connector for Kafka\\n1.6.8\\n23-Aug-2022\\n\\n\\n1.8.1\\nTBD\\n\\nSnowflake Connector for Python\\n2.7.12\\n24-Aug-2022\\n\\nSnowflake Connector for Spark\\n2.10.1\\n15-Aug-2022\\n\\nSnowpark Library for Scala and Java\\n1.6.0\\n12-Aug-2022\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\n1.4.1\\n23-Aug-2022\\n\\n\\n\\n\\n\\nJuly 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.15\\n19-Jul-2022\\n\\nGo Snowflake Driver\\n1.6.12\\n29-Jul-2022\\n\\nJDBC Driver\\n3.13.21\\n13-Jul-2022\\n\\nNode.js Driver\\n1.6.12\\n25-Jul-2022\\n\\nODBC Driver\\nTBD\\nTBD\\n\\nPHP PDO Driver for Snowflake\\n1.2.3\\n08-Jul-2022\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\n2.7.10\\n25-Jul-2022\\n\\n\\n2.7.11\\n28-Jul-2022\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\n1.6.0\\n12-Aug-2022\\n\\nSnowSQL\\n1.2.23\\n28-Jul-2022\\n\\nSQLAlchemy\\n1.4.0\\n21-Jul-2022\\n\\n\\n\\n\\n\\nJune 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.14\\n23-Jun-2022\\n\\nGo Snowflake Driver\\n1.6.11\\n23-Jun-2022\\n\\nJDBC Driver\\n3.13.20\\n23-Jun-2022\\n\\nNode.js Driver\\n1.6.11\\n23-Jun-2022\\n\\nODBC Driver\\n2.25.3\\n29-Jun-2022\\n\\n\\n2.25.2\\n01-Jun-2022\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\n2.7.9\\n24-Jun-2022\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\nTBD\\nTBD\\n\\nSnowSQL\\n1.2.22\\n29-Jun-2022\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\nMay 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.12\\n06-May-2022\\n\\n\\n2.0.13\\n18-May-2022\\n\\nGo Snowflake Driver\\n1.6.10\\n25-May-2022\\n\\nJDBC Driver\\n3.13.19\\n25-May-2022\\n\\nNode.js Driver\\n1.6.10\\n25-May-2022\\n\\nODBC Driver\\n22.25.0\\n09-May-2022\\n\\nPHP PDO Driver for Snowflake\\n1.2.2\\n24-May-2022\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\n2.7.8\\n26-May-2022\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\n1.4.1\\n26-May-2022\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\nTBD\\nTBD\\n\\n\\n\\n\\n\\nApril 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\nTBD\\nTBD\\n\\nGo Snowflake Driver\\n1.6.9\\n19-Apr-2022\\n\\nJDBC Driver\\n3.13.17\\n14-Apr-2022\\n\\nNode.js Driver\\n1.6.9\\n20-Apr-2022\\n\\nODBC Driver\\nTBD\\nTBD\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\n1.7.2\\n26-Apr-2022\\n\\nSnowflake Connector for Python\\n2.7.7\\n27-Apr-2022\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\n1.4.0\\n28-Apr-2022\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\n1.3.4\\n27-Apr-2022\\n\\n\\n\\n\\n\\nMarch 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.11\\n15-Mar-2022\\n\\nGo Snowflake Driver\\n1.6.8\\n15-Mar-2022\\n\\nJDBC Driver\\n3.13.16\\n17-Mar-2022\\n\\nNode.js Driver\\n1.6.8\\n17-Mar-2022\\n\\nODBC Driver\\n2.24.7\\n17-Mar-2022\\n\\nPHP PDO Driver for Snowflake\\n1.2.1\\n16-Mar-2022\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\n2.7.6\\n18-Mar-2022\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\n1.3.0\\n18-Mar-2022\\n\\n\\n1.2.0\\n02-Mar-2022\\n\\nSnowSQL\\nTBD\\nTBD\\n\\nSQLAlchemy\\n1.3.4\\n27-Apr-2022\\n\\n\\n\\n\\n\\nFebruary 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.10\\n16-Feb-2022\\n\\nGo Snowflake Driver\\n1.6.7\\n16-Feb-2022\\n\\nJDBC Driver\\n3.13.15\\n25-Feb-2022\\n\\nNode.js Driver\\n1.6.7\\n16-Feb-2022\\n\\nODBC Driver\\n2.24.7\\n17-Mar-2022\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\nTBD\\nTBD\\n\\nSnowflake Connector for Python\\nTBD\\nTBD\\n\\nSnowflake Connector for Spark\\n2.10.0\\n17-Feb-2022\\n\\nSnowpark Library for Scala and Java\\n1.1.0\\n04-Feb-2022\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\n\\n\\n\\nJanuary 2022 release notes¶\\n\\n\\n\\n\\n\\n\\n\\nClient\\nVersion\\nDate\\n\\n\\n\\n.NET Driver\\n2.0.9   18-Jan-2022\\n\\n\\n\\n1.2.9\\n18-Jan-2022\\n\\nGo Snowflake Driver\\n1.6.6\\n18-Jan-2022\\n\\nJDBC Driver\\n3.13.14\\n21-Jan-2022\\n\\n\\n3.13.13\\n18-Jan-2022\\n\\nNode.js Driver\\nTBD\\nTBD\\n\\nODBC Driver\\n2.24.5\\n21-Jan-2022\\n\\nPHP PDO Driver for Snowflake\\nTBD\\nTBD\\n\\nSnowflake Connector for Kafka\\n1.7.0\\n18-Jan-2022\\n\\nSnowflake Connector for Python\\n2.7.3\\n18-Jan-2022\\n\\nSnowflake Connector for Spark\\nTBD\\nTBD\\n\\nSnowpark Library for Scala and Java\\n1.0.0\\n26-Jan-2022\\n\\n\\n0.12.0\\n04-Jan-2022\\n\\nSnowSQL\\nTBD\\nTBD\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageAugust 2024 release notesJuly 2024 release notesJune 2024 release notesMay 2024 release notesApril 2024 release notesMarch 2024 release notesFebruary 2024 release notesJanuary 2024 release notesDecember 2023 release notesNovember 2023 release notesOctober 2023 release notesSeptember 2023 release notesAugust 2023 release notesJuly 2023 release notesJune 2023 release notesMay 2023 release notesApril 2023 release notesMarch 2023 release notesFebruary 2023 release notesJanuary 2023 release notesDecember 2022 release notesNovember 2022 release notesOctober 2022 release notesSeptember 2022 release notesAugust 2022 release notesJuly 2022 release notesJune 2022 release notesMay 2022 release notesApril 2022 release notesMarch 2022 release notesFebruary 2022 release notesJanuary 2022 release notesLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\"),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/guides-overview-connecting', 'title': 'Connect to Snowflake | Snowflake Documentation', 'language': 'en'}, page_content='Connect to Snowflake | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeEcosystemSnowsightClassic ConsoleSnowSQLVisual Studio Code SQL ExtensionClient DownloadClient ConfigurationTroubleshooting Snowflake client connectivityConnecting to Third-Party SystemsVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesConnecting to Snowflake\\n\\nConnect to Snowflake¶\\nSnowflake provides several different methods to interact with the Snowflake database including Snowsight, SnowSQL\\nand the Snowflake Classic Console.\\n\\nSnowflake EcosystemOverview of the 3rd-party tools and technologies, as well as the Snowflake-provided clients, in the Snowflake ecosystem.\\n\\nSnowsight: The Snowflake web interfaceSnowsight distills Snowflake’s powerful SQL support into a unified, easy-to-use experience.\\nUse Snowsight to perform your critical Snowflake operations.\\n\\nSnowSQL (CLI client)Detailed instructions for installing, configuring, and using the Snowflake command-line client.\\n\\nClassic ConsoleUse the Classic Console to perform tasks that would normally performed using SQL and the command line.\\n\\nSnowflake Extension for Visual Studio CodeUse the Snowflake Extension for Visual Studio Code to connect to Snowflake within Visual Studio Code and perform SQL operations.\\n\\nGeneral Configuration (All Clients)General configuration instructions that apply to all Snowflake-provided client.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about', 'title': 'About the Snowflake Native App Framework | Snowflake Documentation', 'language': 'en'}, page_content='About the Snowflake Native App Framework | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIMachine LearningSnowflake MLSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkKnown limitationsTutorial: Develop an appSnowflake Native App workflowProtect provider intellectual propertyDevelop an application packageCreate a setup scriptCreate a manifest fileCreate an application packageAdd application logicUpdate an appShare data contentAdd frontend experience with StreamlitSet up logging and event sharingAdd billable events to an appRequest access to objects and privileges in a consumer accountInstall and test an app locallyManage containers in a Snowflake Native AppAbout apps with containersKnown limitations in apps with containersTutorial: Create an app with containersWorkflow: Develop an app with containersCosts associated for apps with containersSet up the containers and services managed by an appCreate the manifest file for an app with containersAdd a compute pool to an appUse long running services in an appUse job services in an appUpgrade an app with containersPublish an app to consumersSecurity requirements and guidelinesGuidelines for submitting an app to the Snowflake MarketplaceSpecify the resources required by an appCreate a listing for an appSnowflake Native SDK for ConnectorsAbout the Snowflake Native SDK for ConnectorsExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewReferenceAPI ReferenceDeveloperSnowflake Native App Framework\\n\\nAbout the Snowflake Native App Framework¶\\n\\n Feature — Generally Available\\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see\\nSupport for private connectivity, VPS, and government regions.\\n\\nThis topic provides general information about the Snowflake Native App Framework.\\n\\nIntroduction to the Snowflake Native App Framework¶\\nThe Snowflake Native App Framework allows you to create data applications that leverage core Snowflake functionality.\\nThe Snowflake Native App Framework allows you to:\\n\\nExpand the capabilities of other Snowflake features by sharing data and related\\nbusiness logic with other Snowflake accounts. The business logic of an application can include a Streamlit app,\\nstored procedures, and functions written using Snowpark API,\\nJavaScript, and SQL.\\nShare an application with consumers through listings. A listing can be either free or paid.\\nYou can distribute and monetize your apps in the Snowflake Marketplace or distribute them to\\nspecific consumers using private listings.\\nInclude rich visualizations in your application using Streamlit.\\n\\nThe Snowflake Native App Framework also supports an enhanced development experience that provides:\\n\\nA streamlined testing environment where you can test your applications from a single account.\\nA robust developer workflow. While your data and related database objects remain within Snowflake,\\nyou can manage supporting code files and resources within source control using your preferred\\ndeveloper tools.\\nThe ability to release versions and patches for your application that allows you, as a provider,\\nto change and evolve the logic of your applications and release them incrementally to consumers.\\nSupport for logging of structured and unstructured events so that you can troubleshoot and monitor\\nyour applications.\\n\\n\\n\\nComponents of the Snowflake Native App Framework¶\\nThe following diagram shows a high-level view of the Snowflake Native App Framework.\\n\\n\\n\\nThe Snowflake Native App Framework is built around the concept of provider and consumer used by other\\nSnowflake features, including\\nSnowflake Collaboration\\nand Secure Data Sharing\\n\\nProviderA Snowflake user who wants to share data content and application logic with other Snowflake users.\\n\\nConsumerA Snowflake user who wants to access the data content and application logic shared by providers.\\n\\n\\n\\nDevelop and Test an Application Package¶\\nTo share data content and application logic with a consumer, providers create an application package.\\n\\nApplication packageAn application package encapsulates the data content, application logic,\\nmetadata, and setup script required by an application. An application package also contains\\ninformation about versions and patch levels defined for the application. See\\nCreate an application package for details.\\n\\n\\nAn application package can include references to data content and external code files that a provider\\nwants to include in the application. An application package requires a manifest file and a setup script.\\n\\nManifest fileDefines the configuration and setup properties required by the application, including the location of\\nthe setup script, versions, etc. See Create the manifest file for an application package\\nfor details.\\n\\nSetup scriptContains SQL statements that are run when the consumer installs or upgrades an application or when\\na provider installs or upgrades an application for testing. The location of the setup script is\\nspecified in the manifest.yml file. See Create a setup script\\nfor details.\\n\\n\\n\\n\\nPublish an Application Package¶\\nAfter developing and testing an application package, a provider can share an application with consumers by\\npublishing a listing containing the application package as the data product of a listing. The listing can be a Snowflake Marketplace\\nlisting or a private listing.\\n\\nSnowflake Marketplace listingAllows providers to market applications across the Snowflake Data Cloud. Offering a listing on the Snowflake Marketplace\\nlets providers share applications with many consumers simultaneously, rather than maintain\\nsharing relationships with each individual consumer.\\n\\nPrivate listingAllows providers to take advantage of the capabilities of listings to share applications directly with another\\nSnowflake account in any Snowflake region supported\\nby the Snowflake Native App Framework.\\n\\n\\nSee About Listings for details.\\n\\n\\nInstall and Manage an Application¶\\nAfter a provider publishes a listing containing an application package, consumers can discover the listing and\\ninstall the application.\\n\\nSnowflake Native AppA Snowflake Native App is the database object installed in the consumer account. When a consumer installs the Snowflake Native App,\\nSnowflake creates the application and runs the setup script to create the required objects within the application.\\nSee Install and test an app locally for details.\\n\\n\\nAfter installing the application, consumers can perform additional tasks, including:\\n\\nEnable logging and event sharing\\nto help providers troubleshoot the application.\\nGrant privileges required by the application.\\n\\nSee Working with Applications as a Consumer\\nfor details on how consumers install and manage an application.\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageIntroduction to the Snowflake Native App FrameworkComponents of the Snowflake Native App FrameworkRelated contentSnowflake Native App Framework workflowTutorial: Developing a Snowflake Native App with the Snowflake Native App FrameworkSnowflake Native App Framework commandsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://other-docs.snowflake.com/en/connectors/servicenow/v2/about', 'title': 'About the Snowflake Connector for ServiceNow®V2 | Snowflake Documentation', 'language': 'en'}, page_content='About the Snowflake Connector for ServiceNow®V2 | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeEcosystemSnowsightClassic ConsoleSnowSQLVisual Studio Code SQL ExtensionClient DownloadClient ConfigurationTroubleshooting Snowflake client connectivityConnecting to Third-Party SystemsGoogleSnowflake Connector for Google Analytics Aggregate DataSnowflake Connector for Google Analytics Raw DataSnowflake Connector for Google Looker StudioServiceNow®Snowflake Connector for ServiceNow®Snowflake Connector for ServiceNow®v2Cost GovernanceRole-Based Access Control for ConnectorsTutorial: Snowflake Connector for ServiceNow®ServiceNow® connector installation and configuration tasksDatabaseSnowflake Connector for MySQLSnowflake Connector for PostgreSQLSnowflake Connector for PostgreSQL and MySQL tutorialVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesConnecting to SnowflakeConnecting to Third-Party SystemsSnowflake Connector for ServiceNow®v2\\n\\nAbout the Snowflake Connector for ServiceNow®V2¶\\n\\nSnowflake Connector for ServiceNow®V2¶\\nThe Snowflake Connector for ServiceNow®V2 enables you to ingest ServiceNow® data into your Snowflake account.\\nServiceNow® is a cloud-based platform that delivers workflows for service management including incident management, change management, asset management, configuration management, service catalog, request fulfillment, etc.\\nThe Snowflake Connector for ServiceNow®V2 allows you to ingest data from ServiceNow® into Snowflake automatically. The connector supports both the initial load of historical data as well as incremental updates. The latest data is regularly pulled from ServiceNow®. You control how frequently it is refreshed.\\n\\nNote\\nData ingestion relies on v2  of the ServiceNow® table API.\\n\\nThe connector lets you replicate key dimensions and metrics from ServiceNow®, including:\\n\\nIncidents\\nChanges\\nUsers\\nService catalog items\\nConfiguration items\\nCompany assets\\n\\n\\n\\nKnown Limitations¶\\nThe Snowflake Connector for ServiceNow®V2 has the following limitations:\\n\\nAccounts in government regions are not supported.\\nYou can install only one instance of the connector for each Snowflake account. To install multiple instances\\nof the connector, you must create a separate account for each connector.\\nTo install and configure the connector, you must be logged in as a user with the ACCOUNTADMIN role. Other\\nroles are not supported at this time.\\nThe connector can only ingest ServiceNow® tables with the sys_id column present.\\nChanges to ServiceNow® table schema are not reflected in already ingested rows, unless they are updated.\\nServiceNow® views are not supported.\\nArchived records in ServiceNow® are not ingested into Snowflake. See here for more details.\\nReal-time ingestion is not supported. The smallest possible ingestion schedule is every 30 minutes.\\nThe connector does not work with ServiceNow® instances where IP address access control has been configured to\\ndeny access from an outside network.\\nThe connector does not work with a ServiceNow® instance that is hidden behind a VPN.\\nReplication of the connector to failover region is not automatic and requires additional manual steps.\\nThe connector does not support MANAGED ACCESS destination schemas.\\nThe connector requires the AUTOCOMMIT parameter to be enabled.\\nThe connector requires a virtual warehouse with AUTO_RESUME enabled. Serverless is not supported. Connector procedures are not guaranteed to work when called using serverless compute.\\nExecuting certain procedures via external tasks is not supported, that is:\\n\\n\\nCHECK_IF_AUDIT_ENABLED\\nCHECK_RECORD_HISTORY\\nTEST_CONNECTION\\nTEST_TABLE_ACCESS\\nGET_AVAILABLE_TABLES\\nENABLE_TABLE (with custom configuration parameters)\\nSET_CONNECTION_CONFIGURATION\\nFINALIZE_CONNECTOR_CONFIGURATION\\nCONFIGURE_QUERY_CATEGORY\\nCONFIGURE_CUSTOM_API_PATH\\n\\n\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Supplemental DocumentationAdditional terms of use may apply to features listed on this page.On this pageSnowflake Connector for ServiceNow®V2Known LimitationsRelated contentSnowflake Connector for ServiceNow®V2 installation and configuration tasksCost Governance of the Snowflake Connector for ServiceNow®V2Connector TermsSnowflake Connector for ServiceNow® release notesServiceNow® connector tutorialLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português'),\n"," Document(metadata={'source': 'https://docs.snowflake.com/en/developer-guide/snowpark/index', 'title': 'Snowpark API | Snowflake Documentation', 'language': 'en'}, page_content='Snowpark API | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewBuildersSnowflake DevOpsSnowpark LibrarySnowpark APIJavaPythonScalaMachine LearningSnowflake MLSnowpark Code Execution EnvironmentsSnowpark Container ServicesFunctions and ProceduresSnowflake APIsSnowflake Python APISQL REST APIAppsStreamlit in SnowflakeSnowflake Native App FrameworkExternal IntegrationExternal FunctionsKafka and Spark ConnectorsSnowflake ScriptingSnowflake Scripting Developer GuideToolsSnowflake CLIGitDriversOverviewReferenceAPI ReferenceDeveloperSnowpark API\\n\\nSnowpark API¶\\n\\n\\nThe Snowpark API provides an intuitive library for querying and processing data at scale in Snowflake. Using a library for any of three\\nlanguages, you can build applications that process data in Snowflake without moving data to the system where your application code runs,\\nand process at scale as part of the elastic and serverless Snowflake engine.\\nSnowflake currently provides Snowpark libraries for three languages: Java, Python, and Scala.\\n\\nQuickstarts¶\\nYou can use the following Quickstarts to get a hands-on introduction to Snowpark.\\n\\nMachine Learning with Snowpark Python\\nData Engineering Pipelines with Snowpark Python\\nGetting Started With Snowpark for Python and Streamlit\\nAn Image Recognition App in Snowflake using Snowpark Python, PyTorch, Streamlit and OpenAI\\nGetting Started With Snowpark Scala\\n\\n\\n\\nDeveloper Guides¶\\nYou can use Snowpark libraries for the languages listed in the following table:\\n\\n\\n\\n\\n\\n\\n\\nLanguage\\nDeveloper Guide\\nAPI Reference\\n\\n\\n\\nJava\\nSnowpark Developer Guide for Java\\nSnowpark Library for Java API Reference\\n\\nPython\\nSnowpark Developer Guide for Python\\nSnowpark Library for Python API Reference\\n\\nScala\\nSnowpark Developer Guide for Scala\\nSnowpark Library for Scala API Reference\\n\\n\\n\\n\\n\\nDownload¶\\nYou can download the Snowpark library for any of the three supported languages. For downloads, see\\nSnowpark Client Download (Snowflake Developer Center).\\n\\n\\nKey Features¶\\nSnowpark has several features that distinguish it from other client libraries, as described in the following sections.\\n\\nBenefits When Compared with the Spark Connector¶\\nIn comparison to using the Snowflake Connector for Spark, developing with Snowpark includes the following benefits:\\n\\nSupport for interacting with data within Snowflake using libraries and patterns purpose built for different languages without compromising\\non performance or functionality.\\nSupport for authoring Snowpark code using local tools such as Jupyter, VS Code, or IntelliJ.\\nSupport for pushdown for all operations, including Snowflake UDFs. This means Snowpark pushes down all data transformation and\\nheavy lifting to the Snowflake data cloud, enabling you to efficiently work with data of any size.\\nNo requirement for a separate cluster outside of Snowflake for computations. All of the computations are done within\\nSnowflake. Scale and compute management are handled by Snowflake.\\n\\n\\n\\nAbility to Build SQL Statements with Native Constructs¶\\nThe Snowpark API provides programming language constructs for building SQL statements. For example, the API provides a\\nselect method that you can use to specify the column names to return, rather than writing\\n\\'select column_name\\' as a string.\\nAlthough you can still use a string to specify the SQL statement to execute, you benefit from features like\\nintelligent code completion and type checking when you use the\\nnative language constructs provided by Snowpark.\\n\\nExample¶\\nPython code in the following example performs a select operation on the sample_product_data table, specifying the columns\\nid, name, and serial_number.\\n>>> # Import the col function from the functions module.\\n>>> from snowflake.snowpark.functions import col\\n\\n>>> # Create a DataFrame that contains the id, name, and serial_number\\n>>> # columns in the \"sample_product_data\" table.\\n>>> df = session.table(\"sample_product_data\").select(col(\"id\"), col(\"name\"), col(\"serial_number\"))\\n>>> df.show()\\n\\nCopy\\n\\n\\n\\nReduced Data Transfer¶\\nSnowpark operations are executed lazily on the server, meaning that you can use the library to delay running data transformation until as\\nlate in the pipeline as possible while batching up many operations into a single operation. This reduces the amount of data transferred\\nbetween your client and the Snowflake database. It also improves performance.\\nThe core abstraction in Snowpark is the DataFrame, which represents a set of data and provides methods to operate on that data.\\nIn your client code, you construct a DataFrame object and set it up to retrieve the data that you want to use (for example, the\\ncolumns containing the data, the filter to apply to rows, etc.).\\nThe data isn’t retrieved when you construct the DataFrame object. Instead, when you are ready to retrieve the data,\\nyou can perform an action that evaluates the DataFrame objects and sends the corresponding SQL statements to the Snowflake\\ndatabase for execution.\\n\\nExample¶\\nPython code in the following example sets up a query against a table. It calls the collect method to execute the query and retrieve\\nresults.\\n>>> # Create a DataFrame with the \"id\" and \"name\" columns from the \"sample_product_data\" table.\\n>>> # This does not execute the query.\\n>>> df = session.table(\"sample_product_data\").select(col(\"id\"), col(\"name\"))\\n\\n>>> # Send the query to the server for execution and\\n>>> # return a list of Rows containing the results.\\n>>> results = df.collect()\\n\\nCopy\\n\\n\\n\\nAbility to Create UDFs Inline¶\\nYou can create user-defined functions (UDFs) inline in a Snowpark app. Snowpark can push your code to the server, where the code can\\noperate on the data at scale. This is useful for looping or batch functionality where creating as a UDF will allow Snowflake to parallelize\\nand apply the codeful logic at scale within Snowflake.\\nYou can write functions in the same language that you use to write your client code (for example, by using anonymous functions\\nin Scala or by using lambda functions in Python). To use these functions to process data in the Snowflake database, you define\\nand call user-defined functions (UDFs) in your custom code.\\nSnowpark automatically pushes the custom code for UDFs to the Snowflake engine. When you call the UDF in your client code,\\nyour custom code is executed on the server (where the data is). You don’t need to transfer the data to your client in order to\\nexecute the function on the data.\\n\\nExample¶\\nPython code in the following example creates a UDF called my_udf and assigns it to the add_one variable.\\n>>> from snowflake.snowpark.types import IntegerType\\n>>> add_one = udf(lambda x: x+1, return_type=IntegerType(), input_types=[IntegerType()], name=\"my_udf\", replace=True)\\n\\nCopy\\n\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.On this pageQuickstartsDeveloper GuidesDownloadKey FeaturesRelated contentSnowpark Developer Guide for ScalaSnowpark Developer Guide for JavaSnowpark Developer Guide for PythonRelated info\\n\\nSnowpark Library for Scala and Java release notes\\nGetting Started With Snowpark in Scala (Snowflake Quickstarts)\\n\\nLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português')]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["docs"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/python/3.10.13/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["import faiss\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain import hub\n","from uuid import uuid4\n","\n","embeddings = HuggingFaceEmbeddings()\n","vectorstore = FAISS.from_documents(docs, embeddings)\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["documents = vectorstore.similarity_search('What is a warehouse?')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Virtual warehouses | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesOverviewMulticlusterConsiderationsWorking with warehousesQuery Acceleration ServiceMonitoring loadSnowpark-optimized warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesVirtual warehouses\n","\n","Virtual warehouses¶\n","A virtual warehouse, often referred to simply as a “warehouse”, is a cluster of compute resources in Snowflake. A virtual warehouse is\n","available in two types:\n","\n","Standard\n","Snowpark-optimized\n","\n","A warehouse provides the required resources, such as CPU, memory, and temporary storage, to\n","perform the following operations in a Snowflake session:\n","\n","Executing SQL SELECT statements that require compute resources (e.g. retrieving rows from tables and views).\n","Performing DML operations, such as:\n","\n","Updating rows in tables (DELETE , INSERT , UPDATE).\n","Loading data into tables (COPY INTO <table>).\n","Unloading data from tables  (COPY INTO <location>).\n","\n","\n","\n","\n","Note\n","To perform these operations, a warehouse must be running and in use for the session. While a warehouse is running, it consumes Snowflake\n","credits.\n","\n","\n","Overview of warehousesWarehouses are required for queries, as well as all DML operations, including loading data into tables.\n","In addition to being defined by its type as either Standard or Snowpark-optimized, a warehouse is defined by its size,\n","as well as the other properties that can be set to help control and automate warehouse activity.\n","\n","Snowpark-optimized warehousesSnowpark workloads can be run on both Standard and Snowpark-optimized warehouses. Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases\n","\n","Warehouse considerationsBest practices and general guidelines for using virtual warehouses in Snowflake to process queries\n","\n","Multi-cluster warehousesMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.\n","\n","Working with warehousesLearn how to create, stop, start and otherwise manage Snowflake warehouses.\n","\n","Using the Query Acceleration ServiceThe query acceleration service can accelerate parts of the query workload in a warehouse.\n","When enabled for a warehouse, query acceleration can improve overall warehouse performance by reducing the impact of outlier queries\n","(i.e. queries which use more resources then typical queries).\n","\n","Monitoring warehouse loadWarehouse query load measures the average number of queries that were running or queued within a specific interval.\n","\n","\n","\n","\n","Overview of warehouses\n","Snowpark-optimized warehouses\n","Warehouse considerations\n","Multi-cluster warehouses\n","Working with warehouses\n","Using the Query Acceleration Service\n","Monitoring warehouse load\n","\n","\n","\n","Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnderstanding compute costWorking with resource monitorsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n","Guides - Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingUser GuidesInstructions on performing various Snowflake operationsConnecting to SnowflakeSnowflake provides a variety of mechanisms for connecting to Snowflake and executing database commands. Choose between the web interface or the command line tool to connect to your Snowflake account. Learn how to use connectors to integrate third-party data into Snowflake.See allWeb InterfaceSnowsight distills Snowflake’s powerful SQL support into a unified, easy-to-use experience. Use Snowsight to perform your critical Snowflake operations.Learn moreCommand LineDetailed instructions for installing, configuring, and using the Snowflake command-line client, snowsql.Learn moreConnectorsThe Snowflake Connectors provide native integration of third-party applications and database systems in Snowflake. The connectors provide instant access to current data without the need to manually integrate against API endpoints.Learn moreSnowflake FundamentalsLearn the basics of warehouses, tables, and views in Snowflake.Snowflake WarehousesLearn how to set up and use virtual data warehouses to process the SQL statements that you execute.Overview of WarehousesMulti-cluster WarehousesWarehouse ConsiderationsWorking with WarehousesUsing the Query Acceleration ServiceSee allBasics of Snowflake Tables and ViewsLearn how to design and create tables and views for your data.Understanding Snowflake Table StructuresTable Design ConsiderationsOverview of ViewsWorking with Secure ViewsCloning ConsiderationsTable Storage ConsiderationsSee allBasics of Data TypesLearn about Snowflake data types and their usesIntroduction to Snowflake Data TypesNumeric Data TypesString and Binary Data TypesLogical Data TypesDate & Time Data TypesGeospatial Data TypesSee allGetting data in to SnowflakeSnowflake provides several different methods to load data in to Snowflake, such as by using Snowpipe, loading from cloud storage, or uploading files using Snowsight.Understanding Data LoadingData can be loaded into Snowflake in a number of ways. Learn about data loading concepts, different tasks, tools, and techniques to quickly and easily load data into Snowflake.Bulk Data LoadingLearn to use the COPY command to load data on-demand directly from an AWS S3 bucket, Google Cloud Share, or a Microsoft Azure storage container into Snowflake.SnowpipeUse Snowflake Snowpipe to load data automatically as it arrives.Working with dataQueries and other standard database features are just the beginning when you work with your data in Snowflake. You also use machine learning functions to analyze data in Snowflake.See allQueriesSnowflake supports standard SQL, including a subset of ANSI SQL:1999 and the SQL:2003 analytic extensions. Learn how to use queries to interact with Snowflake using simple queries, joins, and more.Learn moreViews, Materialized Views, & Dynamic TablesViews are just the beginning of how you can examine data. Snowflake provides a number of mechanism for joining data including Materialized Views and Dynamic Tables.Learn moreStreams and TasksStreams and tasks make executing complex task based solutions simple and easy. Streams allow you to track changes to database objects and tasks provide a mechanism to then execute SQL when those events occur.Learn moreML FunctionsML Functions are Snowflake’s intelligent, fully-managed service that enables organizations to quickly analyze data within Snowflake.Learn moreCollaboratingShare data and applications with other Snowflake users. Discover and publish listings of data products on the Snowflake Marketplace, share data products privately, or use a direct share to quickly share data with someone in the same region.What are listings?With listings, you can provide data and other information to other Snowflake users, and you can access data and other information shared by Snowflake providers.Becoming a listing providerBecoming a provider of listings in Snowflake makes it easier to manage sharing from your account to other Snowflake accounts.Becoming a listing consumerGet access to data products shared privately or on the Snowflake Marketplace by becoming a consumer of listings.More GuidesAlerts and NotificationsSetting Up Alerts Based on Data in SnowflakeSending Email NotificationsSee allSecurityAuthenticationAccess ControlEncryption key managementEncryptionNetworkingSee allGovernance and ComplianceData Lineage and DependenciesData Access PoliciesData SensitivityClassificationComplianceSee allPrivacyAggregation PoliciesProjection PoliciesSee allOrganizations and AccountsOrganizationsAccount identifiersSee allBusiness Continuity & Data RecoveryReplication & FailoverClient RedirectTime TravelFail-safeSee allPerformance and CostCost ManagementQuery PerformanceSee allWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n","Bulk loading from a local file system | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemChoosing an Internal StageStaging FilesCopying Data from a Local File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingLoading Semi-Structured DataIntroductionSupported FormatsConsiderationsAccessing Unstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData LoadingLocal File System\n","\n","Bulk loading from a local file system¶\n","This set of topics describes how to use the COPY command to bulk load data from a local file system into tables using an internal (i.e.\n","Snowflake-managed) stage. For instructions on loading data from a cloud storage location that you manage, refer to Bulk loading from Amazon S3, Bulk loading from Google Cloud Storage, or Bulk loading from Microsoft Azure.\n","As illustrated in the diagram below, loading data from a local file system is performed in two, separate steps:\n","\n","Step 1:\n","Upload (i.e. stage) one or more data files to a Snowflake stage (named internal stage or table/user stage) using the PUT command.\n","\n","Step 2:\n","Use the COPY INTO <table> command to load the contents of the staged file(s) into a Snowflake database table.\n","Regardless of the stage you use, this step requires a running virtual warehouse that is also the current (i.e. in use) warehouse for the session. The warehouse provides the compute resources to\n","perform the actual insertion of rows into the table.\n","\n","\n","\n","\n","\n","\n","Tip\n","The instructions in this set of topics assume you have read Preparing to load data and have created a named file format, if desired.\n","Before you begin, you may also want to read Data loading considerations for best practices, tips, and other guidance.\n","\n","Next Topics:\n","\n","Configuration tasks (complete as needed):\n","\n","\n","Choosing an internal stage for local files\n","\n","\n","\n","Data loading tasks (complete for each set of files you load):\n","\n","\n","Staging data files from a local file system\n","Copying data from an internal stage\n","\n","\n","\n","\n","\n","Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnloading into a Snowflake stageLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n","Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusSnowflake DocumentationStreamline your Snowflake journey with comprehensive documentation and learning resourcesFeatured ResourcesDive into our top picksuser guideConnecting to SnowflakeLearn about the applications and tools that you can use to access Snowflakeuser guideWorking with Virtual WarehousesLearn how to create and manage warehouses, which are used to process queriesuser guideDatabases, Tables and ViewsLearn how to create and manage databases, tables, and views for storing and accessing your datauser guideLoad Data into SnowflakeLearn about the different options for getting data into Snowflake and setting up a pipeline to transform your dataDiscover the Most Popular ResourcesGetting StartedBasic information and instructions for first-time users of Snowflake.Getting a Trial AccountSnowflake in 20 MinutesKey Concepts and ArchitectureConnecting to SnowflakeSee allUser GuidesInstructions on performing various Snowflake operations.Understanding & Using Time TravelWorking with Temporary and Transient TablesWorking with Materialized ViewsSee allDeveloper GuidesWrite applications that extend Snowflake, act as a client, or act as an integrating component.Snowflake Native App FrameworkSnowpark APIUser-defined Functions (UDFs) and Stored ProceduresDriversSee allReferenceReference for SQL data types, SQL commands, SQL functions, SQL classes, scripting, views, and other areasSQL Data TypesSQL CommandsSQL FunctionsSQL ClassesSnowflake ScriptingSee allTutorialsTutorials to help you learn the basics of using SnowflakeSnowflake in 20 MinutesBulk Loading from a Local File SystemBulk Loading from Amazon S3 Using COPYSee allReleasesOverview of the new features, enhancements, and important fixes introduced in the most recent releases of Snowflake.New featuresBehavior change logSnowflake connector, driver, and library monthly releasesSee allWhat’s NewOverview of the new features, enhancements, and important behavior changes introduced in the most recent releases of SnowflakeSee allRecent Product Updatesgenerally availableOn April 29, 2024Dynamic TablesWith this release, we are pleased to announce the general availability of dynamic tables, a new table type for continuous processing pipelines.generally availableOn April 11th, 2024BudgetsWith this release, we are pleased to announce the general availability of Budgets which enables account-level monitoring and notification of Snowflake credit usage for a group of specific Snowflake objects.generally availableOn April 23, 2024Snowflake Connector for ServiceNow® V2generally availableOn May 13, 2024ASOF JOINpreviewOn May 09, 2024Python user-defined aggregate functionspreviewOn May 03, 2024Snowflake Model RegistryAnnouncementsStay informed and discover the latest updates, improvements, and enhancementsBehavior ChangesBehavior changes that may impact your usageDeprecated FeaturesFeatures and operating systems/programming languages that are deprecated or pending deprecationPerformance ImprovementsAll features that can improve performance and make queries run fasterVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n"]}],"source":["for docs in documents:\n","    print(docs.page_content)\n","    print('------------------------------------------------------------------------------------------------------------------------------------')"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
