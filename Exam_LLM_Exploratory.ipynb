{"cells":[{"cell_type":"code","execution_count":203,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Python(76685) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langsmith in /opt/homebrew/lib/python3.11/site-packages (0.1.99)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/lib/python3.11/site-packages (from langsmith) (3.10.4)\n","Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/lib/python3.11/site-packages (from langsmith) (1.10.13)\n","Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langsmith) (2.31.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langsmith) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langsmith) (2023.5.7)\n"]},{"name":"stderr","output_type":"stream","text":["Python(76688) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchainhub in /opt/homebrew/lib/python3.11/site-packages (0.1.21)\n","Requirement already satisfied: packaging<25,>=23.2 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from langchainhub) (23.2)\n","Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchainhub) (2.31.0)\n","Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/homebrew/lib/python3.11/site-packages (from langchainhub) (2.32.0.20240602)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2023.5.7)\n"]},{"name":"stderr","output_type":"stream","text":["Python(76698) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (1.38.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (4.3.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.10.13)\n","Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.11/site-packages (from openai) (4.65.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/lib/python3.11/site-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n","Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.5.7)\n","Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"]},{"name":"stderr","output_type":"stream","text":["Python(76699) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Python(76702) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchain_anthropic in /opt/homebrew/lib/python3.11/site-packages (0.1.23)\n","Requirement already satisfied: anthropic<1,>=0.30.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain_anthropic) (0.33.1)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from langchain_anthropic) (0.7.1)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /opt/homebrew/lib/python3.11/site-packages (from langchain_anthropic) (0.2.30)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (4.3.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (0.27.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (0.5.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (1.10.13)\n","Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (1.3.1)\n","Requirement already satisfied: tokenizers>=0.13.0 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (0.19.1)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/lib/python3.11/site-packages (from anthropic<1,>=0.30.0->langchain_anthropic) (4.12.2)\n","Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (6.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (0.1.99)\n","Requirement already satisfied: packaging<25,>=23.2 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (23.2)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (8.3.0)\n","Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.30.0->langchain_anthropic) (3.4)\n","Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.30.0->langchain_anthropic) (2023.5.7)\n","Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.30.0->langchain_anthropic) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.30.0->langchain_anthropic) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (3.10.4)\n","Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (2.31.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain_anthropic) (0.24.5)\n","Requirement already satisfied: filelock in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain_anthropic) (3.12.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain_anthropic) (2023.6.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain_anthropic) (4.65.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (3.1.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_anthropic) (2.2.1)\n"]},{"name":"stderr","output_type":"stream","text":["Python(76709) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openllm in /opt/homebrew/lib/python3.11/site-packages (0.6.9)\n","Requirement already satisfied: bentoml in /opt/homebrew/lib/python3.11/site-packages (from openllm) (1.3.2)\n","Requirement already satisfied: dulwich in /opt/homebrew/lib/python3.11/site-packages (from openllm) (0.22.1)\n","Requirement already satisfied: nvidia-ml-py in /opt/homebrew/lib/python3.11/site-packages (from openllm) (11.525.150)\n","Requirement already satisfied: openai==1.38.0 in /opt/homebrew/lib/python3.11/site-packages (from openllm) (1.38.0)\n","Requirement already satisfied: pathlib in /opt/homebrew/lib/python3.11/site-packages (from openllm) (1.0.1)\n","Requirement already satisfied: pip-requirements-parser in /opt/homebrew/lib/python3.11/site-packages (from openllm) (32.0.1)\n","Requirement already satisfied: psutil in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from openllm) (5.9.8)\n","Requirement already satisfied: pyaml in /opt/homebrew/lib/python3.11/site-packages (from openllm) (24.7.0)\n","Requirement already satisfied: questionary in /opt/homebrew/lib/python3.11/site-packages (from openllm) (2.0.1)\n","Requirement already satisfied: tabulate in /opt/homebrew/lib/python3.11/site-packages (from openllm) (0.9.0)\n","Requirement already satisfied: typer in /opt/homebrew/lib/python3.11/site-packages (from openllm) (0.12.3)\n","Requirement already satisfied: uv in /opt/homebrew/lib/python3.11/site-packages (from openllm) (0.2.36)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (4.3.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (1.10.13)\n","Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (4.65.0)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/lib/python3.11/site-packages (from openai==1.38.0->openllm) (4.12.2)\n","Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (3.8.4)\n","Requirement already satisfied: aiosqlite>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.20.0)\n","Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (23.1.0)\n","Requirement already satisfied: cattrs<23.2.0,>=22.1.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (23.1.2)\n","Requirement already satisfied: circus!=0.17.2,>=0.17.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.18.0)\n","Requirement already satisfied: click-option-group in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.5.6)\n","Requirement already satisfied: click>=7.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (8.1.3)\n","Requirement already satisfied: cloudpickle>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (3.0.0)\n","Requirement already satisfied: deepmerge in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (1.1.1)\n","Requirement already satisfied: fs in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (2.4.16)\n","Requirement already satisfied: httpx-ws>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.6.0)\n","Requirement already satisfied: inflection in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.5.1)\n","Requirement already satisfied: inquirerpy>=0.3.4 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.3.4)\n","Requirement already satisfied: jinja2>=3.0.1 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (3.1.2)\n","Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (1.24.3)\n","Requirement already satisfied: opentelemetry-api==1.20.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (1.20.0)\n","Requirement already satisfied: opentelemetry-instrumentation-aiohttp-client==0.41b0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.41b0)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.41b0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.41b0)\n","Requirement already satisfied: opentelemetry-instrumentation==0.41b0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.41b0)\n","Requirement already satisfied: opentelemetry-sdk==1.20.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (1.20.0)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.41b0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.41b0)\n","Requirement already satisfied: opentelemetry-util-http==0.41b0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.41b0)\n","Requirement already satisfied: packaging>=22.0 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from bentoml->openllm) (23.2)\n","Requirement already satisfied: pathspec in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.11.1)\n","Requirement already satisfied: prometheus-client>=0.10.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.20.0)\n","Requirement already satisfied: python-dateutil in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from bentoml->openllm) (2.8.2)\n","Requirement already satisfied: python-json-logger in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (2.0.7)\n","Requirement already satisfied: python-multipart in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.0.9)\n","Requirement already satisfied: pyyaml>=5.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (6.0)\n","Requirement already satisfied: rich>=11.2.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (13.7.1)\n","Requirement already satisfied: schema in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.7.7)\n","Requirement already satisfied: simple-di>=0.1.4 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.1.5)\n","Requirement already satisfied: starlette>=0.24.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.37.2)\n","Requirement already satisfied: tomli-w in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (1.0.0)\n","Requirement already satisfied: uvicorn>=0.22.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.30.5)\n","Requirement already satisfied: watchfiles>=0.15.0 in /opt/homebrew/lib/python3.11/site-packages (from bentoml->openllm) (0.23.0)\n","Requirement already satisfied: deprecated>=1.2.6 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-api==1.20.0->bentoml->openllm) (1.2.14)\n","Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-api==1.20.0->bentoml->openllm) (6.6.0)\n","Requirement already satisfied: setuptools>=16.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.41b0->bentoml->openllm) (67.8.0)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.41b0->bentoml->openllm) (1.16.0)\n","Requirement already satisfied: asgiref~=3.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.41b0->bentoml->openllm) (3.8.1)\n","Requirement already satisfied: pyparsing in /opt/homebrew/lib/python3.11/site-packages (from pip-requirements-parser->openllm) (3.1.1)\n","Requirement already satisfied: urllib3>=1.25 in /opt/homebrew/lib/python3.11/site-packages (from dulwich->openllm) (2.2.1)\n","Requirement already satisfied: prompt_toolkit<=3.0.36,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from questionary->openllm) (3.0.36)\n","Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typer->openllm) (1.5.4)\n","Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai==1.38.0->openllm) (3.4)\n","Requirement already satisfied: pyzmq>=17.0 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from circus!=0.17.2,>=0.17.0->bentoml->openllm) (25.1.2)\n","Requirement already satisfied: tornado>=5.0.2 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from circus!=0.17.2,>=0.17.0->bentoml->openllm) (6.4)\n","Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.38.0->openllm) (2023.5.7)\n","Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai==1.38.0->openllm) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.38.0->openllm) (0.14.0)\n","Requirement already satisfied: wsproto in /opt/homebrew/lib/python3.11/site-packages (from httpx-ws>=0.6.0->bentoml->openllm) (1.2.0)\n","Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from inquirerpy>=0.3.4->bentoml->openllm) (0.3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2>=3.0.1->bentoml->openllm) (2.1.3)\n","Requirement already satisfied: wcwidth in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->openllm) (0.2.13)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.11/site-packages (from rich>=11.2.0->bentoml->openllm) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from rich>=11.2.0->bentoml->openllm) (2.17.2)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->bentoml->openllm) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->bentoml->openllm) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->bentoml->openllm) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->bentoml->openllm) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->bentoml->openllm) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->bentoml->openllm) (1.3.1)\n","Requirement already satisfied: appdirs~=1.4.3 in /opt/homebrew/lib/python3.11/site-packages (from fs->bentoml->openllm) (1.4.4)\n","Requirement already satisfied: six~=1.10 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from fs->bentoml->openllm) (1.16.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/homebrew/lib/python3.11/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api==1.20.0->bentoml->openllm) (3.15.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->bentoml->openllm) (0.1.2)\n"]},{"name":"stderr","output_type":"stream","text":["Python(76710) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /opt/homebrew/lib/python3.11/site-packages (0.24.5)\n","Requirement already satisfied: filelock in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from huggingface_hub) (3.12.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /Users/abdulhakeemoyaqoob/Library/Python/3.11/lib/python/site-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub) (6.0)\n","Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub) (4.65.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.5.7)\n"]}],"source":["#!pip install langchain_openai\n","#!pip install chromadb\n","#!pip install transformers\n","#!pip install faiss-cpu\n","#!pip install -qU langchain-huggingface\n","#!pip install -U langchain-community\n","#!pip install langchain\n","import os\n","\n","\n","!pip install langsmith\n","!pip install langchainhub\n","!pip install openai\n","!pip install --upgrade --quiet  openllm\n","!pip install -U langchain_anthropic\n","!pip install openllm\n","!pip install huggingface_hub"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[],"source":["import bs4\n","from langchain import hub\n","from langchain.llms import OpenLLM\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","\n","loader = WebBaseLoader(\n","    web_paths=(\"https://other-docs.snowflake.com/en/polaris/overview\",),\n",")\n","docs = loader.load()"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[],"source":["data = []\n","for doc in docs:\n","  page_number = doc.metadata['source']\n","  page_title = doc.metadata['title']\n","  page_content = doc.page_content\n","  data.append([page_number,page_title, page_content])\n","\n","df = pd.DataFrame(data, columns=['Page','Page Title','Content'])"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 40 English links. Loading documents...\n","                                                Page  \\\n","0  https://docs.snowflake.com/en/release-notes/pe...   \n","1  https://docs.snowflake.com/en/developer-guide/...   \n","2  https://docs.snowflake.com/en/user-guide/wareh...   \n","3            https://docs.snowflake.com/en/tutorials   \n","4  https://docs.snowflake.com/en/user-guide/admin...   \n","\n","                                          Page Title  \\\n","0  Performance Improvements | Snowflake Documenta...   \n","1             Snowpark API | Snowflake Documentation   \n","2       Virtual warehouses | Snowflake Documentation   \n","3                Tutorials - Snowflake Documentation   \n","4           Trial accounts | Snowflake Documentation   \n","\n","                                             Content  \n","0  Performance Improvements | Snowflake Documenta...  \n","1  Snowpark API | Snowflake DocumentationDOCUMENT...  \n","2  Virtual warehouses | Snowflake DocumentationDO...  \n","3  Tutorials - Snowflake DocumentationDOCUMENTATI...  \n","4  Trial accounts | Snowflake DocumentationDOCUME...  \n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def get_english_links(url):\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        links = soup.find_all('a')\n","        hrefs = {link.get('href') for link in links if link.get('href')}\n","        english_links = {href for href in hrefs if '/en/' in href}\n","        english_links = {href if href.startswith('http') else url + href for href in english_links}\n","        return english_links\n","    else:\n","        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n","        return set()\n","\n","def load_documents_from_links(links):\n","    loader = WebBaseLoader(web_paths=tuple(links))\n","    docs = loader.load()\n","    data = []\n","    for doc in docs:\n","        page_number = doc.metadata['source']\n","        page_title = doc.metadata['title']\n","        page_content = doc.page_content\n","        data.append([page_number, page_title, page_content])\n","    return pd.DataFrame(data, columns=['Page', 'Page Title', 'Content']), docs\n","\n","\n","url = \"https://docs.snowflake.com\"\n","english_links = get_english_links(url)\n","if english_links:\n","    print(f\"Found {len(english_links)} English links. Loading documents...\")\n","    df, docs = load_documents_from_links(english_links)\n","    print(df.head())\n","else:\n","    print(\"No English links found or webpage could not be accessed.\")"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["import faiss\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain import hub\n","from uuid import uuid4\n","\n","embeddings = HuggingFaceEmbeddings()\n","vectorstore = FAISS.from_documents(docs, embeddings)\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[],"source":["documents = vectorstore.similarity_search('What is a warehouse?')"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[{"data":{"text/plain":["4"]},"execution_count":209,"metadata":{},"output_type":"execute_result"}],"source":["len(documents)"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Virtual warehouses | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesOverviewMulticlusterConsiderationsWorking with warehousesQuery Acceleration ServiceMonitoring loadSnowpark-optimized warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesVirtual warehouses\n","\n","Virtual warehouses¶\n","A virtual warehouse, often referred to simply as a “warehouse”, is a cluster of compute resources in Snowflake. A virtual warehouse is\n","available in two types:\n","\n","Standard\n","Snowpark-optimized\n","\n","A warehouse provides the required resources, such as CPU, memory, and temporary storage, to\n","perform the following operations in a Snowflake session:\n","\n","Executing SQL SELECT statements that require compute resources (e.g. retrieving rows from tables and views).\n","Performing DML operations, such as:\n","\n","Updating rows in tables (DELETE , INSERT , UPDATE).\n","Loading data into tables (COPY INTO <table>).\n","Unloading data from tables  (COPY INTO <location>).\n","\n","\n","\n","\n","Note\n","To perform these operations, a warehouse must be running and in use for the session. While a warehouse is running, it consumes Snowflake\n","credits.\n","\n","\n","Overview of warehousesWarehouses are required for queries, as well as all DML operations, including loading data into tables.\n","In addition to being defined by its type as either Standard or Snowpark-optimized, a warehouse is defined by its size,\n","as well as the other properties that can be set to help control and automate warehouse activity.\n","\n","Snowpark-optimized warehousesSnowpark workloads can be run on both Standard and Snowpark-optimized warehouses. Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases\n","\n","Warehouse considerationsBest practices and general guidelines for using virtual warehouses in Snowflake to process queries\n","\n","Multi-cluster warehousesMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.\n","\n","Working with warehousesLearn how to create, stop, start and otherwise manage Snowflake warehouses.\n","\n","Using the Query Acceleration ServiceThe query acceleration service can accelerate parts of the query workload in a warehouse.\n","When enabled for a warehouse, query acceleration can improve overall warehouse performance by reducing the impact of outlier queries\n","(i.e. queries which use more resources then typical queries).\n","\n","Monitoring warehouse loadWarehouse query load measures the average number of queries that were running or queued within a specific interval.\n","\n","\n","\n","\n","Overview of warehouses\n","Snowpark-optimized warehouses\n","Warehouse considerations\n","Multi-cluster warehouses\n","Working with warehouses\n","Using the Query Acceleration Service\n","Monitoring warehouse load\n","\n","\n","\n","Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnderstanding compute costWorking with resource monitorsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n","Guides - Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingUser GuidesInstructions on performing various Snowflake operationsConnecting to SnowflakeSnowflake provides a variety of mechanisms for connecting to Snowflake and executing database commands. Choose between the web interface or the command line tool to connect to your Snowflake account. Learn how to use connectors to integrate third-party data into Snowflake.See allWeb InterfaceSnowsight distills Snowflake’s powerful SQL support into a unified, easy-to-use experience. Use Snowsight to perform your critical Snowflake operations.Learn moreCommand LineDetailed instructions for installing, configuring, and using the Snowflake command-line client, snowsql.Learn moreConnectorsThe Snowflake Connectors provide native integration of third-party applications and database systems in Snowflake. The connectors provide instant access to current data without the need to manually integrate against API endpoints.Learn moreSnowflake FundamentalsLearn the basics of warehouses, tables, and views in Snowflake.Snowflake WarehousesLearn how to set up and use virtual data warehouses to process the SQL statements that you execute.Overview of WarehousesMulti-cluster WarehousesWarehouse ConsiderationsWorking with WarehousesUsing the Query Acceleration ServiceSee allBasics of Snowflake Tables and ViewsLearn how to design and create tables and views for your data.Understanding Snowflake Table StructuresTable Design ConsiderationsOverview of ViewsWorking with Secure ViewsCloning ConsiderationsTable Storage ConsiderationsSee allBasics of Data TypesLearn about Snowflake data types and their usesIntroduction to Snowflake Data TypesNumeric Data TypesString and Binary Data TypesLogical Data TypesDate & Time Data TypesGeospatial Data TypesSee allGetting data in to SnowflakeSnowflake provides several different methods to load data in to Snowflake, such as by using Snowpipe, loading from cloud storage, or uploading files using Snowsight.Understanding Data LoadingData can be loaded into Snowflake in a number of ways. Learn about data loading concepts, different tasks, tools, and techniques to quickly and easily load data into Snowflake.Bulk Data LoadingLearn to use the COPY command to load data on-demand directly from an AWS S3 bucket, Google Cloud Share, or a Microsoft Azure storage container into Snowflake.SnowpipeUse Snowflake Snowpipe to load data automatically as it arrives.Working with dataQueries and other standard database features are just the beginning when you work with your data in Snowflake. You also use machine learning functions to analyze data in Snowflake.See allQueriesSnowflake supports standard SQL, including a subset of ANSI SQL:1999 and the SQL:2003 analytic extensions. Learn how to use queries to interact with Snowflake using simple queries, joins, and more.Learn moreViews, Materialized Views, & Dynamic TablesViews are just the beginning of how you can examine data. Snowflake provides a number of mechanism for joining data including Materialized Views and Dynamic Tables.Learn moreStreams and TasksStreams and tasks make executing complex task based solutions simple and easy. Streams allow you to track changes to database objects and tasks provide a mechanism to then execute SQL when those events occur.Learn moreML FunctionsML Functions are Snowflake’s intelligent, fully-managed service that enables organizations to quickly analyze data within Snowflake.Learn moreCollaboratingShare data and applications with other Snowflake users. Discover and publish listings of data products on the Snowflake Marketplace, share data products privately, or use a direct share to quickly share data with someone in the same region.What are listings?With listings, you can provide data and other information to other Snowflake users, and you can access data and other information shared by Snowflake providers.Becoming a listing providerBecoming a provider of listings in Snowflake makes it easier to manage sharing from your account to other Snowflake accounts.Becoming a listing consumerGet access to data products shared privately or on the Snowflake Marketplace by becoming a consumer of listings.More GuidesAlerts and NotificationsSetting Up Alerts Based on Data in SnowflakeSending Email NotificationsSee allSecurityAuthenticationAccess ControlEncryption key managementEncryptionNetworkingSee allGovernance and ComplianceData Lineage and DependenciesData Access PoliciesData SensitivityClassificationComplianceSee allPrivacyAggregation PoliciesProjection PoliciesSee allOrganizations and AccountsOrganizationsAccount identifiersSee allBusiness Continuity & Data RecoveryReplication & FailoverClient RedirectTime TravelFail-safeSee allPerformance and CostCost ManagementQuery PerformanceSee allWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n","Bulk loading from a local file system | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemChoosing an Internal StageStaging FilesCopying Data from a Local File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingSemi-Structured DataIntroductionSupported FormatsConsiderationsUnstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingLoading Unstructured Data with Document AIAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData LoadingLocal File System\n","\n","Bulk loading from a local file system¶\n","This set of topics describes how to use the COPY command to bulk load data from a local file system into tables using an internal (i.e.\n","Snowflake-managed) stage. For instructions on loading data from a cloud storage location that you manage, refer to Bulk loading from Amazon S3, Bulk loading from Google Cloud Storage, or Bulk loading from Microsoft Azure.\n","As illustrated in the diagram below, loading data from a local file system is performed in two, separate steps:\n","\n","Step 1:\n","Upload (i.e. stage) one or more data files to a Snowflake stage (named internal stage or table/user stage) using the PUT command.\n","\n","Step 2:\n","Use the COPY INTO <table> command to load the contents of the staged file(s) into a Snowflake database table.\n","Regardless of the stage you use, this step requires a running virtual warehouse that is also the current (i.e. in use) warehouse for the session. The warehouse provides the compute resources to\n","perform the actual insertion of rows into the table.\n","\n","\n","\n","\n","\n","\n","Tip\n","The instructions in this set of topics assume you have read Preparing to load data and have created a named file format, if desired.\n","Before you begin, you may also want to read Data loading considerations for best practices, tips, and other guidance.\n","\n","Next Topics:\n","\n","Configuration tasks (complete as needed):\n","\n","\n","Choosing an internal stage for local files\n","\n","\n","\n","Data loading tasks (complete for each set of files you load):\n","\n","\n","Staging data files from a local file system\n","Copying data from an internal stage\n","\n","\n","\n","\n","\n","Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnloading into a Snowflake stageLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n","Dynamic tables | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesDatabases, Tables, & ViewsData TypesData LoadingOverviewFeature SummaryConsiderationsPreparing to Load DataStaging Files Using SnowsightLoading Data Using the Web InterfaceMonitor data loading activityBulk LoadingLocal File SystemAmazon S3Google Cloud StorageMicrosoft AzureTroubleshootingSnowpipeOverviewAuto IngestREST EndpointsError NotificationsTroubleshootingManagingSnowpipe CostsSnowpipe StreamingOverviewConfigurations and ExamplesRecommendationsSnowpipe Streaming CostsKafka Connector with Snowpipe StreamingSchema Detection and Evolution for Kafka Connector with Snowpipe StreamingSemi-Structured DataIntroductionSupported FormatsConsiderationsUnstructured DataIntroductionDirectory TablesREST APIProcessing with UDF and Procedure HandlersSharingTroubleshootingLoading Unstructured Data with Document AIAccessing Data in Other StorageAmazon S3-compatible StorageQuerying and Transforming DataQuerying Data in Staged FilesQuerying Metadata for Staged FilesTransforming Data During LoadEvolving Table Schema AutomaticallyContinuous Data PipelinesOverviewDynamic tablesOverviewWorking with dynamic tablesBest practicesUnderstanding costLimitationsDynamic table performanceTroubleshootingStreamsTasksExamplesData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesData LoadingDynamic tables\n","\n","Dynamic tables¶\n","Dynamic tables simplify data engineering in Snowflake by providing a reliable, cost-effective, and\n","automated way to transform data. Instead of managing transformation steps with tasks and scheduling, you\n","define the end state using dynamic tables and let Snowflake handle the pipeline management.\n","Here’s why they’re beneficial:\n","\n","Declarative programming: Define your pipeline outcomes using declarative SQL without worrying\n","about the steps to achieve them, reducing complexity.\n","Transparent orchestration: Easily create pipelines of various shapes, from linear chains to\n","directed graphs, by chaining dynamic tables together. Snowflake manages the orchestration and\n","scheduling of pipeline refresh based on your data freshness target.\n","Performance boost with incremental processing: For favorable workloads\n","that are suited for incremental processing, dynamic tables can provide a significant performance\n","improvement over full refreshes.\n","Easy switching: Transition seamlessly from batch to streaming with a single ALTER DYNAMIC TABLE\n","command. You control how often data is refreshed in your pipeline, which helps balance cost and data\n","freshness.\n","Operationalization: Dynamic tables are fully observable and manageable through Snowsight, and also\n","offer programmatic access to build your own observability apps.\n","\n","A dynamic table reflects query results, eliminating the need for a separate target table and custom code\n","for data transformation. An automated process updates the results regularly through scheduled\n","refreshes. Since a dynamic table’s content is based on the\n","query, you can’t modify it using DML operations. The automated refresh process materializes query results\n","into the dynamic table.\n","The following topics introduce dynamic table concepts and explain how to transform data in a continuous\n","data pipeline using dynamic tables.\n","\n","\n","\n","\n","\n","\n","Concept\n","Description\n","\n","\n","\n","How dynamic tables work\n","Learn about the privileges needed to work with dynamic tables, how dynamic table refresh operates, and the distinctions between dynamic tables, streams & tasks, and materialized views.\n","\n","Working with dynamic tables\n","Learn about creating, managing, and monitoring dynamic tables.\n","\n","Best practices for dynamic tables\n","Understand the best practices for working with dynamic tables.\n","\n","Understanding cost for dynamic tables\n","Understand the compute and storage cost for dynamic tables.\n","\n","Known limitations for dynamic tables\n","Some actions might be restricted due to limitations on using dynamic tables or if you don’t have the necessary privileges.\n","\n","\n","\n","\n","Was this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Language: EnglishEnglishFrançaisDeutsch日本語한국어Português\n","------------------------------------------------------------------------------------------------------------------------------------\n"]}],"source":["for docs in documents:\n","    print(docs.page_content)\n","    print('------------------------------------------------------------------------------------------------------------------------------------')"]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(metadata={'source': 'https://docs.snowflake.com/en/user-guide/warehouses', 'title': 'Virtual warehouses | Snowflake Documentation', 'language': 'en'}, page_content='Virtual warehouses | Snowflake DocumentationDOCUMENTATION/Getting StartedGuidesDeveloperReferenceReleasesTutorialsPolaris CatalogStatusOverviewSnowflake HorizonConnecting to SnowflakeVirtual warehousesOverviewMulticlusterConsiderationsWorking with warehousesQuery Acceleration ServiceMonitoring loadSnowpark-optimized warehousesDatabases, Tables, & ViewsData TypesData LoadingData UnloadingQueriesData Sharing and CollaborationSnowflake AI & MLAlerts & NotificationsSecurityData GovernancePrivacyOrganizations & AccountsBusiness Continuity & Data RecoveryPerformance OptimizationCost & BillingGuidesVirtual warehouses\\n\\nVirtual warehouses¶\\nA virtual warehouse, often referred to simply as a “warehouse”, is a cluster of compute resources in Snowflake. A virtual warehouse is\\navailable in two types:\\n\\nStandard\\nSnowpark-optimized\\n\\nA warehouse provides the required resources, such as CPU, memory, and temporary storage, to\\nperform the following operations in a Snowflake session:\\n\\nExecuting SQL SELECT statements that require compute resources (e.g. retrieving rows from tables and views).\\nPerforming DML operations, such as:\\n\\nUpdating rows in tables (DELETE , INSERT , UPDATE).\\nLoading data into tables (COPY INTO <table>).\\nUnloading data from tables  (COPY INTO <location>).\\n\\n\\n\\n\\nNote\\nTo perform these operations, a warehouse must be running and in use for the session. While a warehouse is running, it consumes Snowflake\\ncredits.\\n\\n\\nOverview of warehousesWarehouses are required for queries, as well as all DML operations, including loading data into tables.\\nIn addition to being defined by its type as either Standard or Snowpark-optimized, a warehouse is defined by its size,\\nas well as the other properties that can be set to help control and automate warehouse activity.\\n\\nSnowpark-optimized warehousesSnowpark workloads can be run on both Standard and Snowpark-optimized warehouses. Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases\\n\\nWarehouse considerationsBest practices and general guidelines for using virtual warehouses in Snowflake to process queries\\n\\nMulti-cluster warehousesMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.\\n\\nWorking with warehousesLearn how to create, stop, start and otherwise manage Snowflake warehouses.\\n\\nUsing the Query Acceleration ServiceThe query acceleration service can accelerate parts of the query workload in a warehouse.\\nWhen enabled for a warehouse, query acceleration can improve overall warehouse performance by reducing the impact of outlier queries\\n(i.e. queries which use more resources then typical queries).\\n\\nMonitoring warehouse loadWarehouse query load measures the average number of queries that were running or queued within a specific interval.\\n\\n\\n\\n\\nOverview of warehouses\\nSnowpark-optimized warehouses\\nWarehouse considerations\\nMulti-cluster warehouses\\nWorking with warehouses\\nUsing the Query Acceleration Service\\nMonitoring warehouse load\\n\\n\\n\\nWas this page helpful?YesNoVisit SnowflakeJoin the conversationDevelop with SnowflakeShare your feedbackRead the latest on our blogGet your own certificationPrivacy NoticeSite Terms© 2024 Snowflake, Inc. All Rights Reserved.Related contentUnderstanding compute costWorking with resource monitorsLanguage: EnglishEnglishFrançaisDeutsch日本語한국어Português')]"]},"execution_count":211,"metadata":{},"output_type":"execute_result"}],"source":["retriever.get_relevant_documents('What is the Warehouse?')"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_variables=['context', 'question'] template='Answer the question based only on the following context:\\n{context}\\n\\nQUESTION: {question}'\n","The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /Users/abdulhakeemoyaqoob/.cache/huggingface/token\n","Login successful\n"]},{"name":"stdout","output_type":"stream","text":[" What library is used for creating it and how to perform a search using it?\n","\n","A: A virtual Warehouse is not a term used in the context provided. The context mentions FAISS (Facebook AI Similarity Search) and HuggingFaceEmbeddings, which are libraries used for creating and interacting with vector databases. The FAISS library is used to create a vectorstore object, and a search can be performed using the `search` method of the vectorstore object, passing the query vector and search parameters as arguments. For example, to perform a search with k=1, you can use the following code:\n","\n","```python\n","results = vectorstore.search(query_vector, search_kwargs)\n","```\n","\n","This will return a list containing the vector and index of the closest match in the vectorstore.\n"]}],"source":["from getpass import getpass\n","from langchain_core.prompts import PromptTemplate\n","from langchain_huggingface import HuggingFaceEndpoint\n","import os\n","#HUGGINGFACEHUB_API_TOKEN = getpass()\n","\n","\n","#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n","\n","\n","template = \"\"\"\n","Answer the question based only on the following context:\n","{context}\n","\n","QUESTION: {question}\n","\n","\n","\"\"\".strip()\n","question = 'What is a virtual Warehouse?'\n","\n","prompt = PromptTemplate.from_template(template)\n","\n","print(prompt)\n","\n","repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id=repo_id,\n","    temperature=0.5,\n","    #huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",")\n","llm_chain = prompt | llm\n","print(llm_chain.invoke({\"question\": question, 'context':retriever}))\n"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"data":{"text/plain":["'What is a virtual Warehouse?'"]},"execution_count":213,"metadata":{},"output_type":"execute_result"}],"source":["question"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","ANSWER: A virtual warehouse is a cluster of compute resources in Snowflake. It is available in two types: Standard and Snowpark-optimized. A warehouse provides the required resources, such as CPU, memory, and temporary storage, to perform operations in a Snowflake session, including executing SQL SELECT statements, performing DML operations, loading and unloading data, and more. A warehouse must be running and in use for the session to perform these operations, and while it is running, it consumes Snowflake credits. Warehouses are required for queries and all DML operations. In addition to being defined by its type, a warehouse is also defined by its size and other properties that can be set to help control and automate warehouse activity. Snowpark workloads can be run on both Standard and Snowpark-optimized warehouses, and Snowpark-optimized warehouses are recommended for workloads that have large memory requirements such as ML training use cases. Multi-cluster warehouses enable you to scale compute resources to manage user and query concurrency needs, and the Query Acceleration Service can accelerate parts of the query workload in a warehouse to improve overall performance.\n"]}],"source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","print(rag_chain.invoke(question))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":2}
